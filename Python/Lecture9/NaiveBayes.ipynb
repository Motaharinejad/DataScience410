{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Naive Bayes Models\n",
    "\n",
    "### Stephen Elston\n",
    "### Data Science 350\n",
    "\n",
    "This notebook introduces you to naive Bayes models. Naive Bayes models are a surprisingly useful and effective simplification of the general Bayesian models. Naive Bayes models make the naive assumption of independence of the features.\n",
    "\n",
    "Some properties of naive Bayes models are:\n",
    "\n",
    "- Do not require a prior.\n",
    "- Computational complexity is linear in number of parameter/features.\n",
    "- Requires minimal data to produce models that generalizes well.\n",
    "- Have a simple and inherent regularization.\n",
    "\n",
    "Naive Bayes models are widely used including for:\n",
    "\n",
    "- Document classification\n",
    "- SPAM detection\n",
    "- Image classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of Naive Bayes Models\n",
    "\n",
    "Recall Bayes theorem:\n",
    "\n",
    "$$P(A|B) = P(B|A) \\frac{P(A)}{P(B)}$$\n",
    "\n",
    "Using the chain rule of conditional probabilities, we can create write the joint distribution for the probability of class $C_k$ as: \n",
    "\n",
    "$$\n",
    "p(C_k, x_1, x_2, x_3, \\ldots, x_n) = p(x_1, x_2, x_3, \\ldots, x_n, C_k) \\\\\n",
    "=  p(x_1 | x_2, x_3, \\ldots, x_n, C_k)  p(x_2, x_3, \\ldots, x_n, C_k) \\\\\n",
    "= p(x_1 | x_2, x_3, \\ldots, x_n, C_k)  p(x_2 | x_3, \\ldots, x_n, C_k) p(x_3, \\ldots, x_n, C_k) \\\\\n",
    "\\cdots \\cdots \\\\\\\n",
    "=  p(x_1 | x_2, x_3, \\ldots, x_n, C_k)  p(x_2 | x_3, \\ldots, x_n, C_k) \\dots p(C_k)\n",
    "$$\n",
    "\n",
    "Let the features, $\\{ x_1, x_2, x_3, \\ldots, x_n \\}$, be independent, the above can then be written as:\n",
    "\n",
    "$$p(x_i | x_{i + 1}, x_{i + 2}, \\ldots, x_n, C_k) = p(x_i | C_k)$$\n",
    "\n",
    "This simpification allows us to write the probability of the class $C_k$ as the conditional distribution:\n",
    "\n",
    "$$p(C_k | x_1, x_2, x_3, \\ldots, x_n) \\propto p(C_k) \\prod^N_{j = 1} p(x_j|C_k)$$\n",
    "\n",
    "Given a number of classes, we can find the mostly likely class $\\hat{y}$ as:\n",
    "\n",
    "$$\\hat{y} = argmax_k \\Big[ \\prod^N_{j = 1} p(x_j|C_k) \\Big]$$\n",
    "\n",
    "Notice that the above formulation uses only the emperical probabilities of the features conditioned on the class. Futher no prior distirbution is required. \n",
    "\n",
    "### Pitfalls in naive Bayes models\n",
    "\n",
    "There are some well know pitfalls with known solutions, including:\n",
    "\n",
    "- Multiplication of small probabilities leads to floating point underflow. This problem is corrected by computing with the log probabilities, $ln(p)$.\n",
    "- If there are no samples/data then $p(x_j|C_k) = 0$, leading the product of probabilities to be 0. A Laplace smoother is used to ensure that all $p(x_.j|C_k) > 0$\n",
    "- Colinear features do not exhibit independence. Ideally, such features should be removed from the the data set to prevent problems with the  model.\n",
    "- Regularization is generally a minor issue with naive Bayes models, as uninformative feature tend to a uniform distribution which does not affect the outcome.\n",
    "\n",
    "### Types of naive Bayes models\n",
    "\n",
    "Now that we have looked into the basics of a naive Bayes models, lets look at some specific formulations. It is important to keep in mind that a specific naive Bayes model is required for each class of problem.  \n",
    "\n",
    "The Multinomial naive Bayes classifier is a widely used form of the model. The Multinomial classifier finds the mostly likely class from multiple possibilities. To prevent numerical underflow we write this classifier taking the logarithms of both sides of the equation as follows:\n",
    "\n",
    "$$log \\Big( p(C_k | x) \\Big) \\propto\\ log \\Big( p(C_k) \\prod^N_{j = 1} p_{kj}(x_i) \\Big)\\\\\n",
    "= log \\Big( p(C_k) \\Big) + log \\Big( \\sum^N_{j = 1} p_{kj}(x_i) \\Big)$$\n",
    "\n",
    "The most likely class $\\hat{y}$ is then:\n",
    "\n",
    "$$\\hat{y} = argmax_k \\Big[ log \\Big( p(C_k) \\Big) + log \\Big( \\sum^N_{j = 1} p_{kj}(x_i) \\Big) \\Big]$$\n",
    "\n",
    "The multinomial classifier can be simplified for the Bernoulli or binary case as:\n",
    "\n",
    "$$log \\Big( p(x | C_k) \\Big) = log \\Big( \\sum^N_{j = 1} p_{kj}^{x_i} \\big( 1 -  p_{kj}^{(1 - x_i)} \\big) \\Big)$$\n",
    "\n",
    "\n",
    "### Document classification with naive Bayes\n",
    "\n",
    "Document classification has been one of the most sucessful applications of the naive Bayes mdoel. There is a good chance that the SPAM filter your email serivce uses is a naive Bayes model, at least in part. \n",
    "\n",
    "\n",
    "We say that we classify documents by **topics**. The naive Bayes topic model computes the probability that a document $D$ has topic $C$ based on the occurance of the words $\\{ w_1, w_2, \\ldots, w_n \\}$, using the following relationship:\n",
    "\n",
    "$$p(C|D) \\propto \\prod_{j = 1}^N p(w_j|C)$$\n",
    "\n",
    "Notice that this topic model allows a document to have a number of topics. For example, we can say the topics of $D$ are the 5 topics with the highest probability.\n",
    "\n",
    "For a SPAM classifier, we only need a Bernoulli topic model:\n",
    "\n",
    "$$p(S+|D) \\propto p(S+) \\prod_{j=1}^N p(w_j|S+)$$\n",
    "\n",
    "An hypothesis test is applied to each message to determine if it is SPAM. We use the log likihood ratio to determine if a given message is SPAM or not. If the following ratio is $> 1$ we classify the message as SPAM:\n",
    "\n",
    "$$ln \\Bigg( \\frac{p(S+)}{p(S-)} \\Bigg) = \\sum_{j = 1}^N \\Big[ log \\big( p(w_j|S+) \\big) - log \\big( p(w_j|S-) \\big) \\Big]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy\n",
    "import seaborn\n",
    "from matplotlib import pyplot\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of Naive Bayes Model\n",
    "\n",
    "Let's try a simple example of a naive bayes model. The R mlbench package contains the `HouseVotes84` which contains political party and votes on 16 important bills for 435 members of the US House of Representative. We will use this data set to build and test a classifier to predict the political party of the Congresspeople. \n",
    "\n",
    "Execute the code in the cell below to load the data and examine the dimensions and head of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#https://archive.ics.uci.edu/ml/datasets/congressional+voting+records\n",
    "votes = pandas.read_csv('house-votes-84.csv', header=None, \n",
    "                        names=['class', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', \n",
    "                                   'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16'])\n",
    "print(votes.shape)\n",
    "votes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "each of the vote columns in the above data frame corresponds to the following topics, so let's replace the column names with something easier to remember"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vote_names = [\n",
    "    'handicapped-infants',\n",
    "    'water-project-cost-sharing',\n",
    "    'adoption-of-the-budget-resolution',\n",
    "    'physician-fee-freeze',\n",
    "    'el-salvador-aid',\n",
    "    'religious-groups-in-schools',\n",
    "    'anti-satellite-test-ban',\n",
    "    'aid-to-nicaraguan-contras',\n",
    "    'mx-missile',\n",
    "    'immigration',\n",
    "    'synfuels-corporation-cutback',\n",
    "    'education-spending',\n",
    "    'superfund-right-to-sue',\n",
    "    'crime',\n",
    "    'duty-free-exports',\n",
    "    'export-administration-act-south-africa']\n",
    "\n",
    "votes.columns = ['class'] + vote_names\n",
    "votes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To further understand this data, let's make some plots of the first 5 votes. The code in the cell below creates bar plots for these votes by `Class` or political parties. Execute this code and examine the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's also convert our dataframe columns to \"category\" type to take advantage\n",
    "# categorical utilities like facet-based plotting in seaborn\n",
    "votes['class'] = votes['class'].astype('object').astype(\n",
    "    'category', categories=['republican', 'democrat'])\n",
    "\n",
    "values = ['y', 'n', '?']\n",
    "for c in votes.columns[1:]:\n",
    "    votes[c] = votes[c].astype('object').astype(\n",
    "        'category', categories=values)\n",
    "    \n",
    "for vote_col in votes.columns[1:6]:\n",
    "    seaborn.countplot(votes[vote_col])\n",
    "    pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When examining these charts, keep in mind that there are more Democrats than Republican. What is important is the probability of Yes vote or No vote for the members of each party. Some votes are quite skewed by party affiliation, such as 'adoption of the budget resolution'. Whereas, some votes have similar probabilities by party, such as 'water project cost sharing'. These probabilities of votes by party are used to train the naive Bayes model.\n",
    "\n",
    "Now that we understand a bit about the characteristics of the data, its time to train and test a naive Bayes model. The python `sklearn.naive_bayes` package provides a library that trains a naive Bayes model and produces a model object that can make predictions on new instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "label_col = 'class'\n",
    "\n",
    "# We need to convert our categorical values to numeric feature vectors\n",
    "feature_vecs = numpy.array([\n",
    "        votes[c].cat.codes \n",
    "        for c in votes.columns \n",
    "        if c != label_col]).T\n",
    "feature_vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we also need to convert our label (democrat vs republican) to numeric values\n",
    "labels = votes[label_col].cat.codes\n",
    "# take a look at the mapping for the first 5 values like so\n",
    "list(zip(votes[label_col][:5], labels[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sklearn.naive_bayes\n",
    "\n",
    "# Define the model\n",
    "model = sklearn.naive_bayes.MultinomialNB(alpha=1e-7)\n",
    "# Train the model with our votes dataset\n",
    "model.fit(feature_vecs, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the model computed, let's evaluate the performance. We can get a quick overview of the model's effectiveness by printing the first 10 rows of the result. Execute the code in the cell below to print the first 10 rows of the result and examine the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predicted_party = model.predict(feature_vecs[:10])\n",
    "party_probabilities = model.predict_proba(feature_vecs[:10])\n",
    "\n",
    "results = pandas.DataFrame({\n",
    "        'party': votes['class'][:10],\n",
    "        'predicted': pandas.Categorical.from_codes(\n",
    "            predicted_party, votes['class'][:10].cat.categories),\n",
    "        'proba(democrat)': party_probabilities[:, 0],\n",
    "        'proba(republican)': party_probabilities[:, 1],\n",
    "    })\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you examine these results noticing:\n",
    "1. There is 1 classification error, with 9 cases correctly classified.\n",
    "2. In most cases, the probability of the class predicted (score) is much larger than for the other class, including for the cases with classification errors.\n",
    "3. One case has nearly identical probabilities for the classes.\n",
    "\n",
    "As a next step, we can compute the confusion matrix and performance metrics for this model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sklearn.metrics\n",
    "\n",
    "def performance(results):\n",
    "    accuracy = sklearn.metrics.accuracy_score(\n",
    "        results['party'].cat.codes, results['predicted'].cat.codes)\n",
    "    precision = sklearn.metrics.precision_score(\n",
    "            results['party'].cat.codes, results['predicted'].cat.codes)\n",
    "    recall = sklearn.metrics.recall_score(\n",
    "            results['party'].cat.codes, results['predicted'].cat.codes)\n",
    "\n",
    "    print('Accuracy = %.3f, Precision = %.3f, Recall = %.3f' % (accuracy, precision, recall))\n",
    "    \n",
    "    confusion = pandas.DataFrame(\n",
    "        sklearn.metrics.confusion_matrix(\n",
    "            results['party'], results['predicted']),\n",
    "        index=[results.party.cat.categories], \n",
    "        columns=results.party.cat.categories)\n",
    "    \n",
    "    return confusion    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "performance(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are reasonably good looking at only the first 10 of 435 Congresspeople. \n",
    "\n",
    "Execute the code in the cell below computes and prints an evaluation of the model using all the data and compare the results to the first model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predicted_party = model.predict(feature_vecs)\n",
    "party_probabilities = model.predict_proba(feature_vecs)\n",
    "\n",
    "results_all = pandas.DataFrame({\n",
    "        'party': votes['class'],\n",
    "        'predicted': pandas.Categorical.from_codes(\n",
    "            predicted_party, votes['class'].cat.categories),\n",
    "        'proba(democrat)': party_probabilities[:, 0],\n",
    "        'proba(republican)': party_probabilities[:, 1],\n",
    "    })\n",
    "performance(results_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Laplace smoothing method is an effective way to deal with data sets which do not have sufficient samples to compute probabilities. This method avoids the case where $p(x_j|C_k) = 0$. \n",
    "\n",
    "The code in the cell below computes a naive Bayes model using the same congressional vote data, but with a Laplace smoother with a span of 3 data points. Execute this code and examine the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The `alpha` param controls the Laplace smoothing\n",
    "model = sklearn.naive_bayes.MultinomialNB(alpha=3)\n",
    "model.fit(feature_vecs, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predicted_party = model.predict(feature_vecs)\n",
    "party_probabilities = model.predict_proba(feature_vecs)\n",
    "\n",
    "results_all = pandas.DataFrame({\n",
    "        'party': votes['class'],\n",
    "        'predicted': pandas.Categorical.from_codes(\n",
    "            predicted_party, votes['class'].cat.categories),\n",
    "        'proba(democrat)': party_probabilities[:, 0],\n",
    "        'proba(republican)': party_probabilities[:, 1],\n",
    "    })\n",
    "performance(results_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are similar to the model computed without Laplace smoothing. This result is expected as all the cases in the data set have sufficient data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another Example\n",
    "\n",
    "Let's try another binary classification example. The code in the cell below loads some sample US Census data. We want to build and evaluate a naive Bayes model to classify people by high and low income using $50,000 as the cut-off. Execute this code and examine the features in the data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "income = pandas.read_csv('Adult Census Income Binary Classification dataset.csv', sep=', ', engine='python')\n",
    "income.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see some features which are likely to be colinear. There is also one feature, 'fnlwgt', which is not useful in classifing these people. The code in the cell below removes these columns. Execute this code to create a data set with reduced features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "income = income.drop(['workclass', 'fnlwgt', 'education-num', 'relationship'], axis=1)\n",
    "income.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**Your turn.** Compute a naive Bayes model to classify `income` using the features in the Income data set. Use `laplace = 3` for smoothing.  Print the model and examine the conditional probabilities for the values of the features to get an idea how the classifer works. \n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for c in income.columns:\n",
    "    if income[c].dtype == 'object':\n",
    "        income[c] = income[c].astype('category')\n",
    "\n",
    "label_col = 'income'\n",
    "labels = income.income.cat.codes\n",
    "\n",
    "features = []\n",
    "for c in income.columns:\n",
    "    if c != label_col:\n",
    "        if str(income[c].dtype) == 'category':\n",
    "            features.append(income[c].cat.codes)\n",
    "        else:\n",
    "            features.append(income[c])\n",
    "feature_vecs = numpy.array(features).T\n",
    "\n",
    "model = sklearn.naive_bayes.MultinomialNB(alpha=3)\n",
    "model.fit(feature_vecs, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conditional_probas = pandas.DataFrame(model.feature_log_prob_, columns=income.columns[:-1])\n",
    "conditional_probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(-conditional_probas).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**Your turn.** Using the model you computed for predicting peoples' income class, compute predictions (scores). Also compute and print the performance metrics. \n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predicted_income = model.predict(feature_vecs)\n",
    "income_probabilities = model.predict_proba(feature_vecs)\n",
    "\n",
    "accuracy = sklearn.metrics.accuracy_score(labels, predicted_income)\n",
    "precision = sklearn.metrics.precision_score(labels, predicted_income)\n",
    "recall = sklearn.metrics.recall_score(labels, predicted_income)\n",
    "print('Accuracy = %.3f, Precision = %.3f, Recall = %.3f' % (accuracy, precision, recall))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing Number of Features\n",
    "\n",
    "Let's investigate the effect of adding more data samples to the naive Bayes model. The code in the cell below computes and evaluates naive Bayes models using 2, 3, 4, 5 and 6 votes. Execute this code and compare the results to those obtained using the full data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for num_features in [2, 3, 4, 5, 6]:\n",
    "    model = sklearn.naive_bayes.MultinomialNB()\n",
    "    model.fit(feature_vecs[:, :num_features], labels)\n",
    "\n",
    "    predicted_income = model.predict(feature_vecs[:, :num_features])\n",
    "    income_probabilities = model.predict_proba(feature_vecs[:, :num_features])\n",
    "\n",
    "    print('Number of features = %d' % num_features)\n",
    "    accuracy = sklearn.metrics.accuracy_score(labels, predicted_income)\n",
    "    precision = sklearn.metrics.precision_score(labels, predicted_income)\n",
    "    recall = sklearn.metrics.recall_score(labels, predicted_income)\n",
    "    print('Accuracy = %.3f, Precision = %.3f, Recall = %.3f' % (accuracy, precision, recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see from these results, that the model gains accuracy rapidly with just a few features. In fact, 5 or 6 of the 16 features provides equivalent results. \n",
    "\n",
    "***\n",
    "**Your turn.** Compute a naive Bayes model, compute and  print the performance statistics for 100, 500, 1000, 2000, 8000, and 32561 rows of the Income data set. How many rows are required until the performance is close to the best possible with the model. \n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for num_rows in (100, 500, 1000, 2000, 8000, 32561):\n",
    "    model = sklearn.naive_bayes.MultinomialNB()\n",
    "    model.fit(feature_vecs[:num_rows, :], labels[:num_rows])\n",
    "\n",
    "    predicted_income = model.predict(feature_vecs[:num_rows, :])\n",
    "    income_probabilities = model.predict_proba(feature_vecs[:num_rows, :])\n",
    "    \n",
    "    accuracy = sklearn.metrics.accuracy_score(labels[:num_rows], predicted_income)\n",
    "    precision = sklearn.metrics.precision_score(labels[:num_rows], predicted_income)\n",
    "    recall = sklearn.metrics.recall_score(labels[:num_rows], predicted_income)\n",
    "    print('Accuracy = %.3f, Precision = %.3f, Recall = %.3f' % (accuracy, precision, recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook you have worked through the following:\n",
    "\n",
    "1. Theory of naive Bayes models.\n",
    "2. Examples of computing and evaluating naive Bayes models.\n",
    "3. Examine the effects of data set size on the results of naive Bayes models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Copyright 2017, Stephen F Elston. All rights reserved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
