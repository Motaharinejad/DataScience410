{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Stepwise Regression\n",
    "\n",
    "## Data Science 410\n",
    "\n",
    "In this notebook we will start our exploration of the mathematical basis of linear statistical models. The emphasis is on the ubiquitous problem of **model over-fitting** or **model over-parameterization**. \n",
    "\n",
    "Over-fitting (or over-parameterization) of machine learning models arises in any case where the number of model parameters exceeds the effective dimensions of the feature set. This is most often the result of linear dependency between the features. However, using too complex a model can lead to similar problems. In the extreme case, imagine a model with as many free parameters as training cases. This model might fit the training data perfectly, but will show unstable and unexpected results when used for other data. In machine learning terminology, we say that such an unstable model does not **generalize**. \n",
    "\n",
    "Many methods have been developed and continue to be developed to deal with over-parameterized or **ill-posed** machine learning models. In particular, we will explore three methods for stabilizing over-parameterized models: \n",
    "\n",
    "- Stepwise regression, wherein features are eliminated from an over-parameterized model in a stepwise fashion. Stepwise regression is the focus of this notebook. \n",
    "- Using a mathematical **regularization** technique, known as singular value decomposition, to determine the number of meaningful components for a model. We will explore use of singular value decomposition in another notebook. \n",
    "- Using **regularization** methods known as ridge, lasso, and elastic-net regression to stabilize over-parameterized models. We will explore ridge and lasso regression in another notebook.\n",
    "\n",
    "![](img/Extrapolation.png)\n",
    "<center> Warning!! Extrapolation can be dangerious!! </center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Stepwise Regression\n",
    "\n",
    "**Stepwise regression** uses model performance metrics to prune the number of features in a model. There are two possible approaches: \n",
    "1. The steps can be **forward**, wherein features are added one at a time in order of importance, until the metric used no longer improves. \n",
    "2. The steps can be **backward**, wherein a model using all features is pruned one feature at a time in reverse order of importance until the metric used becomes significantly worse. \n",
    "\n",
    "It is also possible to step in both directions. In practice, either backward steps or using both directions are used, since forward steps have a tendency to get stuck at poor solutions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Akaike Information Criterion\n",
    "\n",
    "A significant issue with stepwise regression is to choose a performance metric. Many commonly used error metrics like RMSE will naturally get better as we add more features or model parameters. Consequently the **Akaike information criterion** (AIC) is often used. \n",
    "\n",
    "<img src=\"img/Akaike_1996.jpg\" alt=\"Drawing\" style=\"width:275x; height:350px\"/>\n",
    "\n",
    "<center>Hirotugu Akaike receiving Japan Statistical Society Prize 1996</center>   \n",
    "\n",
    "The idea of the AIC is to trade-off two completing criteria:\n",
    "1. Maximize the likelihood of the model. \n",
    "2. Minimize the complexity of the model to prevent over-parameterization. \n",
    "\n",
    "We can write the AIC as:\n",
    "\n",
    "$$AIC = 2 k - 2 ln(\\hat{L})\\\\\n",
    "where\\\\\n",
    "\\hat{L} = the\\ likelihood\\ given\\ the\\ fitted\\ model\\ parameters\\ \\hat\\theta = p(x| \\hat\\theta)\\\\\n",
    "x = observed\\ data\\\\\n",
    "k = number\\ of\\ model\\ parameters$$\n",
    "\n",
    "In words, the AIC is the model log-likelihood adjusted for the number of model parameters. The objective is to **minimize the AIC**. \n",
    "\n",
    "The quantity $- 2 ln(p(x| \\hat\\theta))$ is sometimes referred to as the **deviance** of the model. Deviance is a measure of the relative likelihood of the model. Deviance is a generalization of the variance. Strictly, deviance measures the difference in distribution between a saturated model (number of parameters = number of observations), and the model being considered. The saturated model will have a perfect fit to the training data, but is highly over-parameterized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pitfalls of Stepwise Regression\n",
    "\n",
    "Stepwise regression is a form of multiple hypothesis testing. Therefore, it is subject to the problems inherent in this method:\n",
    "1. The most important features may be rejected from the model. \n",
    "2. Unimportant features may be retained in the model. \n",
    "\n",
    "For these reasons, stepwise regression should be used with caution. It is generally the case that stepwise regression will not find optimal models if there are more than a few features to consider. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of Stepwise Regression\n",
    "\n",
    "In this section we will work through an example of stepwise regression. We will start with an over-parameterized model. Both **forward stepwise regression** and **backward stepwise regression** will be applied to finding a better model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the data\n",
    "\n",
    "As a first step we will create a data set to use as a polynomial regression model example. \n",
    "\n",
    "Execute this code in the cells below to created the simulateddata. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy.random as nr\n",
    "import statsmodels.formula.api as sm\n",
    "import scipy.stats as ss\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramters of generated data\n",
    "n_points = 100\n",
    "x_start, x_end = 0, 10\n",
    "y_sd = 2.0\n",
    "\n",
    "# Generate data columns\n",
    "nr.seed(474747)\n",
    "x_data = np.linspace(x_start, x_end, n_points) # The x values\n",
    "y_error = np.random.normal(loc=0, scale=y_sd, size=n_points) # The Normally distributed noise\n",
    "y_data = x_data + 0.6 * np.square(x_data) + y_error + 1.0 # The y values including an intercept\n",
    "\n",
    "# Put data in dataframe\n",
    "sim_data = pd.DataFrame({'x':x_data, 'y':y_data})\n",
    "\n",
    "sim_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now execute the code in the cell below to compute some polynomial features and scale all features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scale function for a column in a pandas df\n",
    "def scale(col):\n",
    "    mean_col = np.mean(col)\n",
    "    sd_col = np.std(col)\n",
    "    std = (col - mean_col) / sd_col\n",
    "    return std\n",
    "\n",
    "## Create polynomial features\n",
    "sim_data['x_square'] = np.square(sim_data.x)\n",
    "sim_data['x_cube'] = np.multiply(sim_data.x, sim_data.x_square)\n",
    "\n",
    "## Scale the features\n",
    "for col in ['x','x_square','x_cube']:\n",
    "    sim_data[col] = scale(sim_data[col])\n",
    "    \n",
    "sim_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, run the code in the cell below to randomly select observations for a train and test sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_data_train = sim_data.sample(frac=0.5,random_state=234) #random state is a seed value\n",
    "sim_data_test = sim_data.drop(sim_data_train.index)\n",
    "print(sim_data_train.shape, '  ', sim_data_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the Polynomial Regression Model\n",
    "\n",
    "The code in the cell below fits a OLS model to the training data. Execute this code and examine the summary of the model fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the regresson model and fit it to the data\n",
    "ols_model_1 = sm.ols(formula = 'y ~ x + x_square + x_cube', data=sim_data_train).fit()\n",
    "\n",
    "## Print the model summary\n",
    "ols_model_1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the table of significance of the model coefficients. **The model is over-fit**, with two of the coefficients not significant. Also, note the value of $R^2$ and AIC. \n",
    "\n",
    "Despite the over-fitting, we should look at the residuals. Execute the code in the cell below to do so.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_resid_dist(resids):\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))\n",
    "    ## Plot a histogram\n",
    "    sns.distplot(resids, bins=20, ax=ax[0])\n",
    "    ax[0].set_title('Histogram of residuals')\n",
    "    ax[0].set_xlabel('Residual values')\n",
    "    ## Plot the Q-Q Normal plot\n",
    "    ss.probplot(resids, plot = ax[1])\n",
    "    ax[1].set_title('Q-Q Normal plot of residuals')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def residual_plot(df):\n",
    "    RMSE = np.std(df.resids)\n",
    "    sns.scatterplot(x='predicted', y='resids', data=df)\n",
    "    plt.axhline(0.0, color='red', linewidth=1.0)\n",
    "    plt.axhline(2.0*RMSE, color='red', linestyle='dashed', linewidth=1.0)\n",
    "    plt.axhline(-2.0*RMSE, color='red', linestyle='dashed', linewidth=1.0)\n",
    "    plt.title('PLot of residuals vs. predicted')\n",
    "    plt.xlabel('Predicted values')\n",
    "    plt.ylabel('Residuals')\n",
    "    plt.show()\n",
    "    \n",
    "   \n",
    "## Add predicted values and residuals to pandas dataframe\n",
    "sim_data_train['predicted'] =  ols_model_1.predict(sim_data_train[['x','x_square','x_cube']])\n",
    "sim_data_train['resids'] = np.subtract(sim_data_train.predicted, sim_data_train.y)\n",
    "\n",
    "## Create the residal plots\n",
    "plot_resid_dist(sim_data_train.resids)\n",
    "residual_plot(sim_data_train) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These residuals look well behaved; close to Normally distributed and homoscedastic. \n",
    "\n",
    "We can now see how well the model fits the test data. Execute the code in the cell below to compute the RMSE and plot the observations vs. the fitted curve.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ols(df, model, features):\n",
    "    ## Compute the predicted values and residuals\n",
    "    df['predicted'] =  ols_model_1.predict(df[features])\n",
    "    df['resids'] = np.subtract(df.predicted, df.y)\n",
    "    print('RMSE = %6.3f' % (np.sum(df.resids)))\n",
    "    ## display the plot\n",
    "    ax = sns.lineplot(x='x', y='predicted', data=df, color='red')\n",
    "    sns.scatterplot(x='x', y='y', data=df, ax=ax)\n",
    "    \n",
    "plot_ols(sim_data_test, ols_model_1, ['x','x_square','x_cube'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the fit of the test data to the model is reasonably good. This will often not be the case with over-fit models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply forward stepwise regression\n",
    "\n",
    "The code we will use for forward stepwise regression assumes that all columns of the data frame, except the label column, are potential features. Therefore, you must execute the code in the cell below first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop the columns we don't want in the training data\n",
    "sim_data_train.drop(['predicted', 'resids'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, execute the code in the cell below to perform forward stepwise regression and examine the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_selected(data, response, verbose=True):\n",
    "    \"\"\"Linear model designed by forward selection. Based on AIC\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : pandas DataFrame with all possible predictors and response\n",
    "\n",
    "    response: string, name of response column in data\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    model: an \"optimal\" fitted statsmodels linear model\n",
    "           with an intercept\n",
    "           selected by forward selection\n",
    "    \"\"\"\n",
    "    # Start with no factors (intercept only)\n",
    "    formula = \"{} ~ 1\".format(response)\n",
    "    best_aic = sm.ols(formula, data).fit().aic\n",
    "    \n",
    "    # Go through remaining sets of variables one-by-one\n",
    "    remaining = set(data.columns)\n",
    "    remaining.remove(response)\n",
    "    selected = []\n",
    "    current_aic = best_aic\n",
    "    \n",
    "    if(verbose):\n",
    "                print('For formula ', formula, ' the AIC = ', current_aic)\n",
    "    \n",
    "    # Check if any variables remain and if we haven't improved by adding any yet\n",
    "    while remaining and current_aic == best_aic:\n",
    "        aic_candidates = []\n",
    "        for candidate in remaining:\n",
    "            # Try adding the candidate column\n",
    "            formula = \"{} ~ {} + 1\".format(response, ' + '.join(selected + [candidate]))\n",
    "            # Get AIC\n",
    "            aic = sm.ols(formula, data).fit().aic\n",
    "            # Append tuple of the form (aic, response)\n",
    "            aic_candidates.append((aic, candidate))\n",
    "            if(verbose):\n",
    "                print('For formula ', formula, ' the AIC = ', aic)\n",
    "        # Sort all the pairs by the first entry of tuple (default of sort() method)\n",
    "        aic_candidates.sort()\n",
    "        # Remember that the sort() method sorts by smallest to largest of first entry here.\n",
    "        #   If you were to change the criteria to something that needs to be maximized, change sort/pop order!\n",
    "        best_new_aic, best_candidate = aic_candidates.pop(0)\n",
    "        # Now check if we have something better:\n",
    "        if best_new_aic < current_aic:\n",
    "            remaining.remove(best_candidate)\n",
    "            selected.append(best_candidate)\n",
    "            current_aic = best_new_aic\n",
    "        # Now we repeat the process with all the remaining candidate columns\n",
    "\n",
    "    # Here is the final formula!\n",
    "    formula = \"{} ~ {} + 1\".format(response, ' + '.join(selected))\n",
    "    # Get the model object\n",
    "    model = sm.ols(formula, data).fit()\n",
    "    \n",
    "    if(verbose):\n",
    "        print('\\nThe selected model formula is ', model.model.formula)\n",
    "        print('The final AIC =', model.aic)\n",
    "        print('\\nThe model summary is')\n",
    "        print(model.summary())\n",
    "    return model\n",
    "\n",
    "#model = forward_selected(male_df, 'childHeight')\n",
    "ols_model_fsw = forward_selected(sim_data_train, 'y')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has one feature plus the intercept. The AIC is bit higher. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Backward stepwise selection\n",
    "\n",
    "Backward stepwise selection is a very similar algorithm.  Execute the code in the cell below and examine the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_selected(data, response, verbose=True):\n",
    "    \"\"\"Linear model designed by backward selection. Based on AIC\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : pandas DataFrame with all possible predictors and response\n",
    "\n",
    "    response: string, name of response column in data\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    model: an \"optimal\" fitted statsmodels linear model\n",
    "           with an intercept\n",
    "           selected by backward selection\n",
    "    \"\"\"\n",
    "    # Start with all factors and intercept\n",
    "    possible_factors = set(data.columns)\n",
    "    possible_factors.remove(response)\n",
    "    formula = \"{} ~ {} + 1\".format(response, ' + '.join(possible_factors))\n",
    "    \n",
    "    # Fill out this formula!\n",
    "    #--------------------------------\n",
    "    # Fill in spot #1!!!!\n",
    "    best_aic = sm.ols(formula, data).fit().aic\n",
    "    #--------------------------------\n",
    "    \n",
    "    current_aic = best_aic\n",
    "    \n",
    "    if(verbose):\n",
    "                print('For formula ', formula, ' the AIC = ', current_aic)\n",
    "    \n",
    "    # Set a non-empty set of columns that will be labeled as \"to remove and try\"\n",
    "    to_try_remove = possible_factors\n",
    "    \n",
    "    # Check if any variables remain and if we haven't improved by adding any yet\n",
    "    while to_try_remove and current_aic == best_aic:\n",
    "        aic_candidates = []\n",
    "        for candidate in to_try_remove:\n",
    "            \n",
    "            columns = possible_factors - set([candidate])\n",
    "            # Try removing the candidate column\n",
    "            formula = \"{} ~ {} + 1\".format(response, ' + '.join(columns))\n",
    "            # Get AIC\n",
    "            aic = sm.ols(formula, data).fit().aic\n",
    "            \n",
    "            if(verbose):\n",
    "                print('For formula ', formula, ' the AIC = ', aic)\n",
    "            \n",
    "            # Append tuple of the form (aic, response)\n",
    "            aic_candidates.append((aic, candidate))\n",
    "            \n",
    "        # Sort all the pairs by the first entry of tuple (default of sort() method)\n",
    "        aic_candidates.sort()\n",
    "        # Remember that the sort() method sorts by smallest to largest of first entry here.\n",
    "        #   If you were to change the criteria to something that needs to be maximized, change sort/pop order!\n",
    "        best_new_aic, best_candidate = aic_candidates.pop(0)\n",
    "        \n",
    "        # Now check if we have something better:\n",
    "        if best_new_aic < current_aic:\n",
    "            # Remove the best candidate's name from possible_factors\n",
    "            \n",
    "            #--------------------------------\n",
    "            # Fill in spot #2!!!!\n",
    "            possible_factors.remove(best_candidate)\n",
    "            current_aic = best_new_aic\n",
    "            #--------------------------------\n",
    "            \n",
    "        # Now we repeat the process with all the remaining candidate columns\n",
    "\n",
    "    # Here is the final formula!\n",
    "    formula = \"{} ~ {} + 1\".format(response, ' + '.join(possible_factors))\n",
    "    # Get the model object\n",
    "    model = sm.ols(formula, data).fit()\n",
    "    \n",
    "    if(verbose):\n",
    "        print('\\nThe selected model formula is ', model.model.formula)\n",
    "        print('The final AIC =', model.aic)\n",
    "        print('\\nThe model summary is')\n",
    "        print(model.summary())\n",
    "    return model\n",
    "\n",
    "\n",
    "ols_model_bsw = backward_selected(sim_data_train, 'y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model selected is identical to the model found with forward stepwise regression. In more complex problems this is often not the case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your turn:**\n",
    "\n",
    "The forward and backward stepwise regression has found the same optimal model. You will now do the following to evaluate the model:\n",
    "1. Plot the distribution of the residuals using the training data.\n",
    "2. Plot the residuals vs. the predicted values using the training data. \n",
    "3. Plot the predicted line along with observations in the sim_data_test data set.  \n",
    "4. Compute and print the RMSE using the sim_data_test data set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does this model compare to the model using all the features? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Copyright 2017, 2018, 2020 Stephen F Elston. All rights reserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
