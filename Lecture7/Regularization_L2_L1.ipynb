{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theory of L2 and L1 Regression and Regularization\n",
    "\n",
    "## Data Science 410\n",
    "\n",
    "In this notebook we will continue our exploration of the mathematical basis of linear statistical models. The emphasis is on the ubiquitous problem of **model over-fitting** or **model over-parameterization**. \n",
    "\n",
    "Over-fitting (or over-parameterization) of machine learning models arises in any case where the number of model parameters exceeds the effective dimensions of the feature set. This is most often the result of linear dependency between the features. However, using too complex a model can lead to similar problems. In the extreme case, imagine a model with as many free parameters as training cases. This model might fit the training data perfectly, but will show unstable and unexpected results when used for other data. In machine learning terminology, we say that such an unstable model does not **generalize**. \n",
    "\n",
    "Many methods have been developed and continue to be developed to deal with over-parameterized or **ill-posed** machine learning models. In particular, we will explore three methods for stabilizing over-parameterized models: \n",
    "\n",
    "- Stepwise regression, wherein features are eliminated from an over-parameterized model in a stepwise fashion. \n",
    "- Using a mathematical **regularization** technique, known as singular value decomposition, to determine the number of meaningful components for a model. We will explore use of singular value decomposition in another notebook. \n",
    "- Using **regularization** methods known as ridge, lasso, and elastic-net regression to stabilize over-parameterized models. We will explore ridge and lasso regression in this notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Load Packages\n",
    "\n",
    "Execute the code in the cell below to import the packages you will need for the rest of this notebook.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy.random as nr\n",
    "import statsmodels.formula.api as sm\n",
    "import scipy.stats as ss\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review of Eigenvalue Decomposition\n",
    "\n",
    "Before we address regularization we will review **eigendecomposition** of matrices. **Eigenvalues** are characteristic roots or characteristic values of a linear system of equations. The eigendecomposition is a factorization of the a matrix. \n",
    "\n",
    "Let's start with a **square matrix**, $A$:\n",
    "\n",
    "$$A = \n",
    "\\begin{bmatrix}\n",
    "   a_{11}  & a_{12} & \\dots & a_{1n} \\\\\n",
    "    a_{21}  & a_{22} & \\dots & a_{2n} \\\\\n",
    "    \\vdots &\\vdots &\\vdots & \\vdots \\\\\n",
    "    a_{n1} & a_{n2} &  \\dots & a_{nn}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Next define a vector, $x$: \n",
    "\n",
    "$$x = \n",
    "\\begin{bmatrix}\n",
    "   x_{1}\\\\\n",
    "    x_{2}\\\\\n",
    "    \\vdots\\\\\n",
    "    x_{n}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Then an **eigenvalue**, $\\lambda$ and **eigenvector**, $x$, of the matrix $A$ have the property: \n",
    "\n",
    "$$A x = \\lambda x$$\n",
    "\n",
    "Or,   \n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "   a_{11}  & a_{12} & \\dots & a_{1n} \\\\\n",
    "    a_{21}  & a_{22} & \\dots & a_{2n} \\\\\n",
    "    \\vdots &\\vdots &\\vdots & \\vdots \\\\\n",
    "    a_{n1} & a_{n2} &  \\dots & a_{nn}\n",
    "\\end{bmatrix}  \n",
    "\\begin{bmatrix}\n",
    "   x_{1}\\\\\n",
    "    x_{2}\\\\\n",
    "    \\vdots\\\\\n",
    "    x_{n}\n",
    "\\end{bmatrix} \n",
    "= \n",
    "\\lambda \n",
    "\\begin{bmatrix}\n",
    "   x_{1}\\\\\n",
    "    x_{2}\\\\\n",
    "    \\vdots\\\\\n",
    "    x_{n}\n",
    "\\end{bmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see that the eigenvalue, $\\lambda$, is a root of the matrix, $A$ we can rearrange the above as follows:   \n",
    "\n",
    "\\begin{align}\n",
    "Ax - \\lambda x &= 0 \\\\\n",
    "(A - I \\lambda) x &= 0\n",
    "\\end{align}\n",
    "\n",
    "Where, $I$ is the **identity matrix** of 1 on the diagonal and 0 elsewhere. These relationships can be written as follows:  \n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "   a_{11} - \\lambda  & a_{12} & \\dots & a_{1n} \\\\\n",
    "    a_{21}  & a_{22} - \\lambda  & \\dots & a_{2n} \\\\\n",
    "    \\vdots &\\vdots &\\vdots & \\vdots \\\\\n",
    "    a_{n1} & a_{n2} &  \\dots & a_{nn} - \\lambda \n",
    "\\end{bmatrix}  \n",
    "\\begin{bmatrix}\n",
    "   x_{1}\\\\\n",
    "    x_{2}\\\\\n",
    "    \\vdots\\\\\n",
    "    x_{n}\n",
    "\\end{bmatrix} \n",
    "= \n",
    "\\begin{bmatrix}\n",
    "   x_{1}\\\\\n",
    "    x_{2}\\\\\n",
    "    \\vdots\\\\\n",
    "    x_{n}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "The foregoing show that the eigenvalue, $\\lambda$, is a **root of the matrix, $A$**. \n",
    "\n",
    "For an $n\\ x\\ n$ matrix, $A$, there are $n$ eigenvalues or roots. These can be found by solving the following equation, using the determinant:  \n",
    "\n",
    "$$det(A - x) = 0$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using these eigenvalues we can factor a square $n\\ x\\ n$ matrix, $A$ into eigenvectors and eigenvalues. Represent the matrix of eigenvectors as $Q$ and the diagonal matrix of n eigenvalues as $\\Lambda$:\n",
    "\n",
    "$$A = Q \\Lambda Q^{-1}$$\n",
    "\n",
    "Where $Q^{-1}$ is the inverse of the eigenvector matrix. The eigenvectors have norm 1, makeing the eigenvector matrices, $Q$, **unitary**.  \n",
    "\n",
    "Using the eigenvalue-eigenvector decompositions, the inverse of a square $n\\ x\\ n$ matrix can be expressed:\n",
    "\n",
    "$$A^{-1} = Q \\Lambda^{-1} Q^{-1}$$  \n",
    "\n",
    "In fact, since the matrix of eigenvectors is unitary, we can compute the value of the matrix $A$ to the Nth power as:\n",
    "\n",
    "$$A^{N} = Q \\Lambda^{N} Q^{-1}$$\n",
    "\n",
    "You can find more information on the properties of eigenvalue-eigenvector decomposition in this [article](https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Regularization with Ridge Regression, L2\n",
    "\n",
    "So far, we have looked at two approached for dealing with over-parameterized models; feature selection by stepwise regression and singular value decomposition. In this section we will explore the most widely used regularization method for optimization-based machine learning models, ridge regression. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review of Normal Equations   \n",
    "\n",
    "Let's start by reviewing the **normal equation** formulation of the linear regression problem. The goal is to compute a vector of **model coefficients**, $b$, or weights which minimize the mean squared **residuals**, $\\epsilon$, given a vector of data $x$ and a **model matrix** $A$. We can write our model as:\n",
    "\n",
    "$$x = A b + \\epsilon$$\n",
    "\n",
    "To solve this problem we would ideally like to compute:\n",
    "\n",
    "$$b = A^{-1}x$$\n",
    "\n",
    "The commonly used Normal Equation form can help:\n",
    "\n",
    "$$b = (A^TA)^{-1}A^Tx$$\n",
    "\n",
    "Now, $A^TA$ is an $m x m$ matrix, and thus is of reduced dimension. But, **$A^TA$ can still be rank deficient!** \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covariance of the Model Matrix\n",
    "\n",
    "There is a simple and useful relationship between the **covariance matrix** of $A$ and $A^T A$:  \n",
    "\n",
    "$$cov(A) = \\frac{1}{n} A^T A$$\n",
    "\n",
    "The question is, how can we use the covariance matrix to evaluate our ability to find an invoice? The answer is to examine the eigenvalues of the covariance matrix.  \n",
    "\n",
    "A covariance matrix of a real valued matrix $A$ is **square** and **symmetric**. This means we can perform **eigendecomposition** on the covariance matrix. \n",
    "\n",
    "$$cov(A) = Q \\Lambda Q^{-1}$$\n",
    "\n",
    "where,   \n",
    "$Q$ is the unitary matrix of eigenvectors.\n",
    "$\\Lambda$ is the diagonal matrix of eigenvalues, defining the **spectrum** of the covariance matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The L2 Regularization Approach\n",
    "\n",
    "The basic idea of ridge regression is to stabilize the **inverse covariance matrix**, $D^+=(A^{T}A)^{-1}$, by **adding a small bias term**, $\\alpha$, to each of the eigenvalues. We can state this operation in matrix notation as follows. We start with a modified form of the normal equations (also know as the **L2 or Euclidean norm** minimization problem):\n",
    "\n",
    "$$min [\\parallel A \\cdot x - b \\parallel +\\ \\alpha \\parallel b\\parallel]\\\\  or \\\\\n",
    "b = (A^TA + \\alpha^2)^{-1}A^Tx$$\n",
    "\n",
    "But, what does it mean that we are using an L2 or Euclidean norm as for regularization? The answer is that we constrain the L2 norm values of the model coefficients using the **penalty term** $\\alpha \\parallel b\\parallel$. The norm of the model coefficients $\\parallel b\\parallel$ is the L2 norm of the model parameter values for $m$ model parameters:\n",
    "\n",
    "$$||b|| = \\big( b_1^2 + b_2^2 + \\ldots + b_m^2 \\big)^{\\frac{1}{2}} = \\Big( \\sum_{i=1}^m b_i^2 \\Big)^{\\frac{1}{2}}$$\n",
    "\n",
    "\n",
    "In this way, the values of small singular values do not blow up when we compute the inverse. You can see this by writing out the $\\Lambda^+$ matrix of the eigenvalues of the covariance matrix with the bias term.\n",
    "\n",
    "$$\\Lambda_{ridge}^+  = \\begin{bmatrix}\n",
    "    \\frac{1}{\\lambda_1 + \\alpha^2}  & 0 & 0 & \\dots & 0 \\\\\n",
    "    0  & \\frac{1}{\\lambda_2 + \\alpha^2} & 0 & \\dots & 0 \\\\\n",
    "    \\vdots &\\vdots &\\vdots & & \\vdots \\\\\n",
    "    0 & 0 & 0 & \\dots & \\frac{1}{\\lambda_m + \\alpha^2}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Adding this bias term creates a 'ridge' in the singular value matrix, giving this method its name **ridge regression**. \n",
    "\n",
    "You can also think of ridge regression as limiting the L2 or Euclidean norm of the values of the model coefficient vector. The value of $\\alpha$ determines how much the norm of the coefficient vector constrains the solution. You can see a view of this geometric interpretation in the figure below.  \n",
    "\n",
    "![](img/L2.jpg)\n",
    "<center> **Geometric view of L2 regularization**</center>\n",
    "    \n",
    "From the above figure, you can see that L2 regularization is a **soft constraint**. The overall Euclidean norm of the model parameter values is constrained. This soft constraint does not force any parameter value to exactly 0. \n",
    "\n",
    "The same method goes by some other names, as it seems to have been 'invented' several times. In particular, **Tikhonov regularization**, or **L2 norm regularization**. In all likelihood the method was first developed by the Russian mathematician Andrey Tikhonov in the late 1940's, and first published in English in 1977.\n",
    "\n",
    "![](img/Tikhonov_board.jpg)\n",
    "<center> **Commemorative plaque for Andrey Nikolayevich Tikhonov at Moscow State University**\n",
    "\n",
    "Let's give this a try. Execute the code in the cell below which computes the $(A^TA + \\lambda^2)^{-1}A^T$ matrix with a lambda value of `0.1`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of Regularized Covariance\n",
    "\n",
    "The foregoing is rather abstract. How can you understand the practical implications of l2 regularization? Let's try an exercise to compute and understand the eigenvalues of the covariance of the Gaulton families data. \n",
    "\n",
    "As a first step execute the code in the cell below to do the following:     \n",
    "- Load the dataset into a data frame.\n",
    "- Compute some features for the model.\n",
    "- Subset the data to male adult children. \n",
    "- Scale the features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(col):\n",
    "    mean_col = np.mean(col)\n",
    "    sd_col = np.std(col)\n",
    "    std = (col - mean_col) / sd_col\n",
    "    return std\n",
    "\n",
    "gaulton = pd.read_csv('GaltonFamilies.csv')\n",
    "\n",
    "gaulton.loc[:,'mother_sqr'] = np.square(gaulton.loc[:,'mother'])\n",
    "gaulton.loc[:,'father_sqr'] = np.square(gaulton.loc[:,'father'])\n",
    "gaulton.loc[:,'mother_father'] = np.multiply(gaulton.loc[:,'mother'],gaulton.loc[:,'father'])\n",
    "\n",
    "gaulton_male = gaulton.loc[gaulton.gender == 'male']\n",
    "\n",
    "for col in ['mother','father','midparentHeight','children','mother_sqr','father_sqr', 'mother_father']:\n",
    "    gaulton_male.loc[:,col] = scale(gaulton_male.loc[:,col])\n",
    "    \n",
    "gaulton_male.head(10)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in the cell below computes the covariance matrix of the features, or design matrix. Execute the code and examine the result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = gaulton_male.loc[:,['mother','father','midparentHeight','children','mother_sqr','father_sqr','mother_father']]\n",
    "\n",
    "C = np.dot(np.transpose(X),X)\n",
    "C = np.multiply(1.0/float(X.shape[0]), C)\n",
    "np.round(C,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that covariance matrix is symmetric about the diagonal. Also, notice the there are several off-diagonal terms with value of 1.0, indicating that inverting this matrix will be mathematically unstable. \n",
    "\n",
    "Execute the code in the cell below and examine the eigenvalues of the covariance matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L, Q = np.linalg.eig(C)\n",
    "np.round(L,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several of these eigenvalues are effectively 0, confirming the inverse of the covariance matrix will be unstable. \n",
    "\n",
    "The code in the cell below adds a small amount of bias to the covariance matrix. Execute the code and examine the result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.01\n",
    "for i in range(C.shape[0]):\n",
    "    C[i,i] = C[i,i] + alpha\n",
    "np.round(C,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your turn 1:** Complete and execute the code in the cell below to compute and display the eigenvalues of the biased covariance matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Complete the code below\n",
    "L_regularized, Q = \n",
    "np.round(L_regularized,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that none of these eigenvalues is 0 anymore. Apparently a small amount of bias has stabilized the inverse of the covariance matrix.   \n",
    "\n",
    "How different are these eigenvalues from the unbiased estimates? Execute the code in the cell below to find out.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(np.subtract(L_regularized,L),3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference in the eigenvalues is minimal and equal to the value of the bias added. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression with L2 Regularization\n",
    "\n",
    "We are ready to try an example. We will perform regression on the subset of the Gaulton family dataset. \n",
    "\n",
    "The statsmodels package allows us to compute a sequence of ridge regression solutions. The code in the cell below computes solutions for a series of values of $\\alpha$. The L1_wt argument of the fit_regularized method is set to 0.0, making this L2 regularized or ridge regression. \n",
    "\n",
    "The function in the cell below computes a regularized linear regression model for each of a series of values of the regularization parameter, $\\alpha$. For each value of $\\alpha$, the model parameters, or partial slope parameters, are stored in an array, which is returned. Execute this code and examine the first few values of the model coefficients as $\\alpha$ increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def regularized_coefs(df, alphas, L1_wt=0.0, n_coefs=7,\n",
    "                      formula = 'childHeight ~ mother + father + midparentHeight + children + mother_sqr + father_sqr + mother_father'):\n",
    "    '''Function that computes a linear model for each value of the regualarization \n",
    "    parameter alpha and returns an array of the coefficient values. The L1_wt \n",
    "    determines the trade-off between L1 and L2 regualarization'''\n",
    "    coefs = np.zeros((len(alphas),n_coefs + 1))\n",
    "    for i,alpha in enumerate(alphas):\n",
    "        temp_mod = sm.ols(formula, data=df).fit_regularized(alpha=alpha,L1_wt=L1_wt)\n",
    "        coefs[i,:] = temp_mod.params\n",
    "    return coefs\n",
    "\n",
    "alphas = np.arange(0.05, 20.0, step = 0.1)\n",
    "#formula = 'childHeight ~ mother + father + mother_sqr + father_sqr'\n",
    "coefs = regularized_coefs(gaulton_male, alphas)\n",
    "coefs[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first parameter, the intercept term, is the largest. Notice how all the parameter values decrease as $\\alpha$ increases. \n",
    "\n",
    "The function in the cell below plots each of the partial slopes vs. the regularization parameter. Execute this code and examine the plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_coefs(coefs, alphas, ylim=None):\n",
    "    ax = plt.figure(figsize=(6, 6)).gca() # define axis\n",
    "    for i in range(coefs.shape[1]): # Iterate over coefficients\n",
    "        ax.plot(alphas, coefs[:,i])\n",
    "    ax.axhline(0.0, color='red', linestyle='--', linewidth=0.5)\n",
    "    ax.set_ylabel('Partial slope values')\n",
    "    ax.set_xlabel('alpha')\n",
    "    ax.set_title('Parial slope values vs. regularization parameter number')\n",
    "    if ylim is not None: ax.set_ylim(ylim)\n",
    "\n",
    "plot_coefs(coefs, alphas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is one parameter, the intercept, which is far from zero. The other parameters all have values near zero. \n",
    "\n",
    "Keep in mind that the height of the adult male children regresses to the mean, which is the intercept value. Initially, the intercept value is close to the mean of the heights. As the regularization parameter increases, so does the bias.  \n",
    "\n",
    "Execute the code in the cell below to examine the smaller model parameters.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_coefs(coefs, alphas, ylim=(-0.25,0.4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that as the regularization parameter increases, the model parameters are driven toward 0. The trend is smooth, consistent with the soft constraint property of L2 regularization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso regression. L1\n",
    "\n",
    "We can also do regularization using other norms. **Lasso or L1 regularization** limits the sum of the absolute values of the model coefficients. The L1 norm is sometime know as the **Manhattan norm**, since distance is measured as if you were traveling on a rectangular grid of streets. \n",
    "\n",
    "The L1 regularized linear model can be written as:\n",
    "\n",
    "$$min [\\parallel A \\cdot x - b \\parallel +\\ \\alpha \\parallel b\\parallel^1]$$\n",
    "\n",
    "This looks a lot like the L2 regularization formulation. However, the norm of the model parameters is now, $\\parallel b\\parallel^1$, indicating an L1 norm. We can compute the l1 norm of the $m$ model parameters:\n",
    "\n",
    "$$||b||^1 = \\big( |b_1| + |b_2| + \\ldots + |b_m| \\big) = \\Big( \\sum_{i=1}^m |b_i| \\Big)^1$$\n",
    "\n",
    "where $|b_i|$ is the absolute value of $b_i$. \n",
    "\n",
    "You can also think of lasso regression as limiting the L1 norm of the values of the model coefficients. The value of $\\alpha$ determines how much the norm of the coefficient values constrains the solution. You can see a view of this geometric interpretation in the figure below.  \n",
    "\n",
    "![](img/L1.jpg)\n",
    "<center> Geometric view of L1 regularization </center>\n",
    "\n",
    "The L1 norm is a **hard constraint** on the parameter values. The L1 norm will force model parameter values to 0. \n",
    "\n",
    "The behavior of the L1 norm means that the solution of the regression problem is no longer linear. This makes the computation of lasso models more computationally intensive than ridge regression or L2 regularized models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of L1 Regularization\n",
    "\n",
    "With some theory in mind, it is time to try an example of L1 regularization. The code in the cell below uses the same function as before to create an array of model parameter values as the regularization parameter increases. Execute this code and examine the head of the model parameter array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = np.arange(0.05, 2.0, step = 0.02)\n",
    "coefs = regularized_coefs(gaulton_male, alphas, L1_wt=1.0)\n",
    "coefs[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first coefficient in each row is the intercept term. The less constrained values are close to the mean height of the adult male children, as expected.    \n",
    "\n",
    "Now, execute the code in the cell below to plot the parameter values vs. the regularization parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_coefs(coefs, alphas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The intercept parameter decreases more slowly than for L2 regularization. The other parameter values are close to zero.  \n",
    "\n",
    "Execute the code in the cell below to display the other model parameter values as the regularization parameter increases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_coefs(coefs, alphas, ylim=(-0.25,1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that model coefficients are much more tightly constrained than for L2 regularization. Notice how the model parameters are driven to zero as the regularization parameter increases. This is typical of L1 or lasso regression as an example of the hard constraint property.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elastic Net Regression\n",
    "\n",
    "DO you always need to choose between the extremes of L2 and L1 regularization? The **elastic net** algorithm uses a weighted combination of L2 and L1 regularization. As you have likely noticed, the same function is used for Lasso and Ridge regression with only the `L1_wt` argument changing. This argument determines the how much weight goes to the L1-norm of the partial slopes. If `L1_wt = 0`, the regularization is pure L2 (Ridge) and if `L1_wt = 1.0` the regularization is pure L1 (Lasso).\n",
    "\n",
    "**Your turn 2:** Complete the code in the cell giving equal weight to each regression method. Execute this code and examine the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = np.arange(0.01, 5.0, step = 0.02)\n",
    "## Complete the code below\n",
    "coefs = \n",
    "coefs[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the L2 and L1 regularizations the intercept parameter is the largest. All other modle parameters are relatively small. \n",
    "\n",
    "Execute the code in the cell below to display the model parameters vs. the elastic net regularization parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_coefs(coefs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The intercept values decreases with the regularization parameter in a manner similar to L2 regularization.  \n",
    "\n",
    "Now, execute the code in the cell below display how the other model parameters change with the regularization parameter.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_coefs(coefs,ylim=(-0.25,1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These model parameters are all being forced to 0. However, they decrease in a mostly smooth manner as the regularization parameter increases. But, there are also jumps in values of some parameters as other parameters are forced to 0. This behavior is a combination of what was observed for L2 and L1 regularizations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copyright 2017, 2018, 2019, 2020 Stephen F Elston. All rights reserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
