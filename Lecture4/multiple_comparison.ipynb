{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dangers of Multiple Comparisons\n",
    "\n",
    "Testing multiple hypothesis from the same data can be problematic. Exhaustively testing all pairwise relationships between variables in a data set is a commonly used, but generally misleading from of multiple comparisons. The chance of finding false significance, using such a **data dredging** approach, can be surprisingly high. \n",
    "\n",
    "In this exercise you will perform multiple comparisons on 20 **identically distributed independent (iid)** variables. Ideally, such tests should not find significant relationships, but the actual result is quite different. \n",
    "\n",
    "To get started, execute the code in the cell below to load the required packages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy.random as nr\n",
    "from scipy.stats import ttest_ind, f_oneway\n",
    "from itertools import product, combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise you will apply a t-test to all pairwise combinations of identical Normally distributed variables. In this case, we will create a data set with 20 iid Normal distributions of 1000 samples each. Execute the code in the cell below to find this data and display the mean and variance of each variable.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The means of the columns are\n",
      " [-1.16191649e-01  2.80829317e-02 -1.78516419e-02 -1.44691489e-02\n",
      "  3.03718152e-02  1.20007442e-02 -9.58845606e-05  1.98662580e-03\n",
      "  4.94154934e-02 -4.11640866e-02 -6.32977862e-03 -5.93868192e-02\n",
      " -2.56373595e-02  1.43568791e-02 -1.44725765e-02 -1.37023955e-02\n",
      "  1.80622439e-02  5.87029691e-02 -2.02650514e-02 -1.56346106e-02]\n",
      "\n",
      "The variances of the columns are\n",
      " [0.94834508 1.04744241 1.0258018  0.96977571 1.0089001  1.04113864\n",
      " 1.00657222 0.99192594 1.04713487 1.04329434 1.04023108 0.96791346\n",
      " 1.03706907 1.07179865 1.01431404 1.05060289 1.02054329 0.9686211\n",
      " 1.02810287 0.99521555]\n"
     ]
    }
   ],
   "source": [
    "ncolumns = 20\n",
    "nr.seed(234)\n",
    "normal_vars = nr.normal(size=(1000,ncolumns))\n",
    "print('The means of the columns are\\n', np.mean(normal_vars, axis = 0))\n",
    "print('\\nThe variances of the columns are\\n', np.var(normal_vars, axis = 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that means and variances are close to 0.0 and 1.0. As expected, there is not much difference between these variables. \n",
    "\n",
    "Now for each pair of variables we will compute the t-statistic and p-value and append them to lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1\n",
      "0 2\n",
      "0 3\n",
      "0 4\n",
      "0 5\n",
      "0 6\n",
      "0 7\n",
      "0 8\n",
      "0 9\n",
      "0 10\n",
      "0 11\n",
      "0 12\n",
      "0 13\n",
      "0 14\n",
      "0 15\n",
      "0 16\n",
      "0 17\n",
      "0 18\n",
      "0 19\n",
      "1 0\n",
      "1 2\n",
      "1 3\n",
      "1 4\n",
      "1 5\n",
      "1 6\n",
      "1 7\n",
      "1 8\n",
      "1 9\n",
      "1 10\n",
      "1 11\n",
      "1 12\n",
      "1 13\n",
      "1 14\n",
      "1 15\n",
      "1 16\n",
      "1 17\n",
      "1 18\n",
      "1 19\n",
      "2 0\n",
      "2 1\n",
      "2 3\n",
      "2 4\n",
      "2 5\n",
      "2 6\n",
      "2 7\n",
      "2 8\n",
      "2 9\n",
      "2 10\n",
      "2 11\n",
      "2 12\n",
      "2 13\n",
      "2 14\n",
      "2 15\n",
      "2 16\n",
      "2 17\n",
      "2 18\n",
      "2 19\n",
      "3 0\n",
      "3 1\n",
      "3 2\n",
      "3 4\n",
      "3 5\n",
      "3 6\n",
      "3 7\n",
      "3 8\n",
      "3 9\n",
      "3 10\n",
      "3 11\n",
      "3 12\n",
      "3 13\n",
      "3 14\n",
      "3 15\n",
      "3 16\n",
      "3 17\n",
      "3 18\n",
      "3 19\n",
      "4 0\n",
      "4 1\n",
      "4 2\n",
      "4 3\n",
      "4 5\n",
      "4 6\n",
      "4 7\n",
      "4 8\n",
      "4 9\n",
      "4 10\n",
      "4 11\n",
      "4 12\n",
      "4 13\n",
      "4 14\n",
      "4 15\n",
      "4 16\n",
      "4 17\n",
      "4 18\n",
      "4 19\n",
      "5 0\n",
      "5 1\n",
      "5 2\n",
      "5 3\n",
      "5 4\n",
      "5 6\n",
      "5 7\n",
      "5 8\n",
      "5 9\n",
      "5 10\n",
      "5 11\n",
      "5 12\n",
      "5 13\n",
      "5 14\n",
      "5 15\n",
      "5 16\n",
      "5 17\n",
      "5 18\n",
      "5 19\n",
      "6 0\n",
      "6 1\n",
      "6 2\n",
      "6 3\n",
      "6 4\n",
      "6 5\n",
      "6 7\n",
      "6 8\n",
      "6 9\n",
      "6 10\n",
      "6 11\n",
      "6 12\n",
      "6 13\n",
      "6 14\n",
      "6 15\n",
      "6 16\n",
      "6 17\n",
      "6 18\n",
      "6 19\n",
      "7 0\n",
      "7 1\n",
      "7 2\n",
      "7 3\n",
      "7 4\n",
      "7 5\n",
      "7 6\n",
      "7 8\n",
      "7 9\n",
      "7 10\n",
      "7 11\n",
      "7 12\n",
      "7 13\n",
      "7 14\n",
      "7 15\n",
      "7 16\n",
      "7 17\n",
      "7 18\n",
      "7 19\n",
      "8 0\n",
      "8 1\n",
      "8 2\n",
      "8 3\n",
      "8 4\n",
      "8 5\n",
      "8 6\n",
      "8 7\n",
      "8 9\n",
      "8 10\n",
      "8 11\n",
      "8 12\n",
      "8 13\n",
      "8 14\n",
      "8 15\n",
      "8 16\n",
      "8 17\n",
      "8 18\n",
      "8 19\n",
      "9 0\n",
      "9 1\n",
      "9 2\n",
      "9 3\n",
      "9 4\n",
      "9 5\n",
      "9 6\n",
      "9 7\n",
      "9 8\n",
      "9 10\n",
      "9 11\n",
      "9 12\n",
      "9 13\n",
      "9 14\n",
      "9 15\n",
      "9 16\n",
      "9 17\n",
      "9 18\n",
      "9 19\n",
      "10 0\n",
      "10 1\n",
      "10 2\n",
      "10 3\n",
      "10 4\n",
      "10 5\n",
      "10 6\n",
      "10 7\n",
      "10 8\n",
      "10 9\n",
      "10 11\n",
      "10 12\n",
      "10 13\n",
      "10 14\n",
      "10 15\n",
      "10 16\n",
      "10 17\n",
      "10 18\n",
      "10 19\n",
      "11 0\n",
      "11 1\n",
      "11 2\n",
      "11 3\n",
      "11 4\n",
      "11 5\n",
      "11 6\n",
      "11 7\n",
      "11 8\n",
      "11 9\n",
      "11 10\n",
      "11 12\n",
      "11 13\n",
      "11 14\n",
      "11 15\n",
      "11 16\n",
      "11 17\n",
      "11 18\n",
      "11 19\n",
      "12 0\n",
      "12 1\n",
      "12 2\n",
      "12 3\n",
      "12 4\n",
      "12 5\n",
      "12 6\n",
      "12 7\n",
      "12 8\n",
      "12 9\n",
      "12 10\n",
      "12 11\n",
      "12 13\n",
      "12 14\n",
      "12 15\n",
      "12 16\n",
      "12 17\n",
      "12 18\n",
      "12 19\n",
      "13 0\n",
      "13 1\n",
      "13 2\n",
      "13 3\n",
      "13 4\n",
      "13 5\n",
      "13 6\n",
      "13 7\n",
      "13 8\n",
      "13 9\n",
      "13 10\n",
      "13 11\n",
      "13 12\n",
      "13 14\n",
      "13 15\n",
      "13 16\n",
      "13 17\n",
      "13 18\n",
      "13 19\n",
      "14 0\n",
      "14 1\n",
      "14 2\n",
      "14 3\n",
      "14 4\n",
      "14 5\n",
      "14 6\n",
      "14 7\n",
      "14 8\n",
      "14 9\n",
      "14 10\n",
      "14 11\n",
      "14 12\n",
      "14 13\n",
      "14 15\n",
      "14 16\n",
      "14 17\n",
      "14 18\n",
      "14 19\n",
      "15 0\n",
      "15 1\n",
      "15 2\n",
      "15 3\n",
      "15 4\n",
      "15 5\n",
      "15 6\n",
      "15 7\n",
      "15 8\n",
      "15 9\n",
      "15 10\n",
      "15 11\n",
      "15 12\n",
      "15 13\n",
      "15 14\n",
      "15 16\n",
      "15 17\n",
      "15 18\n",
      "15 19\n",
      "16 0\n",
      "16 1\n",
      "16 2\n",
      "16 3\n",
      "16 4\n",
      "16 5\n",
      "16 6\n",
      "16 7\n",
      "16 8\n",
      "16 9\n",
      "16 10\n",
      "16 11\n",
      "16 12\n",
      "16 13\n",
      "16 14\n",
      "16 15\n",
      "16 17\n",
      "16 18\n",
      "16 19\n",
      "17 0\n",
      "17 1\n",
      "17 2\n",
      "17 3\n",
      "17 4\n",
      "17 5\n",
      "17 6\n",
      "17 7\n",
      "17 8\n",
      "17 9\n",
      "17 10\n",
      "17 11\n",
      "17 12\n",
      "17 13\n",
      "17 14\n",
      "17 15\n",
      "17 16\n",
      "17 18\n",
      "17 19\n",
      "18 0\n",
      "18 1\n",
      "18 2\n",
      "18 3\n",
      "18 4\n",
      "18 5\n",
      "18 6\n",
      "18 7\n",
      "18 8\n",
      "18 9\n",
      "18 10\n",
      "18 11\n",
      "18 12\n",
      "18 13\n",
      "18 14\n",
      "18 15\n",
      "18 16\n",
      "18 17\n",
      "18 19\n",
      "19 0\n",
      "19 1\n",
      "19 2\n",
      "19 3\n",
      "19 4\n",
      "19 5\n",
      "19 6\n",
      "19 7\n",
      "19 8\n",
      "19 9\n",
      "19 10\n",
      "19 11\n",
      "19 12\n",
      "19 13\n",
      "19 14\n",
      "19 15\n",
      "19 16\n",
      "19 17\n",
      "19 18\n"
     ]
    }
   ],
   "source": [
    "ttest_results = []\n",
    "p_values = []\n",
    "for i,j in product(range(ncolumns), range(ncolumns)):\n",
    "    if(i != j): # We only want to test between different samples \n",
    "        print(i,j)\n",
    "        t1, t2 = ttest_ind(normal_vars[:,i], normal_vars[:,j])\n",
    "        ttest_results.append(t1)\n",
    "        p_values.append(t2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many of these t-tests will show **significance** at the 0.05 cut-off level? There are 380 pairwise combinations, so we expect to find a number of falsely significant test results at this level. To find out, complete and execute the code in the cell below to filter the test results and print those that show significance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t-test with SIGNIFICANT, t-statistic =  -3.23  and p-value =  0.0013\n",
      "t-test with SIGNIFICANT, t-statistic =  -2.21  and p-value =  0.0271\n",
      "t-test with SIGNIFICANT, t-statistic =  -2.32  and p-value =  0.0204\n",
      "t-test with SIGNIFICANT, t-statistic =  -3.31  and p-value =  0.0009\n",
      "t-test with SIGNIFICANT, t-statistic =  -2.87  and p-value =  0.0041\n",
      "t-test with SIGNIFICANT, t-statistic =  -2.62  and p-value =  0.0087\n",
      "t-test with SIGNIFICANT, t-statistic =  -2.68  and p-value =  0.0074\n",
      "t-test with SIGNIFICANT, t-statistic =  -3.71  and p-value =  0.0002\n",
      "t-test with SIGNIFICANT, t-statistic =  -2.46  and p-value =  0.0139\n",
      "t-test with SIGNIFICANT, t-statistic =  -2.03  and p-value =  0.0424\n",
      "t-test with SIGNIFICANT, t-statistic =  -2.9  and p-value =  0.0037\n",
      "t-test with SIGNIFICANT, t-statistic =  -2.29  and p-value =  0.0218\n",
      "t-test with SIGNIFICANT, t-statistic =  -2.29  and p-value =  0.0221\n",
      "t-test with SIGNIFICANT, t-statistic =  -3.02  and p-value =  0.0025\n",
      "t-test with SIGNIFICANT, t-statistic =  -3.99  and p-value =  0.0001\n",
      "t-test with SIGNIFICANT, t-statistic =  -2.16  and p-value =  0.0312\n",
      "t-test with SIGNIFICANT, t-statistic =  -2.28  and p-value =  0.0227\n",
      "t-test with SIGNIFICANT, t-statistic =  3.23  and p-value =  0.0013\n",
      "t-test with SIGNIFICANT, t-statistic =  2.21  and p-value =  0.0271\n",
      "t-test with SIGNIFICANT, t-statistic =  2.32  and p-value =  0.0204\n",
      "t-test with SIGNIFICANT, t-statistic =  3.31  and p-value =  0.0009\n",
      "t-test with SIGNIFICANT, t-statistic =  2.02  and p-value =  0.0437\n",
      "t-test with SIGNIFICANT, t-statistic =  2.87  and p-value =  0.0041\n",
      "t-test with SIGNIFICANT, t-statistic =  2.62  and p-value =  0.0087\n",
      "t-test with SIGNIFICANT, t-statistic =  2.68  and p-value =  0.0074\n",
      "t-test with SIGNIFICANT, t-statistic =  3.71  and p-value =  0.0002\n",
      "t-test with SIGNIFICANT, t-statistic =  1.98  and p-value =  0.0478\n",
      "t-test with SIGNIFICANT, t-statistic =  2.42  and p-value =  0.0155\n",
      "t-test with SIGNIFICANT, t-statistic =  -1.98  and p-value =  0.0478\n",
      "t-test with SIGNIFICANT, t-statistic =  -2.23  and p-value =  0.0262\n",
      "t-test with SIGNIFICANT, t-statistic =  2.46  and p-value =  0.0139\n",
      "t-test with SIGNIFICANT, t-statistic =  -2.02  and p-value =  0.0437\n",
      "t-test with SIGNIFICANT, t-statistic =  -2.42  and p-value =  0.0155\n",
      "t-test with SIGNIFICANT, t-statistic =  -2.68  and p-value =  0.0074\n",
      "t-test with SIGNIFICANT, t-statistic =  2.03  and p-value =  0.0424\n",
      "t-test with SIGNIFICANT, t-statistic =  2.9  and p-value =  0.0037\n",
      "t-test with SIGNIFICANT, t-statistic =  2.29  and p-value =  0.0218\n",
      "t-test with SIGNIFICANT, t-statistic =  2.29  and p-value =  0.0221\n",
      "t-test with SIGNIFICANT, t-statistic =  3.02  and p-value =  0.0025\n",
      "t-test with SIGNIFICANT, t-statistic =  3.99  and p-value =  0.0001\n",
      "t-test with SIGNIFICANT, t-statistic =  2.23  and p-value =  0.0262\n",
      "t-test with SIGNIFICANT, t-statistic =  2.68  and p-value =  0.0074\n",
      "t-test with SIGNIFICANT, t-statistic =  2.16  and p-value =  0.0312\n",
      "t-test with SIGNIFICANT, t-statistic =  2.28  and p-value =  0.0227\n",
      "\n",
      "Number of falsely significant tests =  44\n"
     ]
    }
   ],
   "source": [
    "signifiance_level = 0.05\n",
    "def find_significant(p_values, ttest_results, signifiance_level):\n",
    "    n_cases = 0\n",
    "    for i in range(len(p_values)):\n",
    "        if(p_values[i] < signifiance_level): \n",
    "            n_cases += 1\n",
    "            print('t-test with SIGNIFICANT, t-statistic = ', round(ttest_results[i],2), ' and p-value = ', round(p_values[i],4))\n",
    "    print('\\nNumber of falsely significant tests = ', n_cases)        \n",
    "find_significant(p_values, ttest_results, signifiance_level)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the large number of apparently significant tests. Do you trust these results to show any important relationships in the data? \n",
    "\n",
    "Can the Bonforoni correction help? Execute the code in the cell below to apply the Bonforoni adjusted significance level to the p-value and ttest data.  \n",
    "\n",
    "> ### Bonfirroni correction  \n",
    "> Several adjustments to the multiple comparisons problem have been proposed. In 1979 Holm published a method know as the **Bonfirroni correction**. The adjustment is simple:\n",
    "$$\\alpha_b = \\frac{\\alpha}{m}\\\\\n",
    "with\\\\ \n",
    "m =\\ number\\ of\\ groups$$\n",
    "> The problem with the Bonfirroni correction is the reduction in power as the  grows smaller. For big data problems with large numbers of groups, this issue can be especially serious. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With Bonforoni correction the significance level is now =  0.00013157894736842105\n",
      "t-test with SIGNIFICANT, t-statistic =  -3.99  and p-value =  0.0001\n",
      "t-test with SIGNIFICANT, t-statistic =  3.99  and p-value =  0.0001\n",
      "\n",
      "Number of falsely significant tests =  2\n"
     ]
    }
   ],
   "source": [
    "signifiance_bonforoni = signifiance_level/380.0\n",
    "print('With Bonforoni correction the significance level is now = ', signifiance_bonforoni)\n",
    "find_significant(p_values, ttest_results, signifiance_bonforoni)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even with the Bonforoni correction we have some false significance tests, if only just barely!\n",
    "\n",
    "But, can we detect small effect with Bonforoni correction, as this method significantly reduces power of tests? Execute the code in the cell below, which compares a standard Normal to a Normal with a small mean (effect size), to find out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_indResult(statistic=array([-2.49553488]), pvalue=array([0.01265684]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nr.seed(567)\n",
    "ttest_ind(normal_vars[:,0], nr.normal(loc = 0.01, size=(1000,1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the Bonforoni correction, this difference in means would not be found significant. This illustrates the downside of the correction, which may prevent detection of significant effects, while still finding false significance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Copyright 2020, Stephen F. Elston. All rights reserved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
