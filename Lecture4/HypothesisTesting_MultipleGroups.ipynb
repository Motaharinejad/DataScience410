{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis Testing for Multiple Groups\n",
    "\n",
    "### Data Science 410"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Testing Multiple Groups and ANOVA\n",
    "\n",
    "So far, we have only looked at tests for comparing two samples. What if we have multiple groups and want to compare their means? Why can’t we just do multiple two-sample t-tests for all pairs?\n",
    "- Results in increased probability of accepting a false hypothesis.\n",
    "- For example, if we had 7 groups, there would be (7 Choose 2)=21 pairs to test.  If our alpha cutoff is 5%, then we are likely to accept about 1 false hypothesis (approximately 21*0.05).\n",
    "\n",
    "There is another alternative:\n",
    "\n",
    "- Null Hypothesis: All groups are samples from the same population.\n",
    "- Alternative Hypothesis: At least one group has a statistically different mean.\n",
    "\n",
    "This type of analysis is called “ANalysis Of VAriance”, or ANOVA. ANOVA is one of a large family of models used for **experimental design**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brief History of ANOVA\n",
    "\n",
    "ANOVA is not a new idea. \n",
    "\n",
    "- Laplace pioneered multiple comparison methods in 1827.\n",
    "- Ronald A Fisher published seminal work in 1922, 1925 and 1935. The F (Fisher) statistic is named in his honor.\n",
    "\n",
    "Fisher pioneered the use of linear models for testing multiple groups. Key methods he developed are ANOVA and the design of experiments. \n",
    "\n",
    "<img src=\"img/Ronald_Fisher.jpg\" alt=\"Drawing\" style=\"width:275x; height:350px\"/>\n",
    "\n",
    "<center>Ronald A. Fisher, another scary looking statistics professor!</center>   \n",
    "\n",
    "Fisher had an overwhelming influence on the theory of classical (frequentist) statistics in the 20th Century. He was vehemently opposed to Bayesian methods, and ostracized any practitioners. In fact, Fisher's long shadow explains why we are only beginning to teach Bayesian methods in the 21st century. Unfortunately, as with Pearson, Fisher was also a eugenicist and a racist. Another serious blemish on the early history of statistics. \n",
    "\n",
    "Fisher's two books are still influential and in print. \n",
    "\n",
    "<img src=\"img/Fisher1.jpg\" alt=\"Drawing\" style=\"width:400x; height:350px\"/>\n",
    "\n",
    "<img src=\"img/Fisher2.jpg\" alt=\"Drawing\" style=\"width:400x; height:350px\"/>\n",
    "\n",
    "<center>Fisher's books of 1935 and 1925</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic ANOVA Theory \n",
    "\n",
    "Let's have a look at how we would perform the comparisons between the multiple groups of data. The groups have each been subject to a different treatment. This method is known as **one-way ANOVA**.  \n",
    "\n",
    "The general idea is to determine if the groups within the data set all have the same variance. In other words, did the different treatments lead to significantly different variances within the groups? \n",
    "\n",
    "The differences in variance is measured by the **F statistic**. The F-statistic is defined by the ratio: \n",
    "\n",
    "$$F = \\frac{Variance\\ between\\ treatments}{Variance\\ within\\ treatments}$$  \n",
    "\n",
    "This ratio will be close to 1.0 if the treatments did exhibit a significant effect. On the other hand, if the treatment has a significant effect on **at least one of the groups**, the F statistic will be $\\gt 1$. \n",
    "\n",
    "The ratio of variances, or F statistic, follows an F distribution. There are two parameters which are the **degrees of freedom**. The variance between treatments and the variance within treatments each have a different degrees of freedom. For the F distribution of one-way ANOVA the degrees of freedom can be written:\n",
    "\n",
    "\\begin{align}\n",
    "degrees\\ of\\ freedom\\ between\\ treatments\\ &= DFT = I - 1 \\\\\n",
    "degrees\\ of\\ freedom\\ within\\ treatments\\ &= DFE = n - I\n",
    "\\end{align}\n",
    "\n",
    "Where;\n",
    "\n",
    "\\begin{align}\n",
    "I &= number\\ of\\ treatments\\ or\\ groups \\\\\n",
    "n &= total\\ number\\ of\\ subjects\\ or\\ samples\n",
    "\\end{align}\n",
    "\n",
    "The shape of the F distribution is defined by these two degrees of freedom, as shown in the figure below. Notice how the distribution becomes more symmetric and peaked as the degrees of freedom increases. \n",
    "\n",
    "<img src=\"img/F_Distribution.jpg\" alt=\"Drawing\" style=\"width:300x; height:350px\"/>\n",
    "<center>F distribution for different degrees of freedom</center>\n",
    "\n",
    "The null distribution is that the treatments have had no significant effect. The p-value is computed and compared to the cutoff value using the F distribution, given the two degree of freedom parameters.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing an ANOVA Table\n",
    "\n",
    "Comparisons between the multiple groups is traditionally laid out using an **ANOVA table**. Here we will construct the elements of this table piece by piece.  \n",
    "\n",
    "First, we make data independence and Normality assumptions about the groups. Then define:\n",
    "\n",
    "\\begin{align}\n",
    "I &= number\\ of\\ treatments\\\\\n",
    "n &= number\\ of\\ data\\ or\\ samples\\\\\n",
    "SS &= sum\\ of\\ squares\n",
    "\\end{align}\n",
    "\n",
    "We can calculate the following **sum of squares** quantities:\n",
    "\n",
    "\\begin{align}\n",
    "SST &= SS\\ Treatment\\\\\n",
    "SSE &= SS\\ Error\\ within\\ groups\\\\\n",
    "SSTotal &= SST + SSE\n",
    "\\end{align}\n",
    "\n",
    "Further, \n",
    "\n",
    "\\begin{align}\n",
    "DFT &= degrees\\ of\\ freedom\\ between\\ Treatment\\ groups = I - 1\\\\\n",
    "DFE &= degrees\\ of\\ freedom\\ within\\ treatment\\ groups = n - 1\\\\\n",
    "DFTotoal &= DFT + DFE = (I-1) + (n-I) = n -1\n",
    "\\end{align}\n",
    "\n",
    "And,\n",
    "\n",
    "\\begin{align}\n",
    "MST &= mean\\ square\\ error\\ Treatment\\\\\n",
    "MSE &= mean\\ square\\ error\\ within\\ groups\n",
    "\\end{align}\n",
    "\n",
    "Finally we can compute the F statistic with DFT and DFE degrees of freedom:\n",
    "\n",
    "$$F = \\frac{Variance\\ between\\ treatments}{Variance\\ within\\ treatments} = \\frac{MST}{MSE} =  \\frac{\\frac{SST}{DFT}}{\\frac{SSE}{DFE}}$$\n",
    "\n",
    "Using the F statistic on the degrees of freedom we can compute the p-values of the test. Using the significance of the test is determined significance with respect to the cutoff value. We can lay these results out in an ANOVA table as follows:\n",
    "\n",
    "|Type|Sum of Squares|df|Mean Square E|F|Significance|\n",
    "|---|---|---|---|---|---|\n",
    "|Between Groups|SST|DFT|SST/DFT|F Statistic| p-value|\n",
    "|Within Groups|SSE|DFE|SSE/DFE|||\n",
    "|Groups Total|SSTotal|DFTotal||||"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANOVA Example\n",
    "\n",
    "Let's start with an example with 4 groups. In Fisher's experimental design terminology we say we have data arises from 4 **treatments**. The code in the cell below computes the data for each of the 4 treatments. Run the code in the cell below and examine the difference in the box plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as nr\n",
    "import pandas as pd\n",
    "import scipy.stats as ss\n",
    "import statsmodels.stats.power as ssp\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr.seed(335566)\n",
    "df1 = nr.normal(size = 50).tolist()\n",
    "df2 = nr.normal(size = 50).tolist()\n",
    "df3 = nr.normal(loc = 0.5, size = 60).tolist()\n",
    "df4 = nr.normal(size = 40).tolist()\n",
    "\n",
    "\n",
    "plt.boxplot([df1, df2, df3, df4])\n",
    "plt.ylabel('Value')\n",
    "plt.xlabel('Variable')\n",
    "plt.title('Box plot of variables')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot shows variation between the distributions of the four variables. The question is, are these differences significant. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in the cell below applies uses the [scipy.stats.f_oneway](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.f_oneway.html) function, to perform the one-way ANOVA on the data from the 4 treatment groups. This function computes an F-Statistic and a p-value. Run this code and examine the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_statistic, p_value = ss.f_oneway(df1, df2, df3, df4)\n",
    "print('F statistic = ' + str(f_statistic))\n",
    "print('P-value = ' + str(p_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The F-Statistic is fairly large and the p-value is small. We can reject the null hypothesis that the 4 variables have the same mean. These treatment groups are unlikely to have arisen from the null distribution that all treatments had no effect. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Power of the Test\n",
    "\n",
    "There is also the question of the power of this ANOVA test. In other words, what is the probability that we will detect a difference in means? \n",
    "\n",
    "The code in the cell below uses the [statsmodels.stats.power.FTestAnovaPower.solve_power](https://www.statsmodels.org/stable/generated/statsmodels.stats.power.FTestAnovaPower.solve_power.html) function to compute power for mean differences in the range $\\{ 0.1, 1.0 \\}$. The power is plotted  against the mean difference. To be conservative, we are using the smallest number of samples for the variables as the number of observations, nobs. Execute this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_power(x, y, xlabel, title):\n",
    "    plt.plot(x, y, color = 'red', linewidth = 2)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel('Power')\n",
    "\n",
    "diffs = np.arange(start = 0.1, stop = 1.0, step = 0.01) \n",
    "powers = ssp.FTestAnovaPower().solve_power(effect_size = diffs, nobs=40, alpha=0.05)\n",
    "plot_power(diffs, powers, xlabel = 'Difference', title = 'Power vs. difference') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that even with 40 observations, the probability of detecting a farily small difference in means between the groups is quite high. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your turn:** In a hypothetical example, a new manager at an auto dealership observes changes in the average daily total sales by day of the week. She has collected the daily sales data by day for the past 8 weeks. She wants to know if these differences are significant or just from random variation?\n",
    "\n",
    "To solve the problem you will do the following:\n",
    "1. Execute the code in the cell provided below to compute some simulated data values by day of the week and display a box plot. The parameters for the Normal distributions for each day of the week are based on the average sales for each day and the standard deviation of sales over the month. In other words, we are assuming the variance is constant over the days of the week.\n",
    "2. In the next cell compute and display the F-Statistic and p-value for this sample. Is this p-value significant with a 0.05 cutoff? \n",
    "3. In the third cell compute the power of this test with the  following parameters:\n",
    "  - Range of differences from 1.0 to 10 in steps of 0.1. \n",
    "  - To display the plot of power vs. dollars, you must scale these differences by 50,000, the scale (standard deviation) of the Normal distribution of these data. Do this after you have computed the power values. This process is necessary since the manager will want to see the results in units she understands, dollars. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nr.seed(3356)\n",
    "Mon = nr.normal(loc = 400000, scale = 50000, size = 8).tolist()\n",
    "Tue = nr.normal(loc = 405000, scale = 50000, size = 8).tolist()\n",
    "Wed = nr.normal(loc = 415000, scale = 50000, size = 8).tolist()\n",
    "Thr = nr.normal(loc = 420000, scale = 50000, size = 8).tolist()\n",
    "Fri = nr.normal(loc = 440000, scale = 50000, size = 8).tolist()\n",
    "Sat = nr.normal(loc = 455000, scale = 50000, size = 8).tolist()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.boxplot([Mon,Tue,Wed,Thr,Fri,Sat])\n",
    "plt.ylabel('Value')\n",
    "plt.xlabel('Variable')\n",
    "plt.title('Box plot of variables')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eamine the results of you analysis and answer the following questions:\n",
    "1. Is the difference between the sales on the different days statistically significant at the 95% level? \n",
    "2. For a price difference of $100,000\\ and\\ \\$200,000 what is the approximate power of this test? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Turkey's ANOVA: Telling Groups Apart\n",
    "\n",
    "From the above ANOVA results we know that there is some difference in the means of these variables. However, the **ANOVA does not tell us which variable is significantly different**. From the box plot of the first example, we could guess it that group 3, has the greatest difference with respect to group 2, but we really don't know. ANOVA cannot tell us this. \n",
    "\n",
    "In 1949 John Tukey proposed a test, which he dubbed the HSD, or [**Honest Significant Differences**](https://en.wikipedia.org/wiki/Tukey%27s_range_test), test, also know as **Tukey's range test**. The test exhaustively computes the following for each pair of groups:\n",
    "- Difference of the means\n",
    "- Confidence interval of the difference in the means\n",
    "- A p-value from the distribution of the differences\n",
    "\n",
    "These results are laid out in a table or can be plotted graphically. Only differences in means with a confidence interval not overlapping zero are considered significant.\n",
    "\n",
    "The cells below contain the code to compute the Tukey HSD for the running example. The code uses the [statsmodels.stats.multicomp.pairwise_tukeyhsd](https://www.statsmodels.org/stable/generated/statsmodels.stats.multicomp.pairwise_tukeyhsd.html) function. Run this code and examine the results to determine which differences are significant?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'vals': df1 + df2 + df3 + df4,\n",
    "                   'group': ['1']*50 + ['2']*50 + ['3']*60 + ['4']*40})\n",
    "Tukey_HSD = pairwise_tukeyhsd(df.vals, df.group)\n",
    "print(Tukey_HSD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the table above. If the difference in means between the variables is significant, the confidence interval will not include 0. Which, pairs have a significant difference at the 95% confidence level? You can see the results of this test, with cutoff of 0.05, in the left most column of the table. \n",
    "\n",
    "The [statsmodels.stats.multicomp.tukeyhsd.plot_simultaneous](https://www.statsmodels.org/dev/generated/statsmodels.sandbox.stats.multicomp.TukeyHSDResults.plot_simultaneous.html) method for a pairwise_tukeyhsd object allows you to create a plot of the test results. The plot shows the difference of means as a dot and the confidence interval for this difference. Plot these figures and examine the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = Tukey_HSD.plot_simultaneous()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the plot above. There is a line with a dot shown for each variable. The dot is the mean and the line shows the range of the confidence interval for  that mean. If the difference in means is significant at the confidence level, the confidence intervals will not overlap. Which, pairs in the above plot have a significant difference at the 95% confidence level?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your turn:** It would be useful for the manager of the auto dealership understand which days of the week have significantly different average sales at the 95% confidence level. To solve this problem do the following:\n",
    "1. To do so, you will first need to construct a data frame by concatenating the sales data for each day of week and creating a column that indicates day of the week for each of these cases. \n",
    "2. Compute and print the results of the Tukey HSD test using the `pairwise_tukeyhsd` function. \n",
    "3. Use the `plot_simultneous` method on your model object to display the confidence intervals of the means. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which pairs of the day of the week are statistically different at the 95% confidence level? Do you think this result might help the manager to better schedule her sales people? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Word of Caution\n",
    "\n",
    "While the ANOVA and Tukey HSD methods allow us to tell if there is statistically significant differences between the means of multiple groups without data, there are limitations. Fundamentally, these these tests make **multiple comparisons** between the groups. As the number of groups increases, so does the chance of **false positives!**    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Summary\n",
    "\n",
    "We have covered lot of ground in this lesson. Specifically we have discussed:\n",
    "\n",
    "- Variance comparison test for multiple grouped in the form of ANOVA. The null hypothesis is that there are no differences in the variances of the samples and they are all from the same population. \n",
    "- The Tukey HSD test provides a way determine which group pairs have significant differences. \n",
    "- All of these methods involve multiple comparisons and are therefore subject to finding false positives. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copyright 2017, 2018, 2019, 2020, Stephen F Elston. All rights reserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
