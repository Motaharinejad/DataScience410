{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Bootstrapping regression\n",
    "\n",
    "## Data Science 410\n",
    "\n",
    "Previously, we have investigated the bootstrap resampling method. We used bootstrap resampling to compute a point estimate and confidence interval for simple univariate statistics.      \n",
    "\n",
    "The bootstrap method can also be applied to regression models. Computing the bootstrap distribution of the regression model parameters provides insight into variability of these parameters. It is useful to know how much random variation there is in regression coefficients simply because a small change in training data values. \n",
    "\n",
    "As with most statistics, it is possible to bootstrap the parameters of most any regression model. However, since bootstrap resampling uses a large number of subsamples, it can be computationally intensive. For large-scale problems it is necessary to using other resampling methods like cross-validation.\n",
    "\n",
    "To proceed, we need to introduce some new terminology:  \n",
    "\n",
    "- **Parametric bootstrap:** Linear regression regression is an example of a **parametric model**. Simply, a parametric model is a model with parameters which must be estimated. Bootstrapping to find estimates of these parameters is an example of a parametric bootstrap process. For each bootstrap sample created a new estimate of the model parameters is computed.    \n",
    "- **Nonparametric bootstrap:** Some statistical estimates have no particular model associated with them. Examples include the mean, variance and bootstrap confidence intervals. These values can be estimated using the bootstrap method without specifying a parametric model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Example\n",
    "\n",
    "Let's try an example. We will work with a simple regression model similar to the one used in the one used in the Introduction to Regression notebook. \n",
    "\n",
    "As a fist step, execute the code in the cell below to generate the synthetic data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as nr\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as sm\n",
    "import scipy.stats as ss\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramters of generated data\n",
    "n_points = 50\n",
    "x_start, x_end = 0, 10\n",
    "y_sd = 1\n",
    "\n",
    "# Generate data columns\n",
    "nr.seed(5666)\n",
    "x_data = np.linspace(x_start, x_end, n_points) # The x values\n",
    "y_error = np.random.normal(loc=0, scale=y_sd, size=n_points) # The Normally distributed noise\n",
    "y_data = x_data + y_error + 1.0 # The y values including an intercept\n",
    "\n",
    "# Put data in dataframe\n",
    "sim_data = pd.DataFrame({'x':x_data, 'y':y_data})\n",
    "\n",
    "sim_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, plot these data and examine the result by executing the code in the cell below.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matplotlib may give some font errors when loading for the first time, you can ignore these\n",
    "plt.plot(sim_data['x'], sim_data['y'], 'ko')\n",
    "plt.grid(True)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('x vs y')\n",
    "plt.ylim(0,11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resample Points (Parametric Bootstrap)\n",
    "\n",
    "The first type of bootstrapping we will perform is resampling points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def resample_regression(df, n_boots, n_params=2, formula='y ~ x'):\n",
    "    ## array to hold the bootstrap samples of the parameters\n",
    "    boot_samples = np.zeros((n_boots,n_params))\n",
    "    n_samples = df.shape[0]\n",
    "    ## Loop over the number of resamples\n",
    "    for i in range(n_boots):\n",
    "        ## Create a bootstrap sample of the data frame\n",
    "        boot_sample = df.sample(n=n_samples, replace = True)\n",
    "        ## Compute the OLS model\n",
    "        boot_model = sm.ols(formula=formula, data=boot_sample).fit()\n",
    "        ## Save the model parameters in the array\n",
    "        boot_samples[i,:] = boot_model._results.params\n",
    "    return boot_samples\n",
    "\n",
    "param_boots = resample_regression(sim_data,1000)\n",
    "param_boots[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the first few bootstrap estimates of the intercept and slope coefficients.     \n",
    "\n",
    "Let's look at the distributions of the slope and intercepts. Execute the code in the cell below to examine the distribution of the intercept, along with the confidence intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_CI(values, p=0.05):   \n",
    "    mean = np.mean(values)\n",
    "    UCI = np.percentile(values, 100-p/2.)\n",
    "    LCI = np.percentile(values, p/2.)\n",
    "    print('Mean = %6.2f' % (mean))\n",
    "    print('Upper confidence interval = %6.2f' % (UCI))\n",
    "    print('Lower confidence interval = %6.2f' % (LCI))\n",
    "    return(mean, UCI, LCI)\n",
    "\n",
    "def plot_boot_params(params):\n",
    "    mean, UCI, LCI = compute_CI(params)\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))\n",
    "    ## Plot a histogram\n",
    "    sns.distplot(params, bins=20, ax=ax[0])\n",
    "    ax[0].axvline(mean, color='red', linewidth=1)\n",
    "    ax[0].axvline(UCI, color='red', linewidth=1, linestyle='--')\n",
    "    ax[0].axvline(LCI, color='red', linewidth=1, linestyle='--')\n",
    "    \n",
    "    ax[0].set_title('Histogram of model parameter')\n",
    "    ax[0].set_xlabel('Model parameter values')\n",
    "    ## Plot the Q-Q Normal plot\n",
    "    ss.probplot(params, plot = ax[1])\n",
    "    ax[1].set_title('Q-Q Normal plot of model parameter')\n",
    "    plt.show()\n",
    "\n",
    "plot_boot_params(param_boots[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, execute the code in the cell below to display a plot of the slope parameter, along with the confidence intervals. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_boot_params(param_boots[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plots above you can see the variability and confidence intervals of the parameter estimates. Consider the answers to the following questions:  \n",
    "1. Are the bootstrap distribution values approximately Normal?\n",
    "2. Does one coefficient have greater variability than the other, and if so, which one?  \n",
    "3. Are the known parameter values of the synthetic data within the confidence intervals? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also plot bootstrap realizations of the regression line. This will give you a feel for the variability of the regression solutions. Execute the code in the cell below and examine the result.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_boot_regression(boot_params, df, n_lines = 100):    \n",
    "    ## randomly select n_lines to plot\n",
    "    sample_indx = nr.choice(range(boot_params.shape[0]),n_lines)\n",
    "    ## Plot the observations\n",
    "    ax = sns.scatterplot(x='x', y='y', data=df, color = 'magenta')\n",
    "    ## Loop over the number of bootstrap regression lines to be displayed\n",
    "    for indx in sample_indx:\n",
    "        df['predicted'] = boot_params[indx,0] + np.multiply(boot_params[indx,1],df['x'])\n",
    "        sns.lineplot(x='x', y='predicted', data=df, color='Blue', size = 1.0, alpha=0.1, ax=ax)\n",
    "    ## Find the mean slope and intercept and plot the line \n",
    "    means = np.mean(boot_params, axis=0)\n",
    "    df['predicted'] = means[0] + np.multiply(means[1],df['x'])\n",
    "    sns.lineplot(x='x', y='predicted', data=df, color='red', ax=ax)\n",
    "    ax.get_legend().remove()\n",
    "    \n",
    "plot_boot_regression(param_boots, sim_data)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot above shows the following:   \n",
    "1. The heavy line is the mean parameter regression line.   \n",
    "2. The light blue lines show a selected number of the bootstrap regression lines. Notice the variability in these possible models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Turn:**\n",
    "\n",
    "You have seen how the bootstrap distribution of the model coefficients gives insight into their variability. But, what about the predicted values. How uncertain are they?\n",
    "\n",
    "In the cell below use the 1000 bootstrapped values of the slope and intercept to find the distribution of the predicted value, y, for x = 5.0. Compute the mean and CIs and plot the histogram and Q-Q Normal plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do the above results tell you about the uncertainty in the predicted values from the regression model? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copyright 2017, 2018, 2020 Stephen F Elston. All rights reserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
