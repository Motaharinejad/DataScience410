{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Regression\n",
    "\n",
    "### Data Science 410\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The method of regression is one of the oldest and most widely used analytics methods. The goal of regression is to produce a model that represents the **best fit** to some observed data. Typically the model is a function describing some type of curve (lines, parabolas, etc.) that is determined by a set of parameters (e.g., slope and intercept). *Best fit* means that there is an optimal set of parameters which minimize an error criteria we choose.\n",
    "\n",
    "Regression models attempt to predict the value of one variable, known as the **dependent variable**, **response variable** or **label**, using the values of other variables, known as **independent variables**, **explanatory variables** or **features**. **Single regression** has one label used to predict one feature. **Multiple regression** uses two of more feature variables to predict a label. \n",
    "\n",
    "Many machine learning models, including some of the latest deep learning methods, are a form of regression. **Linear regression** is the foundational form of regression. Linear regression minimizes squared error of the predictions of the dependent variable using the values of the independent variables. This approach is know as the **method of least squares**.\n",
    "\n",
    "By developing an understanding of linear regression, you are building a foundation to understand many other machine learning models. Nearly all machine learning methods suffer from the same problems, including over-fitting and mathematically unstable fitting methods. Understanding these problems in the linear regression context will help you work with other machine learning models.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## History\n",
    "\n",
    "Regression is based on the method of least squares or the method of minimum mean square error. The idea of averaging errors have been applied for nearly three centuries. The fist known publication of a *method of averages* was by the German astronomer Tobias Mayer in 1750. Lapace used a similar method which he published in 1788.\n",
    "\n",
    "<img src=\"img/TobiasMayer.jpg\" alt=\"TobiasMayer\" style=\"width: 200px;\"/>\n",
    "\n",
    "The first publication of the **method or least squares** was by the French mathematician Adrien-Marie Legendre in 1805. Legendre was a brilliant mathematician, known for his unpleasant personality.  \n",
    "\n",
    "![](img/Legendre.jpg)\n",
    "<center>Caricature of Legendre, published method of least squares</center>\n",
    "\n",
    "It is very likely that the German physicist and mathematician Gauss developed the method of least squares as early as 1795, but did not publish the method until 1809, aside from a reference in a letter in 1799. Gauss never disputed Legendre's priority in publication. Legendre did not return the favor, and opposed any notion that Gauss had used the method earlier. \n",
    "\n",
    "![](img/Carl_Friedrich_Gauss.jpg)\n",
    "<center>Carl Friedrich Gauss, early adopter of the least squares method</center>\n",
    "\n",
    "The first use of the term **regression** was by Francis Gaulton, a cousin of Charles Darwin, in 1886. Gaulton was interested in determining which traits of plants and animals, including humans, could be said to be inherited. Gaulton used the term **regression to the mean** to describe the natural processes he observed in inherited traits.  \n",
    "\n",
    "<img src=\"img/Francis_Galton.jpg\" alt=\"Drawing\" style=\"width:225px; height:250px\"/>\n",
    "<center>Francis Galton, inventor of regression</center>\n",
    "\n",
    "While Gaulton invented a modern form regression, it fell to Karl Pearson to put regression and multiple regression on a firm mathematical footing. Pearson's 1898 publication proposed a method of regression as we understand it today. \n",
    "\n",
    "Many others have expanded the theory of regression in the 120 years since Pearson's paper. Notably, Joseph Berkson published the logistic regression method in 1944, one of the first classification algorithms. In recent times the interest in machine learning has lead to a rapid increase in the numbers and types of regression models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Theory of Linear Regression\n",
    "\n",
    "We will focus on the theory of **linear models**, which are foundational. Key properties of linear models include:\n",
    "- Derived with linear algebra.\n",
    "- Include any model **linear in coefficients**, including polynomials, splines, Gaussian kernels and many other nonlinear function.    \n",
    "- Understanding linear models is basis for understanding behavior many other statistical or machine learning models.\n",
    "- Basis of many time series models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear model of a strait line\n",
    "\n",
    "Let's have a look at the simple case of a regression model for a straight line. For this example we will work with single regression with one feature and one label. The data are in the form of some number of values pairs, $\\{x_i,y_i \\}$. \n",
    "\n",
    "The goal of this regression model is to find a straight that best fits the observed data. We can define the line by two coefficients or **parameters**, the **slope** and the **intercept**. A general representation of this parameterization of a straight line is illustrated in the figure below.\n",
    "\n",
    "<img src=\"img/ymxb.jpg\" alt=\"y_equals_mx_plus_b\" style=\"width: 450px;\"/>\n",
    "<center>**Single regression model**</center>\n",
    "\n",
    "Where,  \n",
    "\n",
    "\\begin{align}\n",
    "m &= slope = \\frac{rise}{run} = \\frac{\\delta y}{\\delta x}\\\\\n",
    "and\\\\\n",
    "y &= b\\ at\\ x = 0\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "For each of the pairs of observed values, ${x_i,y_i}$, we can write the equation for the line with the errors as:\n",
    "\n",
    "\\begin{align}\n",
    "y_i &= mx_i + b + \\epsilon_i \\\\\n",
    "where \\\\\n",
    "\\epsilon_i &= error\n",
    "\\end{align}\n",
    "\n",
    "We can visualize these errors as shown in the figure below.\n",
    "\n",
    "<img src=\"img/LSRegression.jpg\" alt=\"LSRegression\" style=\"width: 450px;\"/>\n",
    "<center>Example of least squares regression with errors shown as vertical lines</center>\n",
    "\n",
    "We want to solve for $m$ and $b$ by minimizing the error, $\\epsilon_i$. We call this **least squares regression** problem.\n",
    "\n",
    "$$min \\Sigma_i \\epsilon^2 = min \\Sigma_i{ (y_i - (mx_i + b))^2}$$\n",
    "\n",
    "There are lots of computationally efficient algorithms for finding minimums of equations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A First Regression Model\n",
    "\n",
    "Let's give regression a try. The code in the cell below computes data pairs along a straight line. Normally distributed noise is added to the data values. Run this code and examine the head of the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.475868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.204082</td>\n",
       "      <td>-0.084436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.408163</td>\n",
       "      <td>1.216802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.612245</td>\n",
       "      <td>0.362557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.816327</td>\n",
       "      <td>2.521151</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          x         y\n",
       "0  0.000000  0.475868\n",
       "1  0.204082 -0.084436\n",
       "2  0.408163  1.216802\n",
       "3  0.612245  0.362557\n",
       "4  0.816327  2.521151"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import numpy.random as nr\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Paramters of generated data\n",
    "n_points = 50\n",
    "x_start, x_end = 0, 10\n",
    "y_start, y_end = 0, 10\n",
    "y_sd = 1\n",
    "\n",
    "# Generate data columns\n",
    "nr.seed(5666)\n",
    "x_data = np.linspace(x_start, x_end, n_points) # The x values\n",
    "y_error = np.random.normal(loc=0, scale=y_sd, size=n_points) # The Normally distributed noise\n",
    "y_data = np.linspace(y_start, y_end, n_points) + y_error # The y values\n",
    "\n",
    "# Put data in dataframe\n",
    "sim_data = pd.DataFrame({'x':x_data, 'y':y_data})\n",
    "# Or alternatively:\n",
    "# sim_data = pd.DataFrame(data=np.column_stack((x_data, y_data)), columns=['x','y'])\n",
    "\n",
    "sim_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you can visualize these data by executing the code in the cell below. Notice that the points nearly fall on a straight line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'x vs y')"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Matplotlib may give some font errors when loading for the first time, you can ignore these\n",
    "plt.plot(sim_data['x'], sim_data['y'], 'ko')\n",
    "plt.grid(True)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('x vs y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you are ready to build and evaluate a regression model using Python. Python has libraries that contain linear modeling capabilities.\n",
    "\n",
    "The *Scikit-learn* package has many different types of machine learning algorithms. Here we will concern ourselves with the linear regression capabilities.\n",
    "\n",
    "The second library is called 'Stats-models'.  For those that have experience with the programming language, R, this library is the most similar because it provides easy statistical analysis of linear models that are fitted to the data.\n",
    "\n",
    "To start, we will show how to use the library 'Scikit-learn' for linear regression. Model creation in Scikit-learn generally has two steps, declaring the model formulation and then fitting the model\n",
    "\n",
    "In our case we only have one independent variable and one dependent variable. The code in the cell below does the following:  \n",
    "\n",
    "- Compute the Python model object, `linear_model`, using the simple linear regression.\n",
    "- Use the model object to compute scores (predicted values) for the dependent variable `y`. In this case, we just use the data that was originally used to compute the model. In a more general case, you can use other data to make predictions from the model.\n",
    "- The residuals of the model are computed.\n",
    "\n",
    "Execute this code and examine the head of the data frame computed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we show how to fit a line with Stats Models package. The model declaration and fitting are in a different format as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept = 0.306  Slope = 0.941\n"
     ]
    }
   ],
   "source": [
    "## Define the regresson model and fit it to the data\n",
    "#nr.seed(3344)\n",
    "ols_model = sm.ols(formula = 'y ~ x', data=sim_data).fit()\n",
    "\n",
    "## Print the model coefficient\n",
    "print('Intercept = %4.3f  Slope = %4.3f' % (ols_model._results.params[0], ols_model._results.params[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>predicted</th>\n",
       "      <th>resids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.475868</td>\n",
       "      <td>0.305594</td>\n",
       "      <td>-0.170274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.204082</td>\n",
       "      <td>-0.084436</td>\n",
       "      <td>0.497657</td>\n",
       "      <td>0.582093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.408163</td>\n",
       "      <td>1.216802</td>\n",
       "      <td>0.689720</td>\n",
       "      <td>-0.527082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.612245</td>\n",
       "      <td>0.362557</td>\n",
       "      <td>0.881783</td>\n",
       "      <td>0.519226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.816327</td>\n",
       "      <td>2.521151</td>\n",
       "      <td>1.073846</td>\n",
       "      <td>-1.447306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.020408</td>\n",
       "      <td>-0.181119</td>\n",
       "      <td>1.265908</td>\n",
       "      <td>1.447027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.224490</td>\n",
       "      <td>2.096915</td>\n",
       "      <td>1.457971</td>\n",
       "      <td>-0.638943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.428571</td>\n",
       "      <td>1.631663</td>\n",
       "      <td>1.650034</td>\n",
       "      <td>0.018371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.632653</td>\n",
       "      <td>1.074532</td>\n",
       "      <td>1.842097</td>\n",
       "      <td>0.767565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.836735</td>\n",
       "      <td>1.897740</td>\n",
       "      <td>2.034160</td>\n",
       "      <td>0.136420</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          x         y  predicted    resids\n",
       "0  0.000000  0.475868   0.305594 -0.170274\n",
       "1  0.204082 -0.084436   0.497657  0.582093\n",
       "2  0.408163  1.216802   0.689720 -0.527082\n",
       "3  0.612245  0.362557   0.881783  0.519226\n",
       "4  0.816327  2.521151   1.073846 -1.447306\n",
       "5  1.020408 -0.181119   1.265908  1.447027\n",
       "6  1.224490  2.096915   1.457971 -0.638943\n",
       "7  1.428571  1.631663   1.650034  0.018371\n",
       "8  1.632653  1.074532   1.842097  0.767565\n",
       "9  1.836735  1.897740   2.034160  0.136420"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add predicted to pandas dataframe\n",
    "sim_data['predicted'] = ols_model.predict(sim_data.x)\n",
    "# Add residuals to pandas dataframe\n",
    "sim_data['resids'] = np.subtract(sim_data.predicted, sim_data.y)\n",
    "\n",
    "# View head of data frame\n",
    "sim_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x24579846898>"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.lineplot(x='x', y='predicted', data=sim_data, color='red')\n",
    "sns.scatterplot(x='x', y='y', data=sim_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in the cell below is fairly volumonous, but straight forward. In summary, the code computes summary statistics and makes diagnostic plots for ordinary linear models.\n",
    "\n",
    "The Scikit-learn library is primarily made for fitting models and getting predictions. For linear regression, we are also interested in many diagnostic plots.  To get these plots easily, we can use a different library called statsmodels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of regression models\n",
    "\n",
    "Now that you have built a regression model, let's look at how you can quantitatively evaluate the performance of a regression model. The evaluation of regression models is based on measurements of the errors. The errors of a regression model can be visualized as shown in the figure below. \n",
    "\n",
    "<img src=\"img/Errors.jpg\" alt=\"Regression_Errors\" style=\"width: 450px;\"/>\n",
    "\n",
    "<center>**Measuring errors for a regression model**\n",
    "$$Where\\\\\n",
    "Y = [y_1, y_2, \\ldots, y_n]\\\\\n",
    "and\\\\\n",
    "y_i = ith\\ data\\ value\\\\\n",
    "\\bar{Y} = mean(Y)\\\\\n",
    "\\\\\\hat{y_i} = regression\\ estimate\\ of\\ y_i\\\\\n",
    "SSE = sum\\ square\\ explained\\ = \\Sigma_i{(\\hat{y_i} - \\bar{Y})^2}\\\\\n",
    "SSR = sum\\ square\\ residual\\ = \\Sigma_i{(y_i - \\hat{y_i})^2}\\\\\n",
    "SST = sum\\ square\\ total\\ = \\Sigma_i(y_i - \\bar{Y})^2$$\n",
    "\n",
    "The goal of regression is to minimize the residual error, $SSR$. Specifically we wish to explain the maximum amount of the variance in the original data as possible with our model. We can quantify this idea with coeficient of determination also known as $R^2$.\n",
    "\n",
    "$$R^2 = 1 - \\frac{SSR}{SST}\\\\\n",
    "so\\ as\\\\\n",
    "SSR \\rightarrow 0\\\\\n",
    "R^2 \\rightarrow 1$$\n",
    "\n",
    "In words, $R^2$ is the fraction of the variance of the original data explained by the model. A model with perfectly explain the data has $R^2 = 1$. A model which does not explain the data at all has $R^2 = 0$.\n",
    "\n",
    "However, there are two problems with $R^2$. </center>\n",
    " - $R^2$ is not bias adjusted for degrees of freedom.\n",
    " - More importantly, there is no adjustment for the number of model parameters. As the number of model parameters increases $SSR$ will generally decrease. Without an adjustment you will get a false sense of model performance.\n",
    " \n",
    "To addresses these related issues, we can use adjusted $R^2$.\n",
    "\n",
    "$$R^2_{adj} = 1 - \\frac{\\frac{SSR}{df_{SSR}}}{\\frac{SST}{df_{SST}}} = 1 - \\frac{var_{residual}}{var_{total}}\\\\\n",
    "where\\\\\n",
    "df_SSR = SSR\\ degrees\\ of\\ fredom\\\\\n",
    "df_SST = SST\\ degrees\\ of\\ fredom$$\n",
    "\n",
    "This gives $R^2_{adj}$ as:\n",
    "\n",
    "$$R^2_{adj} = 1 - (1 - R^2) \\frac{n - 1}{n - k}\\\\ \n",
    "where\\\\\n",
    "n = number\\ of\\ data\\ samples\\\\\n",
    "k = number\\ of\\ model\\ coeficients$$\n",
    "\n",
    "Or, we can rewrite $R^2_{adj}$ as:\n",
    "\n",
    "$$R^2_{adj} =  1.0 - \\frac{SSR}{SST}  \\frac{n - 1}{n - 1 - k}$$\n",
    "\n",
    "Another measure of regression performance is root mean square error or $RMSE$:\n",
    "\n",
    "$$RMSE = \\sqrt{ \\frac{\\Sigma^n_{i-1} (y_i - \\hat{y_i})^2}{n}} = \\frac{\\sqrt{SSR}}{n}$$\n",
    "\n",
    "**Your Turn:** Examine the performance metrics for the previous two regressions. How do SSE, SSR, SST, $R^2$, and RMSE compare?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.907</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.905</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   469.5</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Thu, 13 Feb 2020</td> <th>  Prob (F-statistic):</th> <td>1.97e-26</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>19:02:53</td>     <th>  Log-Likelihood:    </th> <td> -64.907</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>    50</td>      <th>  AIC:               </th> <td>   133.8</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    48</td>      <th>  BIC:               </th> <td>   137.6</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>    0.3056</td> <td>    0.252</td> <td>    1.212</td> <td> 0.231</td> <td>   -0.201</td> <td>    0.812</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x</th>         <td>    0.9411</td> <td>    0.043</td> <td>   21.668</td> <td> 0.000</td> <td>    0.854</td> <td>    1.028</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 0.850</td> <th>  Durbin-Watson:     </th> <td>   2.312</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.654</td> <th>  Jarque-Bera (JB):  </th> <td>   0.450</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.228</td> <th>  Prob(JB):          </th> <td>   0.799</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 3.091</td> <th>  Cond. No.          </th> <td>    11.7</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.907\n",
       "Model:                            OLS   Adj. R-squared:                  0.905\n",
       "Method:                 Least Squares   F-statistic:                     469.5\n",
       "Date:                Thu, 13 Feb 2020   Prob (F-statistic):           1.97e-26\n",
       "Time:                        19:02:53   Log-Likelihood:                -64.907\n",
       "No. Observations:                  50   AIC:                             133.8\n",
       "Df Residuals:                      48   BIC:                             137.6\n",
       "Df Model:                           1                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept      0.3056      0.252      1.212      0.231      -0.201       0.812\n",
       "x              0.9411      0.043     21.668      0.000       0.854       1.028\n",
       "==============================================================================\n",
       "Omnibus:                        0.850   Durbin-Watson:                   2.312\n",
       "Prob(Omnibus):                  0.654   Jarque-Bera (JB):                0.450\n",
       "Skew:                          -0.228   Prob(JB):                        0.799\n",
       "Kurtosis:                       3.091   Cond. No.                         11.7\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ols_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE = 0.886\n"
     ]
    }
   ],
   "source": [
    "## Print RMSE\n",
    "RMSE = np.std(sim_data.resids)\n",
    "print('RMSE = %4.3f' % (RMSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEWCAYAAABv+EDhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAgAElEQVR4nO3deZgcdb3v8fcny0BIRo0hokkIUYGgoiwZFI0LCipHiRi3IyK4YHIVOS5XcUXNOa48cjkHDzdgQEAUd4wCLgjKmgvCJKCCgCgSCIEQIJgQcxkm8z1/1G+Szsz0TPdMd1d31+f1PPNkurq66luVnvrWby1FBGZmVjzj8g7AzMzy4QRgZlZQTgBmZgXlBGBmVlBOAGZmBeUEYGZWUE4A1nCS5ku6U9Jjkt5Yx/3MTvsYX+b9JZK+W6N9haQ9a7GtZld6rJLOlPS5Buzz3ZKurfd+isYJoM1JulvSlnQhXCfpXElT0ntXSnrfKLd52BjC+g/g9IiYEhE/G8N2hhUR96R9bK3XPoouIt4fEV8cab3RftesvpwAimFBREwBDgQOAk7KOZ49gFsrWVHShDrHUmjlSkdWDE4ABRIR9wG/AvYdaV1Jb5B0q6RH093bc9Ly7wCzgYtTqeITZT6/SNJfJT0i6SJJM9LyvwHPKvn8TkN89m5Jn5T0R2CzpAmSZki6UNJ6SX+X9KGS9V8oqVvSxlTKOTUtn5OqKyak18+UdJWkTZIuA3Yt2cYhktYMEcdhJfu4Lp2P+yWdLqmjzLG/TtKf037uk/TxIdbZKW1r35Jl01Np7WmSdpV0SVrnEUnXSBrx77X/OCR9RtJD6RiOLnn/PElnSPqlpM3AK1Msp0i6J52/MyVNKvnMiemY10p674D9nSfpSyWvj5R0c/q/+JukwyV9GXgZcHr6Pz89rbuPpMvS8d0h6W0l25mWvjcbJd0APHukY7dRiAj/tPEPcDdwWPp9d7I77y+m11cC7xviM3sDm4FXAxOBTwB/BToGbrPMPl8FPERW4tgJ+G/g6qFiGibmm1O8k8huVFYCnwc6yBLIXcBr0/rXAcek36cAB6ff5wABTChZ79QU08uBTcB303uHAGuGOXfzgIOBCWm7twEfKVk3gD3T7/cDL0u/TwUOLHOc5wBfLnn9QeDX6fevAmem8z+R7AKqCv6/DwF6S47zFen/cm56/zzgH8D8dF53Bv4LuAh4KtAJXAx8Na1/OLCO7KZhMvC9Acd6HvCl9PsL07ZfnbY9E9hnqO9a2ta9wHvSOT2Q7DvzvPT+D4AfpfX2Be4Drs3776ndflwCKIafSXoUuBa4CvjKCOv/K/CLiLgsIp4ATiG7EL+kwv0dDZwTEasi4nHg08CLJc2pIuZvRMS9EbGFrNpqekT8R0T0RMRdwFnA29O6TwB7Sto1Ih6LiOsHbkzS7LSdz0XE4xFxNdmFriIRsTIiro+I3oi4G/gm2cV1KE8Az5X0pIjYEBGryqz3PeCoktfvSMv6t/EMYI+IeCIirol0ZaxQ/3FeBfwCeFvJez+PiBUR0Qc8DiwCPhoRj0TEJrLvR/+5fRtwbkTcEhGbgSXD7PM4sv/3yyKiLyLui4jby6x7BHB3RJybzukq4ELgLala6s3A5yNic0TcAny7imO3CjkBFMMbI+IpEbFHRByfLqrDmQGs7n+RLhT3kt3RVWLg5x8DHq7i86T99dsDmJGqQx5NyewzwG7p/ePISi23S7pR0hFlYtqQLmL9Vg+x3pAk7Z2qZB6QtJHsIrlrmdXfDLwOWJ2qnF5cZr3fAZMkvUjSHsD+wPL03tfJSl2/kXSXpE9VGitDH+eMktel53Y6sAuwsuTc/jotJ32udP3hztnuwN8qjHEP4EUD/k+PBp6e9j2hiv3aKLmBzYayFnh+/wtJIvvjvi8tGulOdC3ZH3j/5ycD00o+X4nSfdwL/D0i9hpyxYg7gaNSHfmbgJ9ImjZgtfuBqZIml1wcZ5fsZzPZhbA/5vFsvwgCnAHcBBwVEZskfQR4S5l4bgSOlDQROIGsKmP3Idbrk/QjslLAOuCSdAdO+vdjwMckPQ+4QtKNEfHbofY5wFDHeUvprkt+fwjYQlb1MtT/z/0DYp89zH7vpXxd/cDvzL3AVRHx6oErpnPfm/bbX4IYbr82Si4B2ARJO5f8TCS7YL1e0qHp9cfIqgr+X/rMOrJ6+HK+B7xH0v7KGnm/Avw+VZ2Mxg3ARmUNw5MkjZe0r6SDACS9U9L0VFJ5NH1mh66fEbEa6Ab+XVKHpJcCC0pW+Quws6TXp2M+iawOvV8nsBF4TNI+wAeGCjRt+2hJT07VZxsHxjLA98iq3I5me/UPko6QtGdKvv3bqKY7a/9xvoysuuXHQ62UztlZwH9Kelra90xJr02r/Ah4t6TnStoF+MIw+/wW2f/7oZLGpe3sk94b+J25BNhb0jGSJqafgyQ9J7Juuz8FlkjaRdJzgXdVcexWIScAO4PsDrD/59yIuAN4J1nj7UNkF8oFEdGTPvNV4KRUdB/UwyXdpX6OrE73frK7wrcPXK9S6YKwgKyK5O8pprOBJ6dVDgdulfQYcBrw9oj4/0Ns6h3Ai4BHyC5k55fs4x/A8Wm795GVCEp7BX08fX4T2QXzh8OEfAxwd6oqej/ZuSx3bL9P+5pB1kOr317A5cBjZI3XSyPiSgBJv5L0mWH2/wCwgawkdgHw/mHq4gE+SVbddH2K+XJgborvV2SNxL9L6/xumGO5gaxR9z/JGoOvYntJ8DSy+v0Nkr6RSjivIfterE0xn8z2pHsCWYP+A2QNzecOE7+NkqprVzKzZibpELKeTbPyjsWan0sAZmYF5QRgZlZQrgIyMysolwDMzAqqpcYB7LrrrjFnzpy8wzAzaykrV658KCKmD1zeUglgzpw5dHd35x2GmVlLkTTkSGpXAZmZFZQTgJlZQTkBmJkVlBOAmVlBOQGYmRWUE4CZWUG1VDdQs3bT1xc8vLmHnt6tdEwYz7TJHYwbp7zDsoJwAjDLSV9fcMe6TSw6v5s1G7Ywa+okzjq2i7m7dToJWEO4CsgsJw9v7tl28QdYs2ELi87v5uHNPSN80qw2nADMctLTu3Xbxb/fmg1b6Omt5sFfZqPnBGCWk44J45k1ddIOy2ZNnUTHhPE5RWRF4wRglpNpkzs469iubUmgvw1g2uSOnCOzonAjsFlOxo0Tc3frZPnx890LyHLhBGCWo3HjxPTOnUZe0awOXAVkZlZQTgBmZgXlBGBmVlBOAGZmBeUEYGZWUE4AZmYF5QRgZlZQTgBmZgXlBGBmVlBOAGZmBeUEYGZWUE4AZmYF5QRgZlZQTgBmZgWVWwKQtLukKyTdJulWSR/OKxYzsyLK83kAvcDHImKVpE5gpaTLIuLPOcZkZlYYuZUAIuL+iFiVft8E3AbMzCseM7OiaYo2AElzgAOA3w/x3mJJ3ZK6169f3+jQzMzaVu4JQNIU4ELgIxGxceD7EbEsIroiomv69OmND9DMrE3lmgAkTSS7+F8QET/NMxYzs6LJsxeQgG8Bt0XEqXnFYWZWVHmWAOYDxwCvknRz+nldjvGYmRVKbt1AI+JaQHnt38ys6HJvBDYzs3w4AZiZFZQTgJlZQTkBmJkVVGslgLVrQdr+s3Jl9lO6bMmSbN0ZM7YvmzcvW7Z48Y7rrl0LF1+847Jly7J1S5ctWJAtW7Bgx+WQrV+67OKLB8e5eHG27rx525fNmJEtW7LEx+Rj8jH5mOp7TGUoIsq+2Wy6urqiu7s77zDMzFqKpJUR0TVweWuVAMzMrGacAMzMCsoJwMysoJwAzMwKKs8ngpmZVaSvL3h4cw89vVvpmDCeaZM7GDdOeYfV8pwAzKyp9fUFd6zbxKLzu1mzYQuzpk7irGO7mLtbp5PAGLkKyMya2sObe7Zd/AHWbNjCovO7eXhzT86RtT6XAMyq5OqIxurp3brt4t9vzYYt9PRuzSmi9uEEUEe+ULQfV0c0XseE8cyaOmmHJDBr6iQ6JozPMar24CqgOum/UCxcuoL5J1/BwqUruGPdJvr6WmfktQ3m6ojGmza5g7OO7WLW1EkA25LutMkdOUfW+lwCqJNyF4rlx89neudOOUdno+XqiMYbN07M3a2T5cfPd2m6xpwA6sQXivbk6oh8jBsn3zjVgauA6qT/QlHKF4rW5+oIayeeDbRO3FjYvty4b62m3GygrgKqE9dbti9XR1i7cAKoI18ozKyZuQ3AzKygnADMzArKCcDMrKCcAMzMCsqNwGZWN+4y29ycAMysLjwWpvm5CsjM6sIT5zU/JwAzqwvPh9X8nADMrC48H1bzcwIws7rwxHnNz43AZlYXng+r+eWaACSdAxwBPBgR++YZi7U/d0lsPM+H1dzyLgGcB5wOnJ9zHNbm3CXRbLBc2wAi4mrgkTxjsGJwl0SzwdwIbIXgLolmgzV9ApC0WFK3pO7169fnHY61KHdJNBus6RNARCyLiK6I6Jo+fXre4ViLcpdEs8HybgQ2awh3STQbLNcSgKTvA9cBcyWtkXRcnvFYe+vvkjhz6i5M79zJF38rvFxLABFxVJ77N7Ox8/iK1uUqIDMbNY+vaG1N3whsZs3L4ytam0sALcLFbGtGHl/R2pwAWoCL2das+sdXlCYBj69oHa4CagGjKWb39QXrNz3OfRv+yfpNj9PXF40K1wrE4ytam0sALaDaYnajSgzlqqVcXVUcHl/R2pwAWkC1xexyJYblx8+v2dS85ZLMXtOncOf6x1xdVSCe8rl1VVQFJGm+pMnp93dKOlXSHvUNzfpVW8xuRMNcuSTz4GOPu1eIWYuotARwBrCfpP2ATwDfIpvD/xX1Csy2q7aY3YiGuXJJ5omtfe4VYtYiKm0E7o2IAI4ETouI04DO+oXVGK3UUFrNNAaNaJgrN7vmxPHjPOtmDbTSd9Nal7Lr+ggrSVcBvwbeA7wcWA/cHBHPr294O+rq6oru7u6abKvdu1bWuyHWbQD1k/d30434lWml8yRpZUR0DVpeYQJ4OvAO4MaIuEbSbOCQiGjooxxrmQDWb3qchUtXDKomqWVDabtzL6D6yPO7mXfyaRWtdp7KJYCKqoAi4oGIODUirkmv72n0xb/WRtNQ6mL5jspVS3nWzbHJc3Stp3aoTLucp2EbgSVtAoa6ygmIiHhSXaJqgGobSlst41vrynN0rad2qEy7nKdhSwAR0RkRTxrip7OVL/5QfUNpu2R8a355jq71ozMr0y7nqaI2gG0rS08Ddu5/HRH31COocmrZBgDVNeLct+GfzD/5ikHLV3zylcycukvNYjKD/BoYXdKtTKudp3JtABWNA5D0BuD/ADOAB4E9gNuA59UyyEarZgSjJ72yRsprdK2ndqhMo85TvW8EKh0I9kXgYODyiDhA0iuBQj3Nq79YPjDje9Irazee2qEy9T5PjShlVNoNtDsiuiT9ATggIvok3RARL6xJFBWqdRVQtdy90ax48vq7r2V34DFVAQGPSpoCXA1cIOlBoLeqCNqA74zMiiXPuv5G9DSqdCqII4EtwEfJRgT/DVhQsyjMzJpQrXv/VTOWqBE9jSoqAUTE5pKX367Z3s3Mmlgt78KHK00Ag6qZGtHuWGkvoNIBYR3ARGBzq48FMLPiqaZOv5a9/8qVJi46YT7rNj4+ZGKod0+jSqeCKB0QtjPwZuD0mkVhZtYA/XfhC5euYP7JV7Bw6QruWLepbFVMLQfllStNbOnZWraaqd7TqozqiWAR8TNJn6ppJGZmdVbt0/Jq2d+/XGlia0Ru00pUWgX0ppKX44Auhp4jyMysaY2mTr9Wvf/K1envPDG/QaaVlgBKe/z0AneT9QwyM2sZeY7oL1eaAHIbZFrVXEB5y3sgmDUPD8qz0WjWOXzq/X0e1UAwSf/NMFU9EfGhGsRmVpVm/SO25tesc/jkNvfTCO93AyvJZgA9ELgz/ewPtNbE122qiA+p8dTcNhb17llTbU+jPA1bAoiIbwNIejfwyoh4Ir0+E/hN3aOzYRX1TrhdHsZh7anankZ5qnQqiBlAZ8nrKWmZ5aiod8Lt8jAOa0+tdINSaQL4GnCTpPMknQesAr5St6isIq30RaulPJ+YZTaSVrpBqXQuoHMl/Qp4UVr0qYh4oH5hWSVG26Wt2gaqZutx44eWWDNrpWeHDNsNVNI+EXG7pAOHej8iVo1p59LhwGnAeODsiPjacOsXoRtoNRfb0bQBVPuZorYzmI1Fs900lesGOlICWBYRiyUNfhguRES8agwBjQf+ArwaWAPcCBwVEX8u95l2TwCjvaBX80Wr9iETtXwohZnlY1TjACJicfr3lXWI6YXAXyPirhTgD8hGF5dNAO1uNL0Hqu0/XG27QVHbGcyKoNK5gN4K/DoiNkk6iWxMwBcj4qYx7HsmcG/J6zVsb2Mo3fdiYDHAvGzBGHbZ3KYD1w71Rg2n3ZtJNo/HICfXZn0zax2VzgX0uYj4saSXAq8FTgHOZIgLdhWGupIPqo+KiGXAMsiqgGjjKqBGVLe0WhtAs9WlmrWkMjfOlSaA/vL+64EzIuLnkpaMMaQ1wO4lr2cBa8e4zZbWiN4D1fagybPHTd7Jx6zdVTQZnKRLgPuAw8hqYrYAN0TEfqPesTSBrBH40LTtG4F3RMSt5T7T7o3A4DveUu3UAO3/V8vTqBqBS7wNOBw4JSIelfQM4MSxBBQRvZJOAC4l6wZ6znAX/6LIa1KoZtQuDdAuyVizqvSRkP8EHgRemhb1kk0KNyYR8cuI2Dsinh0RXx7r9oZSxMnS2kUrjagcTlGn7LDmV1ECkPQF4JPAp9OiicB36xVUrbTSrHw2WLtM+dAuJRlrP5VWAS0EDiCbA4iIWCupc/iP5K+VZuWzwdplyoc8n0JlNpxKJ4Priay1OAAkTa5fSLXjO6/WV++52xuhXUoy1n4qLQH8SNI3gadIWgS8Fzi7fmHVhu+8rBm0S0nG2k+ljcCnAD8BLgTmAp+PiG/UM7Ba8J2XNYt2KMlY+xnVQ+HTRG5vj4gLah9SeaMZB+D+12ZWiXa+Voz2ofBPAj5INiXMRcBl6fWJwM1AQxPAaLhfvZmNpKhjNUaqAvoOWZXPn4D3kT0H+K3AkRFxZJ1jswbzmAkrqqKO1RipEfhZEfF8AElnAw8BsyNiU90js4Yq6h2QGRS3x+BIJYAn+n+JiK3A333xb09FvQMyg/YZdV6tkRLAfpI2pp9NwAv6f5e0sREBWmMU9Q7IDIrbY3CkJ4K1d/qzbTxmwoqsqGM1Kh0JbG2uqHdAZv2KOFaj0pHA1uaKegdUS+3cj9zakxPAEIr6h+wxE6PXqF5URf1uWn04AQzg7pA2Go2YedbfTas1twEM4O6QNhqN6EXl76bVmhPAAO4OaaPRiH7k/m5arTkBDFDUASE2No3oRVWE76anI2msUc0GmpfRzAZaLTfm2WjV+/+03dsA2v348lRuNlAngCH4D9mgOZN0M8ZUK+s3Pc7CpSsGDUb0I1zHblTTQRdVvbtD+lnFza9Zk3Q7d9V1G0fjuQ0gB/6iNz/3uGm8IrRxNBsngBz4i95chmp4dJJuPE9H0niuAspB/xd9YPWCv+iNV66qZ9qUDk+ON0C92x88HUnjuRE4J+3cmNdKyjU8XnTCfNZtfLzp2gDy0qxtIlYZNwI3mXZuzGsl5ap6tvRs9d1oCXdcaE9OAFZowz0HwUl6u2ZtE3FJemzcCGyF5obHyjRjx4X+aqmFS1cw/+QrWLh0BXes2+TRw1VwG4AVnu8iR9aMbQAeOFY5twGYleGqnpE1Yw+dZq2WaiVOAGYFVW3Jp9kSpZ9jPXZuA7CG8myP9VHteW2H+nO334yd2wCsYZqxHrkdjOa8tkv9udtvKlOuDSCXEoCkt0q6VVKfpEFBWXvy/Dr1MZrz2i715/3VUjOn7sL0zp188a9SXlVAtwBvAq7Oaf+Wg3a56DSb0ZzXZuzWaY2XSwKIiNsi4o489m358UWnPkZzXl1/bpBzG4CkK4GPR0TZin1Ji4HFALNnz563evXqBkVnteY2gPoY7Xlt9/rzdj++ajT8iWCSLgeePsRbn42In6d1rmSEBFDKjcCtz3+U9eHzuiPfbOyo4QPBIuKwem3bWlez9SVvFz6vO/LkdZXxOAAzazvucFCZvLqBLpS0Bngx8AtJl+YRh5m1J3c4qExevYCWR8SsiNgpInaLiNfmEYc1D48Qrp8inlv3cqqM5wKy3LnBrn6Kem6bcfK6ZuQ2AMudRwjXT5HPrUcJj8wJwHLnBrv68bm14TgBWO7cYFc/Prc2HCcAy10tG+yK2OA5HDeG2nA8HbQ1hVqMZC1qg+dIPErYmmo6aLOBatFgV+QGz+G4MdTKcQKwtuEGT7PqOAFY23CDp1l1nACsbbjB06w6HglcA25kaw4e/WlWHSeAMXLPk+biaZHNKucqoDFyzxMza1VOAGPknidm1qqcAMbIPU/MrFU5AYyRe56YWatyI/AYueeJmbUqJ4AacM8TM2tFrgIyMysoJwAzs4JyAjAzKygnADOzgnICMDMrKCcAM7OCcgIwMysoJwAzs4JyAjAzK6jWSgBr14K0/WflyuyndNmSJdm6M2ZsXzZvXrZs8eId1127Fi6+eMdly5Zl65YuW7AgW7ZgwY7LIVu/dNnFFw+Oc/HibN1587YvmzEjW7ZkiY/Jx+Rj8jHV95jKUESUfbPZdHV1RXd3d95hmJm1FEkrI6Jr4PLWKgGYmVnNOAGYmRWUE4CZWUE5AZiZFZQTgJlZQeWSACR9XdLtkv4oabmkp+QRh5lZkeVVArgM2DciXgD8Bfh0TnGYmRVWLgkgIn4TEb3p5fXArDziMDMrsmZ4JvB7gR+We1PSYmAxwOzZsxsVk5nVUV9f8PDmHnp6t9IxYTzTJncwbpzyDqtw6pYAJF0OPH2Itz4bET9P63wW6AUuKLediFgGLINsJHAdQjWzBurrC+5Yt4lF53ezZsMWZk2dxFnHdjF3t04ngQarWwKIiMOGe1/Su4AjgEOjleajMLMxeXhzz7aLP8CaDVtYdH43y4+fz/TOnXKOrlhyqQKSdDjwSeAVEfHPPGIws3z09G7ddvHvt2bDFnp6t+YUUXHl1QvodKATuEzSzZLOzCkOM2uwjgnjmTV10g7LZk2dRMeE8TlFVFx59QLaMyJ2j4j908/784jDzBpv2uQOzjq2a1sS6G8DmDa5I+fIiqcZegGZWYGMGyfm7tbJ8uPnuxdQzpwAzKzhxo2TG3ybgOcCMjMrKCcAM7OCcgIwMysoJwAzs4JyAjAzKygnADOzglIrTcMjaT2weoi3dgUeanA4zaToxw8+B0U/fvA5GO7494iI6QMXtlQCKEdSd0R05R1HXop+/OBzUPTjB5+D0Ry/q4DMzArKCcDMrKDaJQEsyzuAnBX9+MHnoOjHDz4HVR9/W7QBmJlZ9dqlBGBmZlVyAjAzK6iWTgCSDpd0h6S/SvpU3vE0mqTdJV0h6TZJt0r6cN4x5UHSeEk3Sbok71jyIOkpkn4i6fb0XXhx3jE1kqSPpu//LZK+L2nnvGOqN0nnSHpQ0i0ly54q6TJJd6Z/p460nZZNAJLGA/8X+BfgucBRkp6bb1QN1wt8LCKeAxwMfLCA5wDgw8BteQeRo9OAX0fEPsB+FOhcSJoJfAjoioh9gfHA2/ONqiHOAw4fsOxTwG8jYi/gt+n1sFo2AQAvBP4aEXdFRA/wA+DInGNqqIi4PyJWpd83kf3hz8w3qsaSNAt4PXB23rHkQdKTgJcD3wKIiJ6IeDTfqBpuAjBJ0gRgF2BtzvHUXURcDTwyYPGRwLfT798G3jjSdlo5AcwE7i15vYaCXfxKSZoDHAD8Pt9IGu6/gE8AfXkHkpNnAeuBc1M12NmSJucdVKNExH3AKcA9wP3APyLiN/lGlZvdIuJ+yG4OgaeN9IFWTgBDPUC0kH1aJU0BLgQ+EhEb846nUSQdATwYESvzjiVHE4ADgTMi4gBgMxUU/dtFquc+EngmMAOYLOmd+UbVOlo5AawBdi95PYsCFP0GkjSR7OJ/QUT8NO94Gmw+8AZJd5NVAb5K0nfzDanh1gBrIqK/5PcTsoRQFIcBf4+I9RHxBPBT4CU5x5SXdZKeAZD+fXCkD7RyArgR2EvSMyV1kDX8XJRzTA0lSWR1v7dFxKl5x9NoEfHpiJgVEXPI/v9/FxGFuvuLiAeAeyXNTYsOBf6cY0iNdg9wsKRd0t/DoRSoEXyAi4B3pd/fBfx8pA9MqGs4dRQRvZJOAC4la/k/JyJuzTmsRpsPHAP8SdLNadlnIuKXOcZkjfdvwAXpRugu4D05x9MwEfF7ST8BVpH1iruJAkwJIen7wCHArpLWAF8Avgb8SNJxZInxrSNux1NBmJkVUytXAZmZ2Rg4AZiZFZQTgJlZQTkBmJkVlBOAmVlBOQFYS5C0VdLNacbHH0vaZQzbOqR/5lBJbxhuJtk00+bxo9jHEkkfH22Mtd6O2VCcAKxVbImI/dOMjz3A+0vfVKbq73NEXBQRXxtmlacAVScAs1bgBGCt6BpgT0lz0vz3S8kGAu0u6TWSrpO0KpUUpsC2Z0fcLula4E39G5L0bkmnp993k7Rc0h/Sz0vIBtc8O5U+vp7WO1HSjZL+KOnfS7b12fR8isuBuQwg6cmS7u5PVGn06r2SJkpalLb5B0kXDlXCkXSlpK70+65pCoz+5yF8vSSm/5WWP0PS1SUlp5fV4uRb+3ACsJaSpvz9F+BPadFc4PySidBOAg6LiAOBbuB/pweEnAUsAF4GPL3M5r8BXBUR+5HNp3Mr2cRqf0uljxMlvQbYi2w68v2BeZJeLmke2XQUB5AlmIMGbjwi/gH8AXhFWrQAuLR/DpuIOCjt+zbguCpOy3Fks2AelPa7SNIzgXek7e9P9pyAm4fZhhVQy04FYYUzqWS6i2vI5kCaAayOiOvT8oPJHg60IpsWhg7gOmAfsgnD7gRIE8YtHmIfrwKOBYiIrcA/hniq0mvSz03p9RSyhNAJLI+If6Z9lJuX6ofAvwJXkCWMpWn5vpK+RFblNIVsipNKvQZ4gaS3pNdPTjHdCJyTJgz8WUQ4AdgOnACsVWxJd7LbpHGAgdcAAAGGSURBVIv85tJFwGURcdSA9fandlOFC/hqRHxzwD4+UuE+LgK+KumpwDzgd2n5ecAbI+IPkt5NNs/LQL1sL7WXPvZQwL9FxKCkIenlZA/M+Y6kr0fE+RXEaAXhKiBrJ9cD8yXtCdvq2PcGbgeeKenZab2jynz+t8AH0mfHK3va1iayu/t+lwLvLWlbmCnpacDVwEJJkyR1klXvDBIRjwE3kD3G8ZJU0iDt4/50t350mfjuJksaAG8pWX4p8IH0WSTtLWmypD3InpdwFlmJqUjTRFsFXAKwthER69Pd8/cl7ZQWnxQRf5G0GPiFpIeAa4F9h9jEh4FlaTbFrcAHIuI6SSuUPXz7V6kd4DnAdakE8hjwzohYJemHZPXsq8mqqcr5IfBjdrzL/xzZ09xWk7VvdA7+GKeQzfZ4DNtLDpA9DnMOsEpZUOvJHgd4CHCipCdSnMcOE5MVkGcDNTMrKFcBmZkVlBOAmVlBOQGYmRWUE4CZWUE5AZiZFZQTgJlZQTkBmJkV1P8AeesNUf++8w4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def residual_plot(df):\n",
    "    RMSE = np.std(sim_data.resids)\n",
    "    sns.scatterplot(x='predicted', y='resids', data=sim_data)\n",
    "    plt.axhline(0.0, color='red', linewidth=1.0)\n",
    "    plt.axhline(2.0*RMSE, color='red', linestyle='dashed', linewidth=1.0)\n",
    "    plt.axhline(-2.0*RMSE, color='red', linestyle='dashed', linewidth=1.0)\n",
    "    plt.title('PLot of residuals vs. predicted')\n",
    "    plt.xlabel('Predicted values')\n",
    "    plt.ylabel('Residuals')\n",
    "    \n",
    "residual_plot(sim_data)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept, Slope : Intercept    0.472582\n",
      "x            0.947984\n",
      "dtype: float64\n",
      "Intercept t-value, Slope t-value: Intercept     1.848916\n",
      "x            21.522124\n",
      "dtype: float64\n",
      "\n",
      "Hypothesis test summary for each coefficient if they differ from zero:\n",
      "Slope:\n",
      "                             Test for Constraints                             \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "c0             0.4726      0.256      1.849      0.071      -0.041       0.986\n",
      "==============================================================================\n",
      "Intercept:\n",
      "                             Test for Constraints                             \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "c0             1.4206      0.219      6.493      0.000       0.981       1.860\n",
      "==============================================================================\n",
      "\n",
      "SSE, SST, SSR, and RMSE:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'y_output' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-58-a2a69ff27442>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\nSSE, SST, SSR, and RMSE:'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[0mmean_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[0msst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_output\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mmean_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[0msse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msst\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mssr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_output' is not defined"
     ]
    }
   ],
   "source": [
    "import statsmodels.formula.api as sm\n",
    "import seaborn as sns\n",
    "\n",
    "ols_model = sm.ols(formula = 'y ~ x', data=sim_data)\n",
    "# Alternatively:\n",
    "#ols_model = sm.OLS(y_output, sm.add_constant(x_input))\n",
    "\n",
    "results = ols_model.fit()\n",
    "\n",
    "# Get slope (m) and y-intercept (b)\n",
    "print('Intercept, Slope : {}'.format(results.params))\n",
    "\n",
    "# Get the t-values (hypothesis test statistics) for linear regression coefficient hypothesis tests.\n",
    "print('Intercept t-value, Slope t-value: {}'.format(results.tvalues))\n",
    "\n",
    "# Get p-values for above t-value statistics\n",
    "print('\\nHypothesis test summary for each coefficient if they differ from zero:')\n",
    "print('Slope:')\n",
    "print(results.t_test([1,0]))\n",
    "print('Intercept:')\n",
    "print(results.t_test([1,1]))\n",
    "\n",
    "print('\\nSSE, SST, SSR, and RMSE:')\n",
    "mean_y = np.mean(y_output)\n",
    "sst = np.sum((y_output - mean_y)**2)\n",
    "sse = sst - results.ssr\n",
    "print('SSE: {}'.format(sse))\n",
    "print('SST: {}'.format(sst))\n",
    "print('SSR: {}'.format(results.ssr))\n",
    "print('RMSE: {}'.format(np.sqrt(results.mse_model)))\n",
    "\n",
    "# Get most of the linear regression statistics we are interested in:\n",
    "print(results.summary())\n",
    "\n",
    "# Plot a histogram of the residuals\n",
    "sns.distplot(results.resid, hist=True)\n",
    "plt.xlabel('Residual')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Residual Histogram')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Turn:** Create a regression moodel from synthetic data with intercept of 0 and maximum value at ${x = 10, y = 10}$, and with a the error having a standard deviation of 5. Plot the result of your model. How does this slope and intercept of this model compare to the model from the data with a standard deviation of 1? **Hint:** You need need to add columns named `score` and `resids` to the data frame before you create the plots. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.119895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.204082</td>\n",
       "      <td>1.171974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.408163</td>\n",
       "      <td>-4.757100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.612245</td>\n",
       "      <td>-0.341797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.816327</td>\n",
       "      <td>-1.857624</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          x         y\n",
       "0  0.000000 -2.119895\n",
       "1  0.204082  1.171974\n",
       "2  0.408163 -4.757100\n",
       "3  0.612245 -0.341797\n",
       "4  0.816327 -1.857624"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Paramters of generated data\n",
    "n_points = 50\n",
    "x_start, x_end = 0, 10\n",
    "y_start, y_end = 0, 10\n",
    "y_sd = 5\n",
    "\n",
    "# Generate data columns\n",
    "x_data = np.linspace(x_start, x_end, n_points)\n",
    "y_error = np.random.normal(loc=0, scale=y_sd, size=n_points)\n",
    "y_data = np.linspace(y_start, y_end, n_points) + y_error\n",
    "\n",
    "# Put data in dataframe\n",
    "reg_data_5 = pd.DataFrame({'x':x_data, 'y':y_data})\n",
    "\n",
    "reg_data_5.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept, Slope : Intercept   -0.468371\n",
      "x            1.336158\n",
      "dtype: float64\n",
      "Intercept t-value, Slope t-value: Intercept   -0.381263\n",
      "x            6.311547\n",
      "dtype: float64\n",
      "\n",
      "Hypothesis test summary for each coefficient if they differ from zero:\n",
      "Slope:\n",
      "                             Test for Constraints                             \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "c0            -0.4684      1.228     -0.381      0.705      -2.938       2.002\n",
      "==============================================================================\n",
      "Intercept:\n",
      "                             Test for Constraints                             \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "c0             0.8678      1.052      0.825      0.413      -1.247       2.982\n",
      "==============================================================================\n",
      "\n",
      "SSE, SST, SSR, and RMSE:\n",
      "SSE: 774.2449689021739\n",
      "SST: 1707.1727908168316\n",
      "SSR: 932.9278219146577\n",
      "RMSE: 27.825257750866815\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.454\n",
      "Model:                            OLS   Adj. R-squared:                  0.442\n",
      "Method:                 Least Squares   F-statistic:                     39.84\n",
      "Date:                Thu, 13 Feb 2020   Prob (F-statistic):           8.37e-08\n",
      "Time:                        17:49:13   Log-Likelihood:                -144.10\n",
      "No. Observations:                  50   AIC:                             292.2\n",
      "Df Residuals:                      48   BIC:                             296.0\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept     -0.4684      1.228     -0.381      0.705      -2.938       2.002\n",
      "x              1.3362      0.212      6.312      0.000       0.911       1.762\n",
      "==============================================================================\n",
      "Omnibus:                        0.990   Durbin-Watson:                   1.585\n",
      "Prob(Omnibus):                  0.610   Jarque-Bera (JB):                1.045\n",
      "Skew:                           0.253   Prob(JB):                        0.593\n",
      "Kurtosis:                       2.504   Cond. No.                         11.7\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'x vs y')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Fit the model and get the linear model summaries/plots.\n",
    "x5_input = reg_data_5['x']\n",
    "y5_output = reg_data_5['y']\n",
    "ols5_model = sm.ols(formula='y ~ x', data = reg_data_5)\n",
    "\n",
    "results5 = ols5_model.fit()\n",
    "\n",
    "# Get slope (m) and y-intercept (b)\n",
    "print('Intercept, Slope : {}'.format(results5.params))\n",
    "\n",
    "# Get the t-values (hypothesis test statistics) for linear regression coefficient hypothesis tests.\n",
    "print('Intercept t-value, Slope t-value: {}'.format(results5.tvalues))\n",
    "\n",
    "# Get p-values for above t-value statistics\n",
    "print('\\nHypothesis test summary for each coefficient if they differ from zero:')\n",
    "print('Slope:')\n",
    "print(results5.t_test([1,0]))\n",
    "print('Intercept:')\n",
    "print(results5.t_test([1,1]))\n",
    "\n",
    "print('\\nSSE, SST, SSR, and RMSE:')\n",
    "mean5_y = np.mean(y5_output)\n",
    "sst5 = np.sum((y5_output - mean5_y)**2)\n",
    "sse5 = sst5 - results5.ssr\n",
    "print('SSE: {}'.format(sse5))\n",
    "print('SST: {}'.format(sst5))\n",
    "print('SSR: {}'.format(results5.ssr))\n",
    "print('RMSE: {}'.format(np.sqrt(results5.mse_model)))\n",
    "\n",
    "# Get most of the linear regression statistics we are interested in:\n",
    "print(results5.summary())\n",
    "\n",
    "# Plot a histogram of the residuals\n",
    "y5_pred = ols5_model.fit().predict(x5_input)\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.distplot(results5.resid, hist=True)\n",
    "plt.grid(True)\n",
    "plt.xlabel('Residual')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Residual Histogram')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(x5_input, y5_output)\n",
    "plt.plot(x5_input, y5_pred, linewidth=2)\n",
    "plt.grid(True)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('x vs y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of regression models\n",
    "\n",
    "Now that you have built a regression model, let's look at how you can quantitatively evaluate the performance of a regression model. The evaluation of regression models is based on measurements of the errors. The errors of a regression model can be visualized as shown in the figure below. \n",
    "\n",
    "<img src=\"img/Errors.jpg\" alt=\"Regression_Errors\" style=\"width: 450px;\"/>\n",
    "\n",
    "<center>**Measuring errors for a regression model**\n",
    "$$Where\\\\\n",
    "Y = [y_1, y_2, \\ldots, y_n]\\\\\n",
    "and\\\\\n",
    "y_i = ith\\ data\\ value\\\\\n",
    "\\bar{Y} = mean(Y)\\\\\n",
    "\\\\\\hat{y_i} = regression\\ estimate\\ of\\ y_i\\\\\n",
    "SSE = sum\\ square\\ explained\\ = \\Sigma_i{(\\hat{y_i} - \\bar{Y})^2}\\\\\n",
    "SSR = sum\\ square\\ residual\\ = \\Sigma_i{(y_i - \\hat{y_i})^2}\\\\\n",
    "SST = sum\\ square\\ total\\ = \\Sigma_i(y_i - \\bar{Y})^2$$\n",
    "\n",
    "The goal of regression is to minimize the residual error, $SSR$. Specifically we wish to explain the maximum amount of the variance in the original data as possible with our model. We can quantify this idea with coeficient of determination also known as $R^2$.\n",
    "\n",
    "$$R^2 = 1 - \\frac{SSR}{SST}\\\\\n",
    "so\\ as\\\\\n",
    "SSR \\rightarrow 0\\\\\n",
    "R^2 \\rightarrow 1$$\n",
    "\n",
    "In words, $R^2$ is the fraction of the variance of the original data explained by the model. A model with perfectly explain the data has $R^2 = 1$. A model which does not explain the data at all has $R^2 = 0$.\n",
    "\n",
    "However, there are two problems with $R^2$. </center>\n",
    " - $R^2$ is not bias adjusted for degrees of freedom.\n",
    " - More importantly, there is no adjustment for the number of model parameters. As the number of model parameters increases $SSR$ will generally decrease. Without an adjustment you will get a false sense of model performance.\n",
    " \n",
    "To addresses these related issues, we can use adjusted $R^2$.\n",
    "\n",
    "$$R^2_{adj} = 1 - \\frac{\\frac{SSR}{df_{SSR}}}{\\frac{SST}{df_{SST}}} = 1 - \\frac{var_{residual}}{var_{total}}\\\\\n",
    "where\\\\\n",
    "df_SSR = SSR\\ degrees\\ of\\ fredom\\\\\n",
    "df_SST = SST\\ degrees\\ of\\ fredom$$\n",
    "\n",
    "This gives $R^2_{adj}$ as:\n",
    "\n",
    "$$R^2_{adj} = 1 - (1 - R^2) \\frac{n - 1}{n - k}\\\\ \n",
    "where\\\\\n",
    "n = number\\ of\\ data\\ samples\\\\\n",
    "k = number\\ of\\ model\\ coeficients$$\n",
    "\n",
    "Or, we can rewrite $R^2_{adj}$ as:\n",
    "\n",
    "$$R^2_{adj} =  1.0 - \\frac{SSR}{SST}  \\frac{n - 1}{n - 1 - k}$$\n",
    "\n",
    "Another measure of regression performance is root mean square error or $RMSE$:\n",
    "\n",
    "$$RMSE = \\sqrt{ \\frac{\\Sigma^n_{i-1} (y_i - \\hat{y_i})^2}{n}} = \\frac{\\sqrt{SSR}}{n}$$\n",
    "\n",
    "**Your Turn:** Examine the performance metrics for the previous two regressions. How do SSE, SSR, SST, $R^2$, and RMSE compare?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling Data\n",
    "\n",
    "When performing regression with numeric variables you will almost **always scale the data**.  Scaling data is important not just for regression, but most other machine learning models. Some reasons to scale regression data include:\n",
    "\n",
    "- The intercept may be a long way from the actual data. With scaled features, the intercept is at the centroid of the distribution. \n",
    "- Scaling prevents features with a large numerical range from overwhelming featuures with small numerical values. Numerical range is not an indicator of feature importance!\n",
    "\n",
    "There are several possibile approaches to scaling data:\n",
    " - Scale the features or independent variables. This is the most common practice.\n",
    " - Scale the label or dependent variable.\n",
    " - Scale both, which is another common practice\n",
    " \n",
    "In this case, we will just scale the one feature. Execute the code in the cell below and examine the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scale function for a column in a pandas df\n",
    "def scale(col):\n",
    "    mean_col = np.mean(col)\n",
    "    sd_col = np.std(col)\n",
    "    std = (col - mean_col) / sd_col\n",
    "    return std\n",
    "\n",
    "# Add scaled x to data frame\n",
    "sim_data['x_scale'] = scale(sim_data['x'])\n",
    "\n",
    "sim_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the new `x_scale` feature has some additional attributes. These attributes are used to scale new data on which you are making predicitons. This model **will not work on unscaled** data. \n",
    "\n",
    "Run the code in the cell below to create and evaluate a regression model using the scaled data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x_scaled = sim_data['x_scale']\n",
    "ols_model = sm.ols(formula='y ~ x_scale', data = sim_data)\n",
    "\n",
    "results = ols_model.fit()\n",
    "\n",
    "# Get slope (m) and y-intercept (b)\n",
    "print('Scaled: Intercept, Slope : {}'.format(results.params))\n",
    "\n",
    "# Get the t-values (hypothesis test statistics) for linear regression coefficient hypothesis tests.\n",
    "print('Scaled: Intercept t-value, Slope t-value: {}'.format(results.tvalues))\n",
    "\n",
    "# Get p-values for above t-value statistics\n",
    "print('\\nHypothesis test summary for each coefficient if they differ from zero:')\n",
    "print('Slope:')\n",
    "print(results.t_test([1,0]))\n",
    "print('Intercept:')\n",
    "print(results.t_test([1,1]))\n",
    "\n",
    "print('\\nScaled: SSE, SST, SSR, and RMSE:')\n",
    "mean_y = np.mean(y_output)\n",
    "sst = np.sum((y_output - mean_y)**2)\n",
    "sse = sst - results.ssr\n",
    "print('SSE: {}'.format(sse))\n",
    "print('SST: {}'.format(sst))\n",
    "print('SSR: {}'.format(results.ssr))\n",
    "print('RMSE: {}'.format(np.sqrt(results.mse_model)))\n",
    "\n",
    "# Get most of the linear regression statistics we are interested in:\n",
    "print(results.summary())\n",
    "\n",
    "# Plot a histogram of the residuals\n",
    "sns.distplot(results.resid, hist=True)\n",
    "plt.xlabel('Residual')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Scaled Residual Histogram')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine these results and compare them to the results for the unscaled regression. Which performance statistics are the same and which are different?\n",
    "\n",
    "# Your Turn:\n",
    "\n",
    "In the cell below use the data you created earlier to compute and evaluate a regression model using a scaled feature. Which performance metrics are the same and which are different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get scaled x\n",
    "scaled_x5_input = scale(x5_input)\n",
    "reg_data_5['x_scaled'] = scaled_x5_input\n",
    "\n",
    "# Fit the model and get the linear model summaries/plots.\n",
    "ols5_model_scaled = sm.ols(formula='y ~ x_scaled', data = reg_data_5)\n",
    "\n",
    "results5_scaled = ols5_model_scaled.fit()\n",
    "\n",
    "# Get slope (m) and y-intercept (b)\n",
    "print('Intercept, Slope : {}'.format(results5_scaled.params))\n",
    "\n",
    "# Get the t-values (hypothesis test statistics) for linear regression coefficient hypothesis tests.\n",
    "print('Intercept t-value, Slope t-value: {}'.format(results5_scaled.tvalues))\n",
    "\n",
    "# Get p-values for above t-value statistics\n",
    "print('\\nHypothesis test summary for each coefficient if they differ from zero:')\n",
    "print('Slope:')\n",
    "print(results5_scaled.t_test([1,0]))\n",
    "print('Intercept:')\n",
    "print(results5_scaled.t_test([1,1]))\n",
    "\n",
    "print('\\nSSE, SST, SSR, and RMSE:')\n",
    "mean5_scaled_y = np.mean(y5_output)\n",
    "sst5_scaled = np.sum((y5_output - mean5_scaled_y)**2)\n",
    "sse5_scaled = sst5_scaled - results5_scaled.ssr\n",
    "print('SSE: {}'.format(sse5_scaled))\n",
    "print('SST: {}'.format(sst5_scaled))\n",
    "print('SSR: {}'.format(results5_scaled.ssr))\n",
    "print('RMSE: {}'.format(np.sqrt(results5_scaled.mse_model)))\n",
    "\n",
    "# Get most of the linear regression statistics we are interested in:\n",
    "print(results5_scaled.summary())\n",
    "\n",
    "# Plot a histogram of the residuals\n",
    "y5_pred_scaled = ols5_model_scaled.fit().predict(scaled_x5_input)\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.distplot(results5_scaled.resid, hist=True)\n",
    "plt.grid(True)\n",
    "plt.xlabel('Residual')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Residual Histogram (Scaled)')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(scaled_x5_input, y5_output)\n",
    "plt.plot(scaled_x5_input, y5_pred_scaled, linewidth=2)\n",
    "plt.grid(True)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('x vs y (Scaled)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression assumptions\n",
    "\n",
    "At this point we should discuss a few key assumtions of linear regression. Keep these points in mind whenever you used these models. \n",
    "\n",
    "- There is a **linear relationship** between dependent variable and the **coeficients** of the independent variables.\n",
    "- Measurement error is independent and random. Technically, we say that the error is **independent identical distribution, or iid**.\n",
    "- Errors arise from the dependent variable only.\n",
    "- There is no multicolinearity. In other words, there is no significant correlation between the independent variables.\n",
    "- Residuals are **homoscedastic** (constant variance).  In other words, the errors are the same across all groups of independent variables. The opposite of homoscedastic is **heteroscedastic**, where there is systematic variation in the residuals with label values.\n",
    "\n",
    "The diagram below illustrates the iid errors for the dependent variable only.\n",
    "\n",
    "![](img/IndependentErrors.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Linear regressions are not just for straight lines\n",
    "\n",
    "A linear model is linear in its coeficients, but that does not mean we are limited to straight lines, **a common misconception**.  In fact, a linear model need only be linear in its coeficients. A **non-comprehensive** list of functions which can be included in a linear model includes:\n",
    "\n",
    "- Polynomials, but beware of polynomials of degree 3 or above.\n",
    "- Splines and smoothing kernels.\n",
    "- trigonometric functions.\n",
    "- Logrithmic and expontential functions.\n",
    "- Interaction terms, which are the product of feature values. For example, the two-way interaction of `var1` and `var2` is specified at `var1:var2`, or `var1*var2`. Adding a third variable, `var3` the three-way interaction, including all two-way interactions is modeled as `var1:var2:var3`. \n",
    "\n",
    "To clarify these concepts, let's look at an example. The code in the cell below computes a curved line using a second order polynomial with coeficients `c1 and c2` and adds Normally distributed noise.  Notice that the polynomial is defined by a linear sum of the components, defined by the coeficients. **Pay attention to the scaling of the features.** Run this code and have a look at the head of the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_data_poly(x1, y1, x2, y2, c1=1.0, c2=0.5, n=50, sd=3):\n",
    "    # Create x-sequence\n",
    "    x_data = np.linspace(x1, x2, n)\n",
    "    # Create y-sequence\n",
    "    y_data = np.linspace(y1, y2, n)\n",
    "    # Create curved y-data\n",
    "    error = np.random.normal(loc=0, scale=sd, size=n)\n",
    "    y_curved = c1 * y_data + c2 * y_data**2 + error\n",
    "    # Scale x:\n",
    "    x_scaled = scale(x_data)\n",
    "    # Create pandas dataframe\n",
    "    df = pd.DataFrame({'x': x_scaled,\n",
    "                       'y': y_curved})\n",
    "    return(df)\n",
    "\n",
    "reg_data_poly = sim_data_poly(0, 0, 10, 10, n=50, sd=3)\n",
    "\n",
    "reg_data_poly.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you will compute a linear polynomial model for these data. The code in the  cell below uses the `I()` function which literally **Interprets** the argument. In this case `I(x^2)` is interpreted as the second order polynomial term. Run this code and examine the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mod_poly = sm.ols(formula = 'y ~ x + I(x**2)', data = reg_data_poly).fit()\n",
    "\n",
    "# Get most of the linear regression statistics we are interested in:\n",
    "print(mod_poly.summary())\n",
    "\n",
    "# Plot a histogram of the residuals\n",
    "y5_pred_poly = mod_poly.predict(reg_data_poly['x'])\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.distplot(mod_poly.resid, hist=True)\n",
    "plt.grid(True)\n",
    "plt.xlabel('Residual')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Residual Histogram (Polynomial)')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(reg_data_poly['x'], reg_data_poly['y'])\n",
    "plt.plot(reg_data_poly['x'], y5_pred_poly, linewidth=2)\n",
    "plt.grid(True)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('x vs y (Polynomial)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "import statsmodels.api as statsmodels\n",
    "from statsmodels.graphics.regressionplots import *\n",
    "\n",
    "# Residuals vs Fitted Values\n",
    "residuals = mod_poly.resid\n",
    "fitted_vals = mod_poly.predict(reg_data_poly['x'])\n",
    "plt.plot(fitted_vals, residuals, 'o')\n",
    "plt.xlabel('Fitted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residuals vs. Fitted Values')\n",
    "\n",
    "# Create Q-Q Normal Plot of the Residuals\n",
    "statsmodels.qqplot(residuals, stats.norm, fit=True, line='45')\n",
    "\n",
    "# Fitted Values vs. Square Root of the Standardized Residuals\n",
    "standardized_resid = (mod_poly.resid - np.min(mod_poly.resid)) / np.max(mod_poly.resid)\n",
    "\n",
    "# Leverage Plot (Cook's Distance)\n",
    "influence_plot(mod_poly)\n",
    "\n",
    "# Additional stats models plots\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "fig = statsmodels.graphics.plot_regress_exog(mod_poly, \"x\", fig=fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is quite a bit of new information both plotted and in the tables. Let's step through what all this means.\n",
    "\n",
    "- The plot of the data and the regression line. Look at this plot and try to decide if the fit is reasonably good.\n",
    "- The histogram of the residuals. Do these residuals appear to be close to Normally distributed?\n",
    "- A plot of fitted values (y in this case) vs. the residuals. Note the fitted smoothing regression line. Ideally, the distribution of residuals should not change with fitted values. \n",
    "- A Q-Q Normal plot of the residuals. Do these residuals appear to be close to Normally distributed?\n",
    "- A plot of fitted values vs. the square root of the standardized residuals. Note the fitted smoothing regression line. Ideally, the distribution of residuals should not change with fitted values and should be in the range $0 \\le \\sqrt{stdresid} \\le 1.5$ standard deviations. \n",
    "- The statistics we have already discussed.\n",
    "- The report from the R `summary` method.\n",
    "  - The model formula.\n",
    "  - Summary statistics of the residuals.\n",
    "  - For each model coeficient, 1) the value of the coeficient, 2) the standard error of the coeficient, 3) the t statistic for the coeficient, and 4) the p-value of the coeficient. The null hypothesis for the coeficient is that it is 0, and not contributing to the model.\n",
    "  - The standard error of the residuals, defined as:\n",
    "$$rse = \\frac{\\Sigma^N_i(y_i - \\hat{y_i})^2}{df} = \\frac{\\Sigma^N_i(y_i - \\hat{y_i})^2}{N - k}\\\\\n",
    "where\\\\\n",
    "k = number\\ of\\ model\\ parameters\n",
    "$$\n",
    "  - $R^2$ and $R^2_{adj}$.\n",
    "  - The F statistic and p-value for the model. The null hypthesis is that the model is not explaining the data, or that the distribution of residuals is the same as the distribution of the original data. \n",
    "- A leverage plot showing cooks distance. More on this latter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your Turn:\n",
    "\n",
    "Compute a linear model using a straight line for the polynomial curve data. Compare the plots and the performance metrics. **Use a different model name and copy the dataframe to a new name so the notebook works correctly**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mod_poly_linear = sm.ols(formula = 'y ~ x', data = reg_data_poly).fit()\n",
    "\n",
    "# Get most of the linear regression statistics we are interested in:\n",
    "print(mod_poly_linear.summary())\n",
    "\n",
    "# Plot a histogram of the residuals\n",
    "y5_pred_poly_linear = mod_poly_linear.predict(reg_data_poly['x'])\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.distplot(mod_poly_linear.resid, hist=True)\n",
    "plt.grid(True)\n",
    "plt.xlabel('Residual')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Residual Histogram (Linear on Polynomial)')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(reg_data_poly['x'], reg_data_poly['y'])\n",
    "plt.plot(reg_data_poly['x'], y5_pred_poly_linear, linewidth=2)\n",
    "plt.grid(True)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('x vs y (Linear on Polynomial)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling Revisited\n",
    "\n",
    "# Your Turn:\n",
    "\n",
    "Now that you have worked with scaled and unscaled models and the various summary statistics try this exercise. Use the various summary and plotting capabilities we have demonstrated to compute model evaluations for the two (scaled and unscaled feature) straight line regression models you computed. Compare these results noticing the diffences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homoscedastic and Heteroscedastic Errors\n",
    "\n",
    "Let's elaborate on some of the assumptions for the linear model. \n",
    "\n",
    "$$y_i = mx_i + b + \\epsilon_i \\\\\n",
    "where \\\\\n",
    "\\epsilon_i = N(0, \\sigma)$$\n",
    "\n",
    "In this model the errors, $\\epsilon_i$, do not depend on the dependent variable `y`. In this case we say the errors are **homoscedastic**.\n",
    "\n",
    "But what if:\n",
    "\n",
    "$$\\epsilon_i = N(0, f(x_i))\\\\\n",
    "such\\ as\\\\\n",
    "\\epsilon_i = N(0, e^{x_i})$$\n",
    "\n",
    "These errors are now **heteroscedastic**, with the errors dependent on `x` and hence not constant in `y`.\n",
    "\n",
    "Let's look at an example. In the code below the error increases linerly as `x` increases. Run this code and examine the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Paramters of generated data\n",
    "n_points = 50\n",
    "x_start, x_end = 0, 10\n",
    "y_start, y_end = 0, 10\n",
    "y_sd = 5\n",
    "\n",
    "# Generate data columns\n",
    "x_data = np.linspace(x_start, x_end, n_points)\n",
    "y_error = np.random.normal(loc=0, scale=y_sd, size=n_points)\n",
    "# Now add heteroscedasticity to y_error\n",
    "y_error = y_error * np.linspace(0, 10, n_points)\n",
    "y_data = np.linspace(y_start, y_end, n_points) + y_error\n",
    "\n",
    "# Put data in dataframe\n",
    "het_data = pd.DataFrame({'x':x_data, 'y':y_data})\n",
    "\n",
    "het_data.head()\n",
    "\n",
    "#mod.het = lm(y ~ x, data = reg.data.het)\n",
    "#reg.data.het = reg.data.het\n",
    "#reg.data.het$score <- predict(mod.het, data = reg.data.het)\n",
    "#reg.data.het$resids <- reg.data.het$y - reg.data.het$score\n",
    "#plot.regression(reg.data.het, mod.het, k = 2)\n",
    "#summary(mod.het)\n",
    "#plot(mod.het)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit linear model and plot results/summaries\n",
    "het_linear = sm.ols(formula = 'y ~ x', data = het_data).fit()\n",
    "\n",
    "# Get most of the linear regression statistics we are interested in:\n",
    "print(het_linear.summary())\n",
    "\n",
    "# Plot a histogram of the residuals\n",
    "y5_pred_het = het_linear.predict(het_data['x'])\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.distplot(het_linear.resid, hist=True)\n",
    "plt.grid(True)\n",
    "plt.xlabel('Residual')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Residual Histogram (Heteroscedastic Error)')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(het_data['x'], het_data['y'])\n",
    "plt.plot(het_data['x'], y5_pred_het, linewidth=2)\n",
    "plt.grid(True)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('x vs y (Heteroscedastic Error)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary graphs:\n",
    "#import scipy.stats as stats\n",
    "#import statsmodels.api as statsmodels\n",
    "#from statsmodels.graphics.regressionplots import *\n",
    "\n",
    "# Residuals vs Fitted Values\n",
    "residuals = het_linear.resid\n",
    "fitted_vals = het_linear.predict(het_data['x'])\n",
    "plt.plot(fitted_vals, residuals, 'o')\n",
    "plt.xlabel('Fitted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residuals vs. Fitted Values')\n",
    "\n",
    "# Create Q-Q Normal Plot of the Residuals\n",
    "statsmodels.qqplot(residuals, stats.norm, fit=True, line='45')\n",
    "\n",
    "# Fitted Values vs. Square Root of the Standardized Residuals\n",
    "standardized_resid = (het_linear.resid - np.min(het_linear.resid)) / np.max(het_linear.resid)\n",
    "\n",
    "# Leverage Plot (Cook's Distance)\n",
    "influence_plot(het_linear)\n",
    "\n",
    "# Additional stats models plots\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "fig = statsmodels.graphics.plot_regress_exog(het_linear, \"x\", fig=fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the following about these results, which violate the homoscedastic error assumption:\n",
    "\n",
    "- The plot of residuals vs. the predicted value shows a systematic increase from left to right.\n",
    "- The Q-Q plot and the histogram show that the distribution of residuals has heavy tails and deviates from Normal.\n",
    "- The plot of the standardized residuals shows an increase from right to left on the plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leverage and Cook's Distance\n",
    "\n",
    "Up to now, we have only looked at regression models with Normally distributed noise or errors. But, in the real world there are errors and outliers in data. These errors and outliers can have greater or lesser effect, depending on how extreem they are and their placement with respect to the other data. \n",
    "\n",
    "You can imagine a regression line as a lever. Outliers that occur near the ends of the lever will have a greater influence all other factors being equal. \n",
    "\n",
    "One way to measure influence of a data point is Cook's distance, introduced by Dennis Cook in 1977. The influence for the `ith` data point can be computed as:\n",
    "\n",
    "$$D_i = \\frac{\\Sigma_{j=1}^n (\\hat{Y_j} - \\hat{Y_{j(i)}})^2}{n (p+1)\\hat{\\sigma^2}} \\\\\n",
    "where \\\\\n",
    "p = number\\ of\\ parameters\\\\\n",
    "n = number\\ of\\ data\\ points$$\n",
    "\n",
    "In effect, cooks distance compares the difference between means with and without a given data point. Computing Cook's distance can be moderately computationally intensive for large data set. Typically, Cook's distance is measured in units of standard deviation.\n",
    "\n",
    "Let's make these concepts concrete with an example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create new copy of original linear data\n",
    "outlier_data = sim_data[['x', 'y']].copy()\n",
    "# Add an outlier row\n",
    "outlier = pd.DataFrame([[0.0, 20.0]], columns = ['x', 'y'])\n",
    "outlier_data.append(outlier, ignore_index=True)\n",
    "\n",
    "# Scale data\n",
    "outlier_data['x_scaled'] = scale(outlier_data['x'])\n",
    "\n",
    "# Fit linear model\n",
    "outlier_linear = sm.ols(formula = 'y ~ x_scaled', data = outlier_data).fit()\n",
    "\n",
    "# Get most of the linear regression statistics we are interested in:\n",
    "print(outlier_linear.summary())\n",
    "\n",
    "# Plot a histogram of the residuals\n",
    "y_outlier = outlier_linear.predict(outlier_data['x_scaled'])\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.distplot(outlier_linear.resid, hist=True)\n",
    "plt.grid(True)\n",
    "plt.xlabel('Residual')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Residual Histogram (With 1 Outlier)')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(outlier_data['x_scaled'], outlier_data['y'])\n",
    "plt.plot(outlier_data['x_scaled'], y_outlier, linewidth=2)\n",
    "plt.grid(True)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('x vs y (With 1 Outlier)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Linear Regression Summary Plots\n",
    "# Summary graphs:\n",
    "#import scipy.stats as stats\n",
    "#import statsmodels.api as statsmodels\n",
    "#from statsmodels.graphics.regressionplots import *\n",
    "\n",
    "# Residuals vs Fitted Values\n",
    "residuals = outlier_linear.resid\n",
    "fitted_vals = outlier_linear.predict(outlier_data['x_scaled'])\n",
    "plt.plot(fitted_vals, residuals, 'o')\n",
    "plt.xlabel('Fitted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residuals vs. Fitted Values (w/ 1 outlier)')\n",
    "\n",
    "# Create Q-Q Normal Plot of the Residuals\n",
    "statsmodels.qqplot(residuals, stats.norm, fit=True, line='45')\n",
    "\n",
    "# Fitted Values vs. Square Root of the Standardized Residuals\n",
    "standardized_resid = (outlier_linear.resid - np.min(outlier_linear.resid)) / np.max(outlier_linear.resid)\n",
    "\n",
    "# Leverage Plot (Cook's Distance)\n",
    "influence_plot(outlier_linear)\n",
    "\n",
    "# Additional stats models plots\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "fig = statsmodels.graphics.plot_regress_exog(outlier_linear, \"x_scaled\", fig=fig)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
