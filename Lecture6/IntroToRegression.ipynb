{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Regression\n",
    "\n",
    "### Data Science 410\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The method of regression is one of the oldest and most widely used analytics methods. The goal of regression is to produce a model that represents the **best fit** to some observed data. Typically the model is a function describing some type of curve (lines, parabolas, etc.) that is determined by a set of parameters (e.g., slope and intercept). *Best fit* means that there is an optimal set of parameters which minimize an error criteria we choose.\n",
    "\n",
    "Regression models attempt to predict the value of one variable using the values of other variable. Unfortunately, the terminology used for these variables is not consistent across authors, statistical software packages, and application domains. The table below list some, but by no means all of the terms used for these variables. \n",
    "\n",
    "Predicted Variable | Variables Used to Predict\n",
    " --- | ---\n",
    " y | x \n",
    " Dependent | Independent \n",
    " Endogenous | Exogenous  \n",
    " Response | Predictor\n",
    " Response | Explanatory\n",
    " Label | Feature \n",
    " Regressand | Regressors  \n",
    " Outcome | Design\n",
    " Left Hand Side | Right Hand Side  \n",
    " \n",
    "\n",
    "Many machine learning models, including some of the latest deep learning methods, are a form of regression. **Linear regression** is the foundational form of regression. Linear regression minimizes squared error of the predictions of the dependent variable using the values of the independent variables. This approach is know as the **method of least squares**.\n",
    "\n",
    "By developing an understanding of linear regression, you are building a foundation to understand many other machine learning models. Nearly all machine learning methods suffer from the same problems, including over-fitting and mathematically unstable fitting methods. Understanding these problems in the linear regression context will help you work with other machine learning models.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## History\n",
    "\n",
    "Regression is based on the method of least squares or the method of minimum mean square error. The idea of averaging errors have been applied for nearly three centuries. The fist known publication of a *method of averages* was by the German astronomer Tobias Mayer in 1750. Lapace used a similar method which he published in 1788.\n",
    "\n",
    "<img src=\"img/TobiasMayer.jpg\" alt=\"TobiasMayer\" style=\"width: 200px;\"/>\n",
    "\n",
    "The first publication of the **method or least squares** was by the French mathematician Adrien-Marie Legendre in 1805. Legendre was a brilliant mathematician, known for his unpleasant personality.  \n",
    "\n",
    "![](img/Legendre.jpg)\n",
    "<center>Caricature of Legendre, published method of least squares</center>\n",
    "\n",
    "It is very likely that the German physicist and mathematician Gauss developed the method of least squares as early as 1795, but did not publish the method until 1809, aside from a reference in a letter in 1799. Gauss never disputed Legendre's priority in publication. Legendre did not return the favor, and opposed any notion that Gauss had used the method earlier. \n",
    "\n",
    "![](img/Carl_Friedrich_Gauss.jpg)\n",
    "<center>Carl Friedrich Gauss, early adopter of the least squares method</center>\n",
    "\n",
    "The first use of the term **regression** was by Francis Gaulton, a cousin of Charles Darwin, in 1886. Gaulton was interested in determining which traits of plants and animals, including humans, could be said to be inherited. Gaulton used the term **regression to the mean** to describe the natural processes he observed in inherited traits.  \n",
    "\n",
    "<img src=\"img/Francis_Galton.jpg\" alt=\"Drawing\" style=\"width:225px; height:250px\"/>\n",
    "<center>Francis Galton, inventor of regression</center>\n",
    "\n",
    "While Gaulton invented a modern form regression, it fell to Karl Pearson to put regression and multiple regression on a firm mathematical footing. Pearson's 1898 publication proposed a method of regression as we understand it today. \n",
    "\n",
    "Many others have expanded the theory of regression in the 120 years since Pearson's paper. Notably, Joseph Berkson published the logistic regression method in 1944, one of the first classification algorithms. In recent times the interest in machine learning has lead to a rapid increase in the numbers and types of regression models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Theory of Linear Regression\n",
    "\n",
    "We will focus on the theory of **linear models**, which are foundational. Key properties of linear models include:\n",
    "- Derived with linear algebra.\n",
    "- Include any model **linear in coefficients**, including polynomials, splines, Gaussian kernels and many other nonlinear function.    \n",
    "- Understanding linear models is basis for understanding behavior many other statistical or machine learning models.\n",
    "- Basis of many time series models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear model of a strait line\n",
    "\n",
    "Let's have a look at the simple case of a regression model for a straight line. For this example we will work with single regression with one feature and one label. The data are in the form of some number of values pairs, $\\{x_i,y_i \\}$. \n",
    "\n",
    "The goal of this regression model is to find a straight that best fits the observed data. We can define the line by two coefficients or **parameters**, the **slope** and the **intercept**. A general representation of this parameterization of a straight line is illustrated in the figure below.\n",
    "\n",
    "<img src=\"img/ymxb.jpg\" alt=\"y_equals_mx_plus_b\" style=\"width: 450px;\"/>\n",
    "<center>**Single regression model**</center>\n",
    "\n",
    "Where,  \n",
    "\n",
    "\\begin{align}\n",
    "m &= slope = \\frac{rise}{run} = \\frac{\\delta y}{\\delta x}\\\\\n",
    "and\\\\\n",
    "y &= b\\ at\\ x = 0\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "For each of the pairs of observed values, ${x_i,y_i}$, we can write the equation for the line with the errors as:\n",
    "\n",
    "\\begin{align}\n",
    "y_i &= mx_i + b + \\epsilon_i \\\\\n",
    "where \\\\\n",
    "\\epsilon_i &= error\n",
    "\\end{align}\n",
    "\n",
    "We can visualize these errors as shown in the figure below.\n",
    "\n",
    "<img src=\"img/LSRegression.jpg\" alt=\"LSRegression\" style=\"width: 450px;\"/>\n",
    "<center>Example of least squares regression with errors shown as vertical lines</center>\n",
    "\n",
    "We want to solve for $m$ and $b$ by minimizing the error, $\\epsilon_i$. We call this **least squares regression** problem.\n",
    "\n",
    "$$min \\Sigma_i \\epsilon^2 = min \\Sigma_i{ (y_i - (mx_i + b))^2}$$\n",
    "\n",
    "There are lots of computationally efficient algorithms for finding minimums of equations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression assumptions\n",
    "\n",
    "Now we should discuss a few key assumptions of linear regression. Keep these points in mind whenever you use these models. \n",
    "\n",
    "1. There is a **linear relationship** between dependent variable and the **coefficients** of the independent variables. This does not mean the function approximation used must be linear. Only that the model must be linear in the coefficients. \n",
    "2. Measurement error is independent and random. Technically, we say that the error is **independent identical distribution, or iid**.\n",
    "3. Errors arise from the dependent variable only. Other models, such as complete regression, must be used if there are errors in the independent variable. \n",
    "The diagram below illustrates the iid errors for the dependent variable only.\n",
    "\n",
    "![](img/IndependentErrors.jpg)\n",
    "\n",
    "4. There is no **multicolinearity** between the features or independent variables. In other words, there is no significant correlation between the features.\n",
    "5. The **residuals** are **homoscedastic** (constant variance).  In other words, the errors are the same across all values of the independent variables. We have explore this concept further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A First Regression Model\n",
    "\n",
    "Let's give regression a try. The code in the cell below computes data pairs along a straight line. Normally distributed noise is added to the data values. Run this code and examine the head of the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as nr\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as sm\n",
    "from statsmodels.graphics.regressionplots import influence_plot, plot_regress_exog\n",
    "import scipy.stats as ss\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramters of generated data\n",
    "n_points = 50\n",
    "x_start, x_end = 0, 10\n",
    "y_start, y_end = 0, 10\n",
    "y_sd = 1\n",
    "\n",
    "# Generate data columns\n",
    "nr.seed(5666)\n",
    "x_data = np.linspace(x_start, x_end, n_points) # The x values\n",
    "y_error = np.random.normal(loc=0, scale=y_sd, size=n_points) # The Normally distributed noise\n",
    "y_data = np.linspace(y_start, y_end, n_points) + y_error + 1.0 # The y values including an intercept\n",
    "\n",
    "# Put data in dataframe\n",
    "sim_data = pd.DataFrame({'x':x_data, 'y':y_data})\n",
    "\n",
    "sim_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you can visualize these data by executing the code in the cell below. Notice that the points nearly fall on a straight line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matplotlib may give some font errors when loading for the first time, you can ignore these\n",
    "plt.plot(sim_data['x'], sim_data['y'], 'ko')\n",
    "plt.grid(True)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('x vs y')\n",
    "plt.ylim(0,11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Fitting a Linear Regression Model\n",
    "\n",
    "Now, you are ready to build and evaluate a regression model using Python. There are a number of Python libraries that contain linear modeling capabilities.\n",
    "\n",
    "The [scikit-learn](https://scikit-learn.org/stable/) package has many different types of machine learning algorithms. Scikit-lean model interfaces take a machine learning perspective. The data arguments for Scikit-learn models are numpy arrays which must be dimensioned properly.\n",
    "\n",
    "Another Python package with linear model capability is [statsmodels](https://www.statsmodels.org/stable/index.html). This package takes a statistical perspective, which we adopt here. A nice feature of statsmodels is that the data argument is a Pandas data frame. \n",
    "\n",
    "You can specify statsmodels models using the [R-style model language](https://www.statsmodels.org/devel/example_formulas.html). If you are not familiar with the R model language interface, read the summary below before proceeding. For those that have experience with the programming language, R, statsmodels will seem the most similar because it provides a R-like model language interface. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **R-Style Model Formulas**    \n",
    "> The code in the cell below uses an R style model formula. This modeling language was introduced in [Chambers and Hastie, 1992, Statistical Models in S](https://www.taylorfrancis.com/books/e/9780203738535).     \n",
    "\n",
    "> For a good [**cheatsheet and summary of the R modeling language**](http://faculty.chicagobooth.edu/richard.hahn/teaching/formulanotation.pdf) look at the posting by Richard Hahn of the Chicago Booth School.    \n",
    "\n",
    "> Models are defined by an equation using the $\\sim$ symbol to mean *modeled by*. In summary, the variable to be modeled is always on the left. The relationship between the variable to be modeled on the right. This basic scheme can be written: \n",
    "\n",
    "$$dependent\\ variable\\sim indepenent\\ variables$$\n",
    "\n",
    "> For example, if the dependent variable (dv) is modeled by two independent variables (var1 and var2), with no interaction, the formula would be:\n",
    "$$dv \\sim var1 + var2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case, there is only one independent variable and one dependent variable. The code in the cell below does the following:  \n",
    "\n",
    "- The model formula is specified as $y \\sim x$.\n",
    "- An [ols (ordinary least squares)](http://www.statsmodels.org/stable/generated/statsmodels.regression.linear_model.OLS.html#statsmodels.regression.linear_model.OLS) model object is specified using the model formula and the data frame. Here, we use the lower case *ols* function so that the model language can be specified in the call. \n",
    "- The *fit* method is applied to the ols object. \n",
    "- The slope and intercept point estimates are printed. \n",
    "\n",
    "Execute this code and note the coefficient values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the regresson model and fit it to the data\n",
    "ols_model = sm.ols(formula = 'y ~ x', data=sim_data).fit()\n",
    "\n",
    "## Print the model coefficient\n",
    "print('Intercept = %4.3f  Slope = %4.3f' % (ols_model._results.params[0], ols_model._results.params[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The intercept and slope are close to the actual values of 1.0 and 1.0. However, we need a more thorough examination of the results before we can say this is a good model for these data.  \n",
    "\n",
    "As a first step toward evaluating this model, we can compute the predicted values of y given the values of x. Execute the code in the cell below which uses the *predict* method to compute these predicted values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add predicted to pandas dataframe\n",
    "sim_data['predicted'] = ols_model.predict(sim_data.x)\n",
    "# View head of data frame\n",
    "sim_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a single regression model, we can plot the values of the predicted line along with the actual data values on a 2-dimensional plot. For models with multiple features, [partial regression plots](https://en.wikipedia.org/wiki/Partial_regression_plot) can be created, but are more complex to understand. \n",
    "\n",
    "Execute the code in the cell below to create the plot and examine the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.lineplot(x='x', y='predicted', data=sim_data, color='red')\n",
    "sns.scatterplot(x='x', y='y', data=sim_data, ax=ax)\n",
    "_=ax.set_ylim(0,11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, so good. The predicted regression line does seem to fit the data well. But, how can we quantify the performance of this model? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of regression models\n",
    "\n",
    "Now that you have built a regression model, let's look at how you can quantitatively evaluate the performance of a regression model. There is no one metric that can be used to evaluate a linear model, or any other type of machine learning model. Here as in any other case, we will in fact use multiple metrics to evaluate the linear regression model. \n",
    "\n",
    "The evaluation of regression models is based on measurements of the errors. The errors of a regression model can be visualized as shown in the figure below. \n",
    "\n",
    "<img src=\"img/Errors.jpg\" alt=\"Regression_Errors\" style=\"width: 450px;\"/>\n",
    "<center>Measuring errors for a regression model</center>  \n",
    "    \n",
    "    \n",
    "Let's start with the observed values of the feature, $X$, and label, $Y$.      \n",
    "\n",
    "\\begin{align}\n",
    "X &= [x_1, x_2, \\ldots, x_n]\\\\\n",
    "Y &= [y_1, y_2, \\ldots, y_n]\\\\\n",
    "where\\\\\n",
    "x_i &= ith\\ feature\\ value\\\\\n",
    "y_i &= ith\\ label\\ value\\\\\n",
    "\\end{align}\n",
    "\n",
    "The results of the regression model are **estimates** which we write:   \n",
    "\n",
    "\\begin{align}\n",
    "\\bar{Y} &= mean(Y)\\\\\n",
    "\\hat{y_i} &= regression\\ estimate\\ of\\ y_i\n",
    "\\end{align}  \n",
    "\n",
    "Given the above we can define the follow **sum of squares** relationships:   \n",
    "\n",
    "\\begin{align}\n",
    "SSE &= sum\\ square\\ explained\\ = \\Sigma_i{(\\hat{y_i} - \\bar{Y})^2}\\\\\n",
    "SSR &= sum\\ square\\ residual\\ = \\Sigma_i{(y_i - \\hat{y_i})^2}\\\\\n",
    "SST &= sum\\ square\\ total\\ = \\Sigma_i(y_i - \\bar{Y})^2 \\\\\n",
    "SST &= SSR + SSE\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of regression is to minimize the residual error, $SSR$. In other words, when fitting the model we wish to explain the maximum amount of the variance in the original data. We can quantify the **faction of squared error explained** with the **coefficient of determination** also known as $R^2$. We can express $R^2$ as follows:\n",
    "\n",
    "$$R^2 = 1 - \\frac{SSR}{SST}$$\n",
    "\n",
    "The $R^2$ for a perfect model would behave as follows:   \n",
    "\n",
    "\\begin{align}\n",
    "SSR &\\rightarrow 0\\\\\n",
    "which\\ leads\\ to \\\\\n",
    "R^2 &\\rightarrow 1\n",
    "\\end{align}\n",
    "\n",
    "In words, a model which perfectly explains the data has $R^2 = 1$. A model which does not explain the data at all has: \n",
    "\n",
    "\\begin{align}\n",
    "SSR &= SST \\\\ \n",
    "and \\\\ \n",
    "R^2 &= 0\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, there are two problems with $R^2$. </center>\n",
    " - $R^2$ is not bias adjusted for degrees of freedom.\n",
    " - More importantly, there is no adjustment for the number of model parameters. As the number of model parameters increases $SSR$ will generally decrease. Without an adjustment you will get a false sense of model performance.    \n",
    " \n",
    "To addresses these related issues, we use **adjusted $R^2$**.\n",
    "\n",
    "\\begin{align}\n",
    "R^2_{adj} &= 1 - \\frac{\\frac{SSR}{df_{SSR}}}{\\frac{SST}{df_{SST}}} = 1 - \\frac{var_{residual}}{var_{total}}\\\\\n",
    "where\\\\\n",
    "df_{SSR} &= SSR\\ degrees\\ of\\ freedom\\\\\n",
    "df_{SST} &= SST\\ degrees\\ of\\ freedom\n",
    "\\end{align}\n",
    "\n",
    "This gives $R^2_{adj}$ as:\n",
    "\n",
    "\\begin{align}\n",
    "R^2_{adj} &= 1 - (1 - R^2) \\frac{n - 1}{n - k}\\\\ \n",
    "where\\\\\n",
    "n &= number\\ of\\ data\\ samples\\\\\n",
    "k &= number\\ of\\ model\\ coefficients\n",
    "\\end{align}\n",
    "\n",
    "Or, we can rewrite $R^2_{adj}$ as:\n",
    "\n",
    "$$R^2_{adj} =  1.0 - \\frac{SSR}{SST}  \\frac{n - 1}{n - 1 - k}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see an extensive summary of the fit of the linear model with the *summary* method. Execute the code in the cell below and examine the output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine some of these values.   \n",
    "\n",
    "- Starting with the **model coefficients**, the report includes an hypothesis test on the statistical significance of the model coefficients. In this case we can interpret this significance as follows: For both the **intercept** and **slope** the t-statistic is large, p-value is small, and the confidence interval does not include zero. These coefficients are statistically significant.\n",
    "- The **F-statistic** and **Prob (F-statistic)** are a measure of the significance of the model against a **null model that does not explain the data**. In this case the large F-statistic and small probability indicate that we can reject this null hypothesis and say the model is significant in terms of explaining the data.  \n",
    "- The $R^2$ value is shown in the upper right corner. The value of 0.91 indicates a relatively good model fit.   \n",
    "- The **adjusted $R^2$** shown in the summary indicates that the model is a good fit. To find this quantity, notice the following: \n",
    "  - The **number of observations** and is the $df_{SST}$. \n",
    "  - The **degrees of freedom residuals** is the $df_{SSR}$. \n",
    "  - Notice that $df_{SST} - df_{SSR} =$ number of model coefficients. \n",
    "  \n",
    "> **Warning:** The hypothesis tests on model coefficients suffer from the same problems of any hypothesis test. These problems are especially prevalent when the are large numbers of model parameters. For example, finding coefficients significant that are not, or vice versa is not uncommon. This situation can be aggravated when features have significant colinearity (correlation). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of Residuals\n",
    "\n",
    "There is one more important topic in evaluating regression models, the analysis of the **residuals**. The residuals of a regression model are the difference between the predicted values and actual values of the label. In other words, the residuals are the error term we write as $\\epsilon_i$ for the ith observation.\n",
    "\n",
    "A good linear regression model should have residuals with the following properties: \n",
    "\n",
    "1. The residuals should be approximately **Normally distributed with zero mean**. This criteria applied to any regression model using a least squares loss function. The least squares fitting criteria is only optimal for Normally distributed and zero mean residuals. We can express this important relationship mathematically as:  \n",
    "\n",
    "\\begin{align}\n",
    "y_i &  mx_i + b + \\epsilon_i \\\\\n",
    "where, \\\\\n",
    "\\epsilon_i &= N(0, \\sigma)\n",
    "\\end{align}\n",
    "\n",
    "2. The residuals should be **homoscedastic** with respect to the predicted values, $\\hat{Y}$. Homoscedastic residuals have constant variance, $\\sigma$ with respect to the predicted values. This criteria applies to any form of regression model. If this is not the case, we say that the residuals are **heteroscedastic**, with variance changing with respect to the predicted values. In other words, the variance is a function of the predicted values, $\\sigma(x_i) = f(x_i)$. A model with heteroscedastic residuals will have a better fit for small predicted values than large predicted values, or vice versa. We can write a model for hetroscedastic residuals as: \n",
    "\n",
    "\\begin{align}\n",
    "\\epsilon_i &= N(0, f(x_i))\\\\\n",
    "for\\ example\\\\\n",
    "\\epsilon_i &= N(0, e^{x_i})\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "To start our analysis of residuals, execute the code in the cell below to compute residuals for the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add residuals to pandas dataframe\n",
    "sim_data['resids'] = np.subtract(sim_data.predicted, sim_data.y)\n",
    "\n",
    "# View head of data frame\n",
    "sim_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can measure the dispersion of the residuals as a measure of regression performance. The metric is root mean square error or RMSE, which is very close to the standard deviation:\n",
    "\n",
    "$$RMSE = \\sqrt{ \\frac{\\Sigma^n_{i-1} (y_i - \\hat{y_i})^2}{n}} = \\frac{\\sqrt{SSR}}{n}$$\n",
    "\n",
    "We should also determine if the mean of the residuals is approximately 0. \n",
    "\n",
    "Execute the code in the cell below to compute and display the mean and RMSE for the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The mean of the residuals = %4.3f' % (np.mean(sim_data.resids)))\n",
    "## Print RMSE\n",
    "RMSE = np.std(sim_data.resids)\n",
    "print('RMSE = %4.3f' % (RMSE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a reasonable value for RMSE considering the scale of the lable, $\\{0,11\\}$. Further, the mean of the residuals is effectively 0. \n",
    "\n",
    "Next, we need to determine if the distribution of the residuals is approximately Normal. Execute the code in the cell below to plot a histogram and Q-Q Normal plot of the residuals.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_resid_dist(resids):\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))\n",
    "    ## Plot a histogram\n",
    "    sns.distplot(resids, bins=20, ax=ax[0])\n",
    "    ax[0].set_title('Histogram of residuals')\n",
    "    ax[0].set_xlabel('Residual values')\n",
    "    ## Plot the Q-Q Normal plot\n",
    "    ss.probplot(resids, plot = ax[1])\n",
    "    ax[1].set_title('Q-Q Normal plot of residuals')\n",
    "    plt.show()\n",
    "\n",
    "plot_resid_dist(sim_data.resids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_plot(df):\n",
    "    RMSE = np.std(df.resids)\n",
    "    sns.scatterplot(x='predicted', y='resids', data=df)\n",
    "    plt.axhline(0.0, color='red', linewidth=1.0)\n",
    "    plt.axhline(2.0*RMSE, color='red', linestyle='dashed', linewidth=1.0)\n",
    "    plt.axhline(-2.0*RMSE, color='red', linestyle='dashed', linewidth=1.0)\n",
    "    plt.title('PLot of residuals vs. predicted')\n",
    "    plt.xlabel('Predicted values')\n",
    "    plt.ylabel('Residuals')\n",
    "    plt.show()\n",
    "    \n",
    "residual_plot(sim_data)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Turn:** Create a regression model from synthetic data with intercept of 0 and maximum value at ${x = 10, y = 10}$, and with the error having a standard deviation of 5. \n",
    "\n",
    "As a first step, execute the code in the cell below to generated the simulated data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Paramters of generated data\n",
    "n_points = 50\n",
    "x_start, x_end = 0, 10\n",
    "y_start, y_end = 0, 10\n",
    "y_sd = 5\n",
    "\n",
    "# Generate data columns\n",
    "nr.seed(545454)\n",
    "x_data = np.linspace(x_start, x_end, n_points)\n",
    "y_error = np.random.normal(loc=0, scale=y_sd, size=n_points)\n",
    "y_data = np.linspace(y_start, y_end, n_points) + y_error + 1.0 \n",
    "\n",
    "# Put data in dataframe\n",
    "reg_data_5 = pd.DataFrame({'x':x_data, 'y':y_data})\n",
    "\n",
    "reg_data_5.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, in the cell below create and execute the code to define and fit the OLS model and print the summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model and get the linear model summaries/plots.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the model summary and consider the answers to these questions:\n",
    "1. Are both model coefficients still significant?    \n",
    "2. Is the model still significant based on the F-statistic? \n",
    "3. How have $R^2$ and adjusted $R^2$ changed and what does this mean in terms of the fit of the model? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create and execute code to do the following:   \n",
    "1. Compute and print the mean of the residuals.\n",
    "2. Compute and print the RMSE.\n",
    "3. Plot the histogram and Q-Q Normal plot of the residuals.\n",
    "4. Plot the residuals vs. the predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_data_5['predicted'] = ols_model.predict(reg_data_5.x)\n",
    "reg_data_5['resids'] = np.subtract(reg_data_5.predicted, reg_data_5.y)\n",
    "\n",
    "print('The mean of the residuals = %4.3f' % (np.mean(reg_data_5.resids)))\n",
    "## Print RMSE\n",
    "RMSE = np.std(reg_data_5.resids)\n",
    "print('RMSE = %4.3f' % (RMSE))\n",
    "\n",
    "plot_resid_dist(reg_data_5.resids)\n",
    "residual_plot(reg_data_5)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine these results and consider the answers to the following questions:\n",
    "1. How has the RMSE changed?\n",
    "2. Do the residuals appear to be approximately Normally distributed with zero mean?\n",
    "3. Are the residuals homoscedastic?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Linear regressions are not just for straight lines\n",
    "\n",
    "A linear model is linear in its coefficients. But, that does not mean we are limited to straight lines, **a common misconception**.  A partial list of functions which can be included in a linear model includes:\n",
    "\n",
    "- Polynomials, but beware of polynomials of degree 3 or above.\n",
    "- Splines and smoothing kernels.\n",
    "- trigonometric functions.\n",
    "- Logarithmic and exponential functions.\n",
    "- Interaction terms, which are the product of feature values. For example, the two-way interaction of var1 and var2. In the R modeling language, interactions are specified as $var1:var2$. You can express the using both varibles plus the interaction as, $var1*var2 = var1 + var2 + var1:var2$.  \n",
    "\n",
    "### An Example\n",
    "\n",
    "To clarify these concepts, let's look at an example. The code in the cell below computes a data set with a polynomial trend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramters of generated data\n",
    "n_points = 50\n",
    "x_start, x_end = 0, 10\n",
    "y_sd = 1\n",
    "\n",
    "# Generate data columns\n",
    "nr.seed(474747)\n",
    "x_data = np.linspace(x_start, x_end, n_points) # The x values\n",
    "y_error = np.random.normal(loc=0, scale=y_sd, size=n_points) # The Normally distributed noise\n",
    "y_data = x_data + 0.6 * np.square(x_data) + y_error + 1.0 # The y values including an intercept\n",
    "\n",
    "# Put data in dataframe\n",
    "sim_data_poly = pd.DataFrame({'x':x_data, 'y':y_data})\n",
    "\n",
    "sim_data_poly.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in the cell below computes an simple ordinary least squares regression model for the data just created. Exectute this code and note the values of the slope and intecept coefficients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the regresson model and fit it to the data\n",
    "ols_model = sm.ols(formula = 'y ~ x', data=sim_data_poly).fit()\n",
    "\n",
    "# Add predicted to pandas dataframe\n",
    "sim_data_poly['predicted'] = ols_model.predict(sim_data_poly.x)\n",
    "\n",
    "## Print the model coefficient\n",
    "print('Intercept = %4.3f  Slope = %4.3f' % (ols_model._results.params[0], ols_model._results.params[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a feel for how well this model fits the data, execute the code in the cell below to plot the regression line and the observations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.lineplot(x='x', y='predicted', data=sim_data_poly, color='red')\n",
    "sns.scatterplot(x='x', y='y', data=sim_data_poly, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_data_poly['resids'] = np.subtract(sim_data_poly.predicted, sim_data_poly.y)\n",
    "\n",
    "print('The mean of the residuals = %4.3f' % (np.mean(sim_data_poly.resids)))\n",
    "## Print RMSE\n",
    "RMSE = np.std(sim_data_poly.resids)\n",
    "print('RMSE = %4.3f' % (RMSE))\n",
    "\n",
    "plot_resid_dist(sim_data_poly.resids)\n",
    "residual_plot(sim_data_poly)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Polynomial Features\n",
    "\n",
    "Given the poor fit of the first model, it is time to try something else. In this case, we will try using polynomial terms. \n",
    "\n",
    "The code in the cell below adds second and third order polynomial features to the data frame. Execute the code.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_data_poly['x_square'] = np.square(sim_data_poly.x)\n",
    "sim_data_poly['x_cube'] = np.multiply(sim_data_poly.x, sim_data_poly.x_square)\n",
    "sim_data_poly.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Scaling Data\n",
    "\n",
    "When performing regression with multiple numeric features, you will almost **always scale the data**.  Scaling data is important not just for regression, but most other machine learning models. Some reasons to scale regression data include:\n",
    "\n",
    "- The intercept may be a long way from the actual data. With scaled features, the intercept is at the centroid of the distribution. \n",
    "- Scaling prevents features with a large numerical range from overwhelming featuures with small numerical values. Numerical range is not an indicator of feature importance!\n",
    "- Related to the above point, you may encounter convergence problems with many machine learning algorithms if the numeric range of the features is different. \n",
    "\n",
    "There are several possibile approaches to scaling data:\n",
    " - Scale the features or independent variables. This is the most common practice.\n",
    " - Scale the label or dependent variable. It is more common paractice to perform a transform the distribution to be closer to Normal. \n",
    " - Scale both features and label. \n",
    " \n",
    "In this case, we will just scale the features. Execute the code in the cell below and examine the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scale function for a column in a pandas df\n",
    "def scale(col):\n",
    "    mean_col = np.mean(col)\n",
    "    sd_col = np.std(col)\n",
    "    std = (col - mean_col) / sd_col\n",
    "    return std\n",
    "\n",
    "for col in ['x','x_square','x_cube']:\n",
    "    sim_data_poly[col] = scale(sim_data_poly[col])\n",
    "    \n",
    "sim_data_poly.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Turn:** In the cell below create and execute code to do the following:\n",
    "\n",
    "1. Define and fit an OLS model using the x, x_square, and x_cube features. This model will be linear in the coefficents of these features. \n",
    "2. Add a predicted column to the sim_data_poly data frame. \n",
    "3. Print the 4 model coefficients, intercept and 3 **partial slopes**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the regresson model and fit it to the data\n",
    "ols_model_poly = sm.ols(formula = 'y ~ x + x_square + x_cube', data=sim_data_poly).fit()\n",
    "\n",
    "# Add predicted to pandas dataframe\n",
    "\n",
    "\n",
    "## Print the model coefficient\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below create and execute code to print the summary of the OLS model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the model summary and consider the answers to these questions:\n",
    "1. Are all model coefficients significant or only some?    \n",
    "2. Is the model significant based on the F-statistic? \n",
    "3. How have $R^2$ and adjusted $R^2$ changed and what does this mean in terms of the fit of the model? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a feel for the model fit create and execute code to plot the regression line (predicted values) and the observations in the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you think qualitatively the polynomial model fits the data well? \n",
    "\n",
    "Finally, you will evaluate the behavior of the residuals. In the cell below, create and execute code to do the following:   \n",
    "1. Compute the residuals and save these in the resids column of the data frame. \n",
    "2. Compute and print the mean and RMSE of these residuals. \n",
    "3. PLot the histogram and Q-Q Normal plot of the residuals. \n",
    "4. Plot the residuals vs. the predicted values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the plots and consider the answers to these questions:\n",
    "1. How has the RMSE changed?\n",
    "2. Do the residuals appear to be approximately Normally distributed with zero mean?\n",
    "3. Are the residuals homoscedastic?\n",
    "4. Now, consider the entire evaluation of the model. Is this a good model for the data? Is the fit good? Is the model over-fit? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leverage and Cook's Distance\n",
    "\n",
    "Up to now, we have only looked at regression models with Normally distributed noise or errors. But, in the real world there are errors and outliers in data. These errors and outliers can have greater or lesser effect, depending on how extreme they are and their placement with respect to the other data. \n",
    "\n",
    "You can imagine a regression line as a lever. Outliers that occur near the ends of the lever will have a greater influence, all other factors being equal. \n",
    "\n",
    "One way to measure influence of a data point is **Cook's distance**, introduced by Dennis Cook in 1977. The influence for the ith data point can be computed as:\n",
    "\n",
    "$$D_i = \\frac{\\Sigma_{j=1}^n (\\hat{Y}_j - \\hat{Y}_{j(i)})^2}{n (p+1)\\hat{\\sigma^2}}$$\n",
    "where,\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{Y}_{j} &= the\\ jth\\ prediction\\ computed\\ with\\ all\\ observations\\\\\n",
    "\\hat{Y}_{j(i)} &= the\\ jth\\ prediction\\ computed\\ without\\ the\\ ith\\ observation\\\\\n",
    "p &= number\\ of\\ parameters\\\\\n",
    "n &= number\\ of\\ data\\ points\n",
    "\\end{align}\n",
    "\n",
    "In effect, cooks distance compares the mean squared difference between predicted values with and without a given data point. This calculation is updated excluding each observation one at a time. Thus, Cook's distance is a resampling method. Therefore, computing Cook's distance can be computationally intensive for large data set. Typically, Cook's distance is measured in units of standard deviation.\n",
    "\n",
    "Let's make these concepts concrete with an example. To start, execute the code in the cell below to create a data set with an outlier added and plot the observations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data frame with single outlier\n",
    "outliers = pd.DataFrame({'x':[0.0], 'y':[15], 'predicted':[0.0], 'resids':[0.0]})\n",
    "sim_data_outlier = pd.concat([outliers,sim_data])\n",
    "\n",
    "ax = sns.scatterplot(x='x', y='y', data=sim_data_outlier)\n",
    "_=ax.set_title('Observations with outlier')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in the cell below does the following:\n",
    "- Defines and fits an OLS model to the data set with the outlier. \n",
    "- Computes predictions based on this data set.\n",
    "- Plots the regression line along with the observations. \n",
    "\n",
    "Execute this code and examine the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the regresson model and fit it to the data\n",
    "ols_model_outlier = sm.ols(formula = 'y ~ x', data=sim_data_outlier).fit()\n",
    "\n",
    "# Add predicted to pandas dataframe\n",
    "sim_data_outlier['predicted'] = ols_model_outlier.predict(sim_data_outlier['x'])\n",
    "\n",
    "ax = sns.lineplot(x='x', y='predicted', data=sim_data_outlier, color='red')\n",
    "sns.scatterplot(x='x', y='y', data=sim_data_outlier, ax=ax)\n",
    "_=ax.set_title('Regression line with influence of outlier')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the slope of the line has changed because of the outlier. This is an example of effect of a **high leverage outlier**.  \n",
    "\n",
    "You can create an **influence plot**, which shows the influence and leverage of the observations vs. the standardized residuals. The [statsmodels.graphics.regressionplots.influence_plot](https://www.statsmodels.org/stable/generated/statsmodels.graphics.regressionplots.influence_plot.html) displays Cook's distance as a measure of influence. Execute the code in the cell below to display the influence plot for the OLS model of the data with the outlier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_=influence_plot(ols_model_outlier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is one data point, point 0, which has significant influence on the regression model. You can see this outlier as the large marker and the label, 0. \n",
    "\n",
    "Stats models also provides the [statsmodels.graphics.regressionplots.plot_regress_exog](https://www.statsmodels.org/stable/generated/statsmodels.graphics.regressionplots.plot_regress_exog.html) plotting method which displays a series of **partial regression plots**. A partial regression plot displays various measures of the regression model fit vs. a selected exogenous variable or feature. Execute the code in the cell below to see an example of these plots and examine the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 6))\n",
    "_=plot_regress_exog(ols_model_outlier, 'x', fig=fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The effect of the outlier is quite evident in these plots.\n",
    "\n",
    "Notice that these plots are all different from the standard residual plot which puts the predicted value on the horizontal axis. In contrast, the above plots put one of the exogenous variables on the horizontal axis. In a simple case, the difference is not that great. However with many exogenous variables (features) the partial plot can be difficult to interpret. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your turn:** As already stated, the partial regression plots can be a bit difficult to interpret at times. Now you will create and execute code to make more standard plots:    \n",
    "- The histogram and Q-Q Normal plot of the residuals. \n",
    "- The plot of residuals vs. the predicted values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine these plots and consider the answers to the following questions:   \n",
    "1. Does the outlier standout in the distribution of the residuals?\n",
    "2. With the exception of the outlier, do the residuals have an approximate Normal distribution?\n",
    "3. Are the residuals homoscedastic or heteroscedastic?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copyright 2017, 2018, 2019, 2020, Stephen F Elston. All rights reserved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
